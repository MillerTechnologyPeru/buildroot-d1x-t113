From d5c300b989692291cddd74b609075c4c28493e16 Mon Sep 17 00:00:00 2001
From: YuzukiTsuru <gloomyghost@gloomyghost.com>
Date: Fri, 25 Mar 2022 17:23:14 +0800
Subject: [PATCH 78/93] add sunxi driver crypto

---
 drivers/crypto/Kconfig                        |   29 +-
 drivers/crypto/Makefile                       |    3 +-
 drivers/crypto/sunxi-ce/Kconfig               |   22 +
 drivers/crypto/sunxi-ce/Makefile              |   60 +
 .../crypto/sunxi-ce/sun4i/sun4i-ss-cipher.c   |  545 ++++++
 drivers/crypto/sunxi-ce/sun4i/sun4i-ss-core.c |  427 +++++
 drivers/crypto/sunxi-ce/sun4i/sun4i-ss-hash.c |  521 ++++++
 drivers/crypto/sunxi-ce/sun4i/sun4i-ss.h      |  201 +++
 drivers/crypto/sunxi-ce/sunxi_ce.c            | 1527 ++++++++++++++++
 drivers/crypto/sunxi-ce/sunxi_ce.h            |  491 ++++++
 drivers/crypto/sunxi-ce/sunxi_ce_cdev.c       |  539 ++++++
 drivers/crypto/sunxi-ce/sunxi_ce_cdev.h       |  473 +++++
 drivers/crypto/sunxi-ce/sunxi_ce_proc.h       |   79 +
 drivers/crypto/sunxi-ce/sunxi_ce_proc_comm.c  |  416 +++++
 drivers/crypto/sunxi-ce/v2/sunxi_ce_proc.c    |  618 +++++++
 drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.c     |  382 ++++
 drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.h     |  234 +++
 .../crypto/sunxi-ce/v3/sunxi_ce_cdev_comm.c   |  447 +++++
 drivers/crypto/sunxi-ce/v3/sunxi_ce_proc.c    |  894 ++++++++++
 .../crypto/sunxi-ce/v3/sunxi_ce_proc_walk.c   | 1171 ++++++++++++
 drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.c     |  420 +++++
 drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.h     |  324 ++++
 .../crypto/sunxi-ce/v4/sunxi_ce_cdev_comm.c   |  447 +++++
 drivers/crypto/sunxi-ce/v4/sunxi_ce_proc.c    | 1490 ++++++++++++++++
 drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.c     |  450 +++++
 drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.h     |  296 ++++
 .../crypto/sunxi-ce/v5/sunxi_ce_cdev_comm.c   |  455 +++++
 drivers/crypto/sunxi-ce/v5/sunxi_ce_proc.c    | 1565 +++++++++++++++++
 drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.c     |  509 ++++++
 drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.h     |  301 ++++
 drivers/crypto/virtio/Kconfig                 |    1 -
 31 files changed, 15309 insertions(+), 28 deletions(-)
 create mode 100644 drivers/crypto/sunxi-ce/Kconfig
 create mode 100644 drivers/crypto/sunxi-ce/Makefile
 create mode 100644 drivers/crypto/sunxi-ce/sun4i/sun4i-ss-cipher.c
 create mode 100644 drivers/crypto/sunxi-ce/sun4i/sun4i-ss-core.c
 create mode 100644 drivers/crypto/sunxi-ce/sun4i/sun4i-ss-hash.c
 create mode 100644 drivers/crypto/sunxi-ce/sun4i/sun4i-ss.h
 create mode 100644 drivers/crypto/sunxi-ce/sunxi_ce.c
 create mode 100644 drivers/crypto/sunxi-ce/sunxi_ce.h
 create mode 100644 drivers/crypto/sunxi-ce/sunxi_ce_cdev.c
 create mode 100644 drivers/crypto/sunxi-ce/sunxi_ce_cdev.h
 create mode 100644 drivers/crypto/sunxi-ce/sunxi_ce_proc.h
 create mode 100644 drivers/crypto/sunxi-ce/sunxi_ce_proc_comm.c
 create mode 100644 drivers/crypto/sunxi-ce/v2/sunxi_ce_proc.c
 create mode 100644 drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.c
 create mode 100644 drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.h
 create mode 100644 drivers/crypto/sunxi-ce/v3/sunxi_ce_cdev_comm.c
 create mode 100644 drivers/crypto/sunxi-ce/v3/sunxi_ce_proc.c
 create mode 100644 drivers/crypto/sunxi-ce/v3/sunxi_ce_proc_walk.c
 create mode 100644 drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.c
 create mode 100644 drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.h
 create mode 100644 drivers/crypto/sunxi-ce/v4/sunxi_ce_cdev_comm.c
 create mode 100644 drivers/crypto/sunxi-ce/v4/sunxi_ce_proc.c
 create mode 100644 drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.c
 create mode 100644 drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.h
 create mode 100644 drivers/crypto/sunxi-ce/v5/sunxi_ce_cdev_comm.c
 create mode 100644 drivers/crypto/sunxi-ce/v5/sunxi_ce_proc.c
 create mode 100644 drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.c
 create mode 100644 drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.h

diff --git a/drivers/crypto/Kconfig b/drivers/crypto/Kconfig
index 0952f059d..902a75756 100644
--- a/drivers/crypto/Kconfig
+++ b/drivers/crypto/Kconfig
@@ -659,31 +659,6 @@ config CRYPTO_DEV_IMGTEC_HASH
 	  hardware hash accelerator. Supporting MD5/SHA1/SHA224/SHA256
 	  hashing algorithms.
 
-config CRYPTO_DEV_SUN4I_SS
-	tristate "Support for Allwinner Security System cryptographic accelerator"
-	depends on ARCH_SUNXI && !64BIT
-	select CRYPTO_MD5
-	select CRYPTO_SHA1
-	select CRYPTO_AES
-	select CRYPTO_LIB_DES
-	select CRYPTO_BLKCIPHER
-	help
-	  Some Allwinner SoC have a crypto accelerator named
-	  Security System. Select this if you want to use it.
-	  The Security System handle AES/DES/3DES ciphers in CBC mode
-	  and SHA1 and MD5 hash algorithms.
-
-	  To compile this driver as a module, choose M here: the module
-	  will be called sun4i-ss.
-
-config CRYPTO_DEV_SUN4I_SS_PRNG
-	bool "Support for Allwinner Security System PRNG"
-	depends on CRYPTO_DEV_SUN4I_SS
-	select CRYPTO_RNG
-	help
-	  Select this option if you want to provide kernel-side support for
-	  the Pseudo-Random Number Generator found in the Security System.
-
 config CRYPTO_DEV_ROCKCHIP
 	tristate "Rockchip's Cryptographic Engine driver"
 	depends on OF && ARCH_ROCKCHIP
@@ -806,5 +781,7 @@ config CRYPTO_DEV_CCREE
 	  If unsure say Y.
 
 source "drivers/crypto/hisilicon/Kconfig"
-
+menu "Support for Allwinner Sunxi CryptoEngine"
+source "drivers/crypto/sunxi-ce/Kconfig"
+endmenu
 endif # CRYPTO_HW
diff --git a/drivers/crypto/Makefile b/drivers/crypto/Makefile
index afc4753b5..114ffe304 100644
--- a/drivers/crypto/Makefile
+++ b/drivers/crypto/Makefile
@@ -39,7 +39,8 @@ obj-$(CONFIG_CRYPTO_DEV_ROCKCHIP) += rockchip/
 obj-$(CONFIG_CRYPTO_DEV_S5P) += s5p-sss.o
 obj-$(CONFIG_CRYPTO_DEV_SAHARA) += sahara.o
 obj-$(CONFIG_ARCH_STM32) += stm32/
-obj-$(CONFIG_CRYPTO_DEV_SUN4I_SS) += sunxi-ss/
+obj-$(CONFIG_CRYPTO_DEV_SUNXI) += sunxi-ce/
+obj-$(CONFIG_CRYPTO_DEV_SUNXI_IOCTL) += sunxi-ce/
 obj-$(CONFIG_CRYPTO_DEV_TALITOS) += talitos.o
 obj-$(CONFIG_CRYPTO_DEV_UX500) += ux500/
 obj-$(CONFIG_CRYPTO_DEV_VIRTIO) += virtio/
diff --git a/drivers/crypto/sunxi-ce/Kconfig b/drivers/crypto/sunxi-ce/Kconfig
new file mode 100644
index 000000000..24e62c3b2
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/Kconfig
@@ -0,0 +1,22 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+comment"Choose one according to the actual usage"
+
+config CRYPTO_DEV_SUNXI
+	tristate "CE support the AF_ALG interface for user api"
+	depends on ARCH_SUNXI
+	select CRYPTO_MD5
+	select CRYPTO_SHA1
+	select CRYPTO_AES
+	select CRYPTO_DES
+	select CRYPTO_BLKCIPHER
+	help
+	  Allwinner Sunxi SoC have a crypto accelerator named
+	  CryptoEngine.Select this if you want to use it.
+
+config CRYPTO_DEV_SUNXI_IOCTL
+	tristate "CE support the systemcall interface for user api"
+	depends on ARCH_SUNXI
+	help
+	  Allwinner Sunxi SoC have a crypto accelerator named
+	  CryptoEngine.Select this if you want to use it.
diff --git a/drivers/crypto/sunxi-ce/Makefile b/drivers/crypto/sunxi-ce/Makefile
new file mode 100644
index 000000000..fb53c3711
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/Makefile
@@ -0,0 +1,60 @@
+obj-$(CONFIG_CRYPTO_DEV_SUN4I_SS) += sun4i-ss.o
+sun4i-ss-y += sun4i/sun4i-ss-core.o sun4i/sun4i-ss-hash.o sun4i/sun4i-ss-cipher.o
+
+obj-$(CONFIG_CRYPTO_DEV_SUNXI) += sunxi-ce.o
+obj-$(CONFIG_CRYPTO_DEV_SUNXI_IOCTL) += sunxi-ce-ioctl.o
+
+sunxi-ce-ioctl-$(CONFIG_CRYPTO_DEV_SUNXI_IOCTL) += sunxi_ce_cdev.o
+sunxi-ce-$(CONFIG_CRYPTO_DEV_SUNXI) += sunxi_ce.o sunxi_ce_proc_comm.o
+
+#ifdef CONFIG_ARCH_SUN8IW6
+#	SUNXI_CE_VER = v2
+#endif
+ifdef CONFIG_ARCH_SUN20IW1
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW11
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW12
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW15
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW17
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW7
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW18
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN50I
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW16
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW19
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN8IW20
+	SUNXI_CE_VER = v3
+endif
+ifdef CONFIG_ARCH_SUN50IW8
+	SUNXI_CE_VER = v4
+endif
+ifdef CONFIG_ARCH_SUN50IW10
+	SUNXI_CE_VER = v4
+endif
+ifdef CONFIG_ARCH_SUN50IW12
+	SUNXI_CE_VER = v5
+endif
+
+sunxi-ce-$(CONFIG_CRYPTO_DEV_SUNXI) +=  $(SUNXI_CE_VER)/sunxi_ce_reg.o $(SUNXI_CE_VER)/sunxi_ce_proc.o
+sunxi-ce-ioctl-$(CONFIG_CRYPTO_DEV_SUNXI_IOCTL) += $(SUNXI_CE_VER)/sunxi_ce_reg.o $(SUNXI_CE_VER)/sunxi_ce_cdev_comm.o
+
+ccflags-y += -I$(srctree)/drivers/crypto/sunxi-ce/$(SUNXI_CE_VER)
+#ccflags-y += -DDEBUG
diff --git a/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-cipher.c b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-cipher.c
new file mode 100644
index 000000000..90efd10d5
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-cipher.c
@@ -0,0 +1,545 @@
+/*
+ * sun4i-ss-cipher.c - hardware cryptographic accelerator for Allwinner A20 SoC
+ *
+ * Copyright (C) 2013-2015 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * This file add support for AES cipher with 128,192,256 bits
+ * keysize in CBC and ECB mode.
+ * Add support also for DES and 3DES in CBC and ECB mode.
+ *
+ * You could find the datasheet in Documentation/arm/sunxi/README
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include "sun4i-ss.h"
+
+static int sun4i_ss_opti_poll(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_ss_ctx *ss = op->ss;
+	unsigned int ivsize = crypto_ablkcipher_ivsize(tfm);
+	struct sun4i_cipher_req_ctx *ctx = ablkcipher_request_ctx(areq);
+	u32 mode = ctx->mode;
+	/* when activating SS, the default FIFO space is SS_RX_DEFAULT(32) */
+	u32 rx_cnt = SS_RX_DEFAULT;
+	u32 tx_cnt = 0;
+	u32 spaces;
+	u32 v;
+	int err = 0;
+	unsigned int i;
+	unsigned int ileft = areq->nbytes;
+	unsigned int oleft = areq->nbytes;
+	unsigned int todo;
+	struct sg_mapping_iter mi, mo;
+	unsigned int oi, oo; /* offset for in and out */
+	unsigned long flags;
+
+	if (areq->nbytes == 0)
+		return 0;
+
+	if (!areq->info) {
+		dev_err_ratelimited(ss->dev, "ERROR: Empty IV\n");
+		return -EINVAL;
+	}
+
+	if (!areq->src || !areq->dst) {
+		dev_err_ratelimited(ss->dev, "ERROR: Some SGs are NULL\n");
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&ss->slock, flags);
+
+	for (i = 0; i < op->keylen; i += 4)
+		writel(*(op->key + i / 4), ss->base + SS_KEY0 + i);
+
+	if (areq->info) {
+		for (i = 0; i < 4 && i < ivsize / 4; i++) {
+			v = *(u32 *)(areq->info + i * 4);
+			writel(v, ss->base + SS_IV0 + i * 4);
+		}
+	}
+	writel(mode, ss->base + SS_CTL);
+
+	sg_miter_start(&mi, areq->src, sg_nents(areq->src),
+		       SG_MITER_FROM_SG | SG_MITER_ATOMIC);
+	sg_miter_start(&mo, areq->dst, sg_nents(areq->dst),
+		       SG_MITER_TO_SG | SG_MITER_ATOMIC);
+	sg_miter_next(&mi);
+	sg_miter_next(&mo);
+	if (!mi.addr || !mo.addr) {
+		dev_err_ratelimited(ss->dev, "ERROR: sg_miter return null\n");
+		err = -EINVAL;
+		goto release_ss;
+	}
+
+	ileft = areq->nbytes / 4;
+	oleft = areq->nbytes / 4;
+	oi = 0;
+	oo = 0;
+	do {
+		todo = min3(rx_cnt, ileft, (mi.length - oi) / 4);
+		if (todo > 0) {
+			ileft -= todo;
+			writesl(ss->base + SS_RXFIFO, mi.addr + oi, todo);
+			oi += todo * 4;
+		}
+		if (oi == mi.length) {
+			sg_miter_next(&mi);
+			oi = 0;
+		}
+
+		spaces = readl(ss->base + SS_FCSR);
+		rx_cnt = SS_RXFIFO_SPACES(spaces);
+		tx_cnt = SS_TXFIFO_SPACES(spaces);
+
+		todo = min3(tx_cnt, oleft, (mo.length - oo) / 4);
+		if (todo > 0) {
+			oleft -= todo;
+			readsl(ss->base + SS_TXFIFO, mo.addr + oo, todo);
+			oo += todo * 4;
+		}
+		if (oo == mo.length) {
+			sg_miter_next(&mo);
+			oo = 0;
+		}
+	} while (oleft > 0);
+
+	if (areq->info) {
+		for (i = 0; i < 4 && i < ivsize / 4; i++) {
+			v = readl(ss->base + SS_IV0 + i * 4);
+			*(u32 *)(areq->info + i * 4) = v;
+		}
+	}
+
+release_ss:
+	sg_miter_stop(&mi);
+	sg_miter_stop(&mo);
+	writel(0, ss->base + SS_CTL);
+	spin_unlock_irqrestore(&ss->slock, flags);
+	return err;
+}
+
+/* Generic function that support SG with size not multiple of 4 */
+static int sun4i_ss_cipher_poll(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_ss_ctx *ss = op->ss;
+	int no_chunk = 1;
+	struct scatterlist *in_sg = areq->src;
+	struct scatterlist *out_sg = areq->dst;
+	unsigned int ivsize = crypto_ablkcipher_ivsize(tfm);
+	struct sun4i_cipher_req_ctx *ctx = ablkcipher_request_ctx(areq);
+	u32 mode = ctx->mode;
+	/* when activating SS, the default FIFO space is SS_RX_DEFAULT(32) */
+	u32 rx_cnt = SS_RX_DEFAULT;
+	u32 tx_cnt = 0;
+	u32 v;
+	u32 spaces;
+	int err = 0;
+	unsigned int i;
+	unsigned int ileft = areq->nbytes;
+	unsigned int oleft = areq->nbytes;
+	unsigned int todo;
+	struct sg_mapping_iter mi, mo;
+	unsigned int oi, oo;	/* offset for in and out */
+	char buf[4 * SS_RX_MAX];/* buffer for linearize SG src */
+	char bufo[4 * SS_TX_MAX]; /* buffer for linearize SG dst */
+	unsigned int ob = 0;	/* offset in buf */
+	unsigned int obo = 0;	/* offset in bufo*/
+	unsigned int obl = 0;	/* length of data in bufo */
+	unsigned long flags;
+
+	if (areq->nbytes == 0)
+		return 0;
+
+	if (!areq->info) {
+		dev_err_ratelimited(ss->dev, "ERROR: Empty IV\n");
+		return -EINVAL;
+	}
+
+	if (!areq->src || !areq->dst) {
+		dev_err_ratelimited(ss->dev, "ERROR: Some SGs are NULL\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * if we have only SGs with size multiple of 4,
+	 * we can use the SS optimized function
+	 */
+	while (in_sg && no_chunk == 1) {
+		if ((in_sg->length % 4) != 0)
+			no_chunk = 0;
+		in_sg = sg_next(in_sg);
+	}
+	while (out_sg && no_chunk == 1) {
+		if ((out_sg->length % 4) != 0)
+			no_chunk = 0;
+		out_sg = sg_next(out_sg);
+	}
+
+	if (no_chunk == 1)
+		return sun4i_ss_opti_poll(areq);
+
+	spin_lock_irqsave(&ss->slock, flags);
+
+	for (i = 0; i < op->keylen; i += 4)
+		writel(*(op->key + i / 4), ss->base + SS_KEY0 + i);
+
+	if (areq->info) {
+		for (i = 0; i < 4 && i < ivsize / 4; i++) {
+			v = *(u32 *)(areq->info + i * 4);
+			writel(v, ss->base + SS_IV0 + i * 4);
+		}
+	}
+	writel(mode, ss->base + SS_CTL);
+
+	sg_miter_start(&mi, areq->src, sg_nents(areq->src),
+		       SG_MITER_FROM_SG | SG_MITER_ATOMIC);
+	sg_miter_start(&mo, areq->dst, sg_nents(areq->dst),
+		       SG_MITER_TO_SG | SG_MITER_ATOMIC);
+	sg_miter_next(&mi);
+	sg_miter_next(&mo);
+	if (!mi.addr || !mo.addr) {
+		dev_err_ratelimited(ss->dev, "ERROR: sg_miter return null\n");
+		err = -EINVAL;
+		goto release_ss;
+	}
+	ileft = areq->nbytes;
+	oleft = areq->nbytes;
+	oi = 0;
+	oo = 0;
+
+	while (oleft > 0) {
+		if (ileft > 0) {
+			/*
+			 * todo is the number of consecutive 4byte word that we
+			 * can read from current SG
+			 */
+			todo = min3(rx_cnt, ileft / 4, (mi.length - oi) / 4);
+			if (todo > 0 && ob == 0) {
+				writesl(ss->base + SS_RXFIFO, mi.addr + oi,
+					todo);
+				ileft -= todo * 4;
+				oi += todo * 4;
+			} else {
+				/*
+				 * not enough consecutive bytes, so we need to
+				 * linearize in buf. todo is in bytes
+				 * After that copy, if we have a multiple of 4
+				 * we need to be able to write all buf in one
+				 * pass, so it is why we min() with rx_cnt
+				 */
+				todo = min3(rx_cnt * 4 - ob, ileft,
+					    mi.length - oi);
+				memcpy(buf + ob, mi.addr + oi, todo);
+				ileft -= todo;
+				oi += todo;
+				ob += todo;
+				if (ob % 4 == 0) {
+					writesl(ss->base + SS_RXFIFO, buf,
+						ob / 4);
+					ob = 0;
+				}
+			}
+			if (oi == mi.length) {
+				sg_miter_next(&mi);
+				oi = 0;
+			}
+		}
+
+		spaces = readl(ss->base + SS_FCSR);
+		rx_cnt = SS_RXFIFO_SPACES(spaces);
+		tx_cnt = SS_TXFIFO_SPACES(spaces);
+		dev_dbg(ss->dev, "%x %u/%u %u/%u cnt=%u %u/%u %u/%u cnt=%u %u\n",
+			mode,
+			oi, mi.length, ileft, areq->nbytes, rx_cnt,
+			oo, mo.length, oleft, areq->nbytes, tx_cnt, ob);
+
+		if (tx_cnt == 0)
+			continue;
+		/* todo in 4bytes word */
+		todo = min3(tx_cnt, oleft / 4, (mo.length - oo) / 4);
+		if (todo > 0) {
+			readsl(ss->base + SS_TXFIFO, mo.addr + oo, todo);
+			oleft -= todo * 4;
+			oo += todo * 4;
+			if (oo == mo.length) {
+				sg_miter_next(&mo);
+				oo = 0;
+			}
+		} else {
+			/*
+			 * read obl bytes in bufo, we read at maximum for
+			 * emptying the device
+			 */
+			readsl(ss->base + SS_TXFIFO, bufo, tx_cnt);
+			obl = tx_cnt * 4;
+			obo = 0;
+			do {
+				/*
+				 * how many bytes we can copy ?
+				 * no more than remaining SG size
+				 * no more than remaining buffer
+				 * no need to test against oleft
+				 */
+				todo = min(mo.length - oo, obl - obo);
+				memcpy(mo.addr + oo, bufo + obo, todo);
+				oleft -= todo;
+				obo += todo;
+				oo += todo;
+				if (oo == mo.length) {
+					sg_miter_next(&mo);
+					oo = 0;
+				}
+			} while (obo < obl);
+			/* bufo must be fully used here */
+		}
+	}
+	if (areq->info) {
+		for (i = 0; i < 4 && i < ivsize / 4; i++) {
+			v = readl(ss->base + SS_IV0 + i * 4);
+			*(u32 *)(areq->info + i * 4) = v;
+		}
+	}
+
+release_ss:
+	sg_miter_stop(&mi);
+	sg_miter_stop(&mo);
+	writel(0, ss->base + SS_CTL);
+	spin_unlock_irqrestore(&ss->slock, flags);
+
+	return err;
+}
+
+/* CBC AES */
+int sun4i_ss_cbc_aes_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_AES | SS_CBC | SS_ENABLED | SS_ENCRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_cbc_aes_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_AES | SS_CBC | SS_ENABLED | SS_DECRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+/* ECB AES */
+int sun4i_ss_ecb_aes_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_AES | SS_ECB | SS_ENABLED | SS_ENCRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_ecb_aes_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_AES | SS_ECB | SS_ENABLED | SS_DECRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+/* CBC DES */
+int sun4i_ss_cbc_des_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_DES | SS_CBC | SS_ENABLED | SS_ENCRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_cbc_des_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_DES | SS_CBC | SS_ENABLED | SS_DECRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+/* ECB DES */
+int sun4i_ss_ecb_des_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_DES | SS_ECB | SS_ENABLED | SS_ENCRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_ecb_des_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_DES | SS_ECB | SS_ENABLED | SS_DECRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+/* CBC 3DES */
+int sun4i_ss_cbc_des3_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_3DES | SS_CBC | SS_ENABLED | SS_ENCRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_cbc_des3_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_3DES | SS_CBC | SS_ENABLED | SS_DECRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+/* ECB 3DES */
+int sun4i_ss_ecb_des3_encrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_3DES | SS_ECB | SS_ENABLED | SS_ENCRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_ecb_des3_decrypt(struct ablkcipher_request *areq)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(areq);
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_cipher_req_ctx *rctx = ablkcipher_request_ctx(areq);
+
+	rctx->mode = SS_OP_3DES | SS_ECB | SS_ENABLED | SS_DECRYPTION |
+		op->keymode;
+	return sun4i_ss_cipher_poll(areq);
+}
+
+int sun4i_ss_cipher_init(struct crypto_tfm *tfm)
+{
+	struct sun4i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+	struct crypto_alg *alg = tfm->__crt_alg;
+	struct sun4i_ss_alg_template *algt;
+
+	memset(op, 0, sizeof(struct sun4i_tfm_ctx));
+
+	algt = container_of(alg, struct sun4i_ss_alg_template, alg.crypto);
+	op->ss = algt->ss;
+
+	tfm->crt_ablkcipher.reqsize = sizeof(struct sun4i_cipher_req_ctx);
+
+	return 0;
+}
+
+/* check and set the AES key, prepare the mode to be used */
+int sun4i_ss_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			unsigned int keylen)
+{
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_ss_ctx *ss = op->ss;
+
+	switch (keylen) {
+	case 128 / 8:
+		op->keymode = SS_AES_128BITS;
+		break;
+	case 192 / 8:
+		op->keymode = SS_AES_192BITS;
+		break;
+	case 256 / 8:
+		op->keymode = SS_AES_256BITS;
+		break;
+	default:
+		dev_err(ss->dev, "ERROR: Invalid keylen %u\n", keylen);
+		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+	op->keylen = keylen;
+	memcpy(op->key, key, keylen);
+	return 0;
+}
+
+/* check and set the DES key, prepare the mode to be used */
+int sun4i_ss_des_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			unsigned int keylen)
+{
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_ss_ctx *ss = op->ss;
+	u32 flags;
+	u32 tmp[DES_EXPKEY_WORDS];
+	int ret;
+
+	if (unlikely(keylen != DES_KEY_SIZE)) {
+		dev_err(ss->dev, "Invalid keylen %u\n", keylen);
+		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+
+	flags = crypto_ablkcipher_get_flags(tfm);
+
+	ret = des_ekey(tmp, key);
+	if (unlikely(ret == 0) && (flags & CRYPTO_TFM_REQ_WEAK_KEY)) {
+		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_WEAK_KEY);
+		dev_dbg(ss->dev, "Weak key %u\n", keylen);
+		return -EINVAL;
+	}
+
+	op->keylen = keylen;
+	memcpy(op->key, key, keylen);
+	return 0;
+}
+
+/* check and set the 3DES key, prepare the mode to be used */
+int sun4i_ss_des3_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			 unsigned int keylen)
+{
+	struct sun4i_tfm_ctx *op = crypto_ablkcipher_ctx(tfm);
+	struct sun4i_ss_ctx *ss = op->ss;
+
+	if (unlikely(keylen != 3 * DES_KEY_SIZE)) {
+		dev_err(ss->dev, "Invalid keylen %u\n", keylen);
+		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+	op->keylen = keylen;
+	memcpy(op->key, key, keylen);
+	return 0;
+}
diff --git a/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-core.c b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-core.c
new file mode 100644
index 000000000..3ac6c6c4a
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-core.c
@@ -0,0 +1,427 @@
+/*
+ * sun4i-ss-core.c - hardware cryptographic accelerator for Allwinner A20 SoC
+ *
+ * Copyright (C) 2013-2015 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * Core file which registers crypto algorithms supported by the SS.
+ *
+ * You could find a link for the datasheet in Documentation/arm/sunxi/README
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/clk.h>
+#include <linux/crypto.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <crypto/scatterwalk.h>
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/reset.h>
+
+#include "sun4i-ss.h"
+
+static struct sun4i_ss_alg_template ss_algs[] = {
+{       .type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_MD5,
+	.alg.hash = {
+		.init = sun4i_hash_init,
+		.update = sun4i_hash_update,
+		.final = sun4i_hash_final,
+		.finup = sun4i_hash_finup,
+		.digest = sun4i_hash_digest,
+		.export = sun4i_hash_export_md5,
+		.import = sun4i_hash_import_md5,
+		.halg = {
+			.digestsize = MD5_DIGEST_SIZE,
+			.statesize = sizeof(struct md5_state),
+			.base = {
+				.cra_name = "md5",
+				.cra_driver_name = "md5-sun4i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH,
+				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun4i_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun4i_hash_crainit
+			}
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = SS_OP_SHA1,
+	.alg.hash = {
+		.init = sun4i_hash_init,
+		.update = sun4i_hash_update,
+		.final = sun4i_hash_final,
+		.finup = sun4i_hash_finup,
+		.digest = sun4i_hash_digest,
+		.export = sun4i_hash_export_sha1,
+		.import = sun4i_hash_import_sha1,
+		.halg = {
+			.digestsize = SHA1_DIGEST_SIZE,
+			.statesize = sizeof(struct sha1_state),
+			.base = {
+				.cra_name = "sha1",
+				.cra_driver_name = "sha1-sun4i-ss",
+				.cra_priority = 300,
+				.cra_alignmask = 3,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH,
+				.cra_blocksize = SHA1_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sun4i_req_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_type = &crypto_ahash_type,
+				.cra_init = sun4i_hash_crainit
+			}
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "cbc(aes)",
+		.cra_driver_name = "cbc-aes-sun4i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+		.cra_ctxsize = sizeof(struct sun4i_tfm_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun4i_ss_cipher_init,
+		.cra_ablkcipher = {
+			.min_keysize	= AES_MIN_KEY_SIZE,
+			.max_keysize	= AES_MAX_KEY_SIZE,
+			.ivsize		= AES_BLOCK_SIZE,
+			.setkey         = sun4i_ss_aes_setkey,
+			.encrypt        = sun4i_ss_cbc_aes_encrypt,
+			.decrypt        = sun4i_ss_cbc_aes_decrypt,
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "ecb(aes)",
+		.cra_driver_name = "ecb-aes-sun4i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+		.cra_ctxsize = sizeof(struct sun4i_tfm_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun4i_ss_cipher_init,
+		.cra_ablkcipher = {
+			.min_keysize	= AES_MIN_KEY_SIZE,
+			.max_keysize	= AES_MAX_KEY_SIZE,
+			.ivsize		= AES_BLOCK_SIZE,
+			.setkey         = sun4i_ss_aes_setkey,
+			.encrypt        = sun4i_ss_ecb_aes_encrypt,
+			.decrypt        = sun4i_ss_ecb_aes_decrypt,
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "cbc(des)",
+		.cra_driver_name = "cbc-des-sun4i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = DES_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+		.cra_ctxsize = sizeof(struct sun4i_req_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun4i_ss_cipher_init,
+		.cra_u.ablkcipher = {
+			.min_keysize    = DES_KEY_SIZE,
+			.max_keysize    = DES_KEY_SIZE,
+			.ivsize         = DES_BLOCK_SIZE,
+			.setkey         = sun4i_ss_des_setkey,
+			.encrypt        = sun4i_ss_cbc_des_encrypt,
+			.decrypt        = sun4i_ss_cbc_des_decrypt,
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "ecb(des)",
+		.cra_driver_name = "ecb-des-sun4i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = DES_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+		.cra_ctxsize = sizeof(struct sun4i_req_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun4i_ss_cipher_init,
+		.cra_u.ablkcipher = {
+			.min_keysize    = DES_KEY_SIZE,
+			.max_keysize    = DES_KEY_SIZE,
+			.setkey         = sun4i_ss_des_setkey,
+			.encrypt        = sun4i_ss_ecb_des_encrypt,
+			.decrypt        = sun4i_ss_ecb_des_decrypt,
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "cbc(des3_ede)",
+		.cra_driver_name = "cbc-des3-sun4i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+		.cra_ctxsize = sizeof(struct sun4i_req_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun4i_ss_cipher_init,
+		.cra_u.ablkcipher = {
+			.min_keysize    = DES3_EDE_KEY_SIZE,
+			.max_keysize    = DES3_EDE_KEY_SIZE,
+			.ivsize         = DES3_EDE_BLOCK_SIZE,
+			.setkey         = sun4i_ss_des3_setkey,
+			.encrypt        = sun4i_ss_cbc_des3_encrypt,
+			.decrypt        = sun4i_ss_cbc_des3_decrypt,
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.alg.crypto = {
+		.cra_name = "ecb(des3_ede)",
+		.cra_driver_name = "ecb-des3-sun4i-ss",
+		.cra_priority = 300,
+		.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+		.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+		.cra_ctxsize = sizeof(struct sun4i_req_ctx),
+		.cra_module = THIS_MODULE,
+		.cra_alignmask = 3,
+		.cra_type = &crypto_ablkcipher_type,
+		.cra_init = sun4i_ss_cipher_init,
+		.cra_u.ablkcipher = {
+			.min_keysize    = DES3_EDE_KEY_SIZE,
+			.max_keysize    = DES3_EDE_KEY_SIZE,
+			.ivsize         = DES3_EDE_BLOCK_SIZE,
+			.setkey         = sun4i_ss_des3_setkey,
+			.encrypt        = sun4i_ss_ecb_des3_encrypt,
+			.decrypt        = sun4i_ss_ecb_des3_decrypt,
+		}
+	}
+},
+};
+
+static int sun4i_ss_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	u32 v;
+	int err, i;
+	unsigned long cr;
+	const unsigned long cr_ahb = 24 * 1000 * 1000;
+	const unsigned long cr_mod = 150 * 1000 * 1000;
+	struct sun4i_ss_ctx *ss;
+
+	if (!pdev->dev.of_node)
+		return -ENODEV;
+
+	ss = devm_kzalloc(&pdev->dev, sizeof(*ss), GFP_KERNEL);
+	if (!ss)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	ss->base = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(ss->base)) {
+		dev_err(&pdev->dev, "Cannot request MMIO\n");
+		return PTR_ERR(ss->base);
+	}
+
+	ss->ssclk = devm_clk_get(&pdev->dev, "mod");
+	if (IS_ERR(ss->ssclk)) {
+		err = PTR_ERR(ss->ssclk);
+		dev_err(&pdev->dev, "Cannot get SS clock err=%d\n", err);
+		return err;
+	}
+	dev_dbg(&pdev->dev, "clock ss acquired\n");
+
+	ss->busclk = devm_clk_get(&pdev->dev, "ahb");
+	if (IS_ERR(ss->busclk)) {
+		err = PTR_ERR(ss->busclk);
+		dev_err(&pdev->dev, "Cannot get AHB SS clock err=%d\n", err);
+		return err;
+	}
+	dev_dbg(&pdev->dev, "clock ahb_ss acquired\n");
+
+	ss->reset = devm_reset_control_get_optional(&pdev->dev, "ahb");
+	if (IS_ERR(ss->reset)) {
+		if (PTR_ERR(ss->reset) == -EPROBE_DEFER)
+			return PTR_ERR(ss->reset);
+		dev_info(&pdev->dev, "no reset control found\n");
+		ss->reset = NULL;
+	}
+
+	/* Enable both clocks */
+	err = clk_prepare_enable(ss->busclk);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot prepare_enable busclk\n");
+		return err;
+	}
+	err = clk_prepare_enable(ss->ssclk);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot prepare_enable ssclk\n");
+		goto error_ssclk;
+	}
+
+	/*
+	 * Check that clock have the correct rates given in the datasheet
+	 * Try to set the clock to the maximum allowed
+	 */
+	err = clk_set_rate(ss->ssclk, cr_mod);
+	if (err != 0) {
+		dev_err(&pdev->dev, "Cannot set clock rate to ssclk\n");
+		goto error_clk;
+	}
+
+	/* Deassert reset if we have a reset control */
+	if (ss->reset) {
+		err = reset_control_deassert(ss->reset);
+		if (err) {
+			dev_err(&pdev->dev, "Cannot deassert reset control\n");
+			goto error_clk;
+		}
+	}
+
+	/*
+	 * The only impact on clocks below requirement are bad performance,
+	 * so do not print "errors"
+	 * warn on Overclocked clocks
+	 */
+	cr = clk_get_rate(ss->busclk);
+	if (cr >= cr_ahb)
+		dev_dbg(&pdev->dev, "Clock bus %lu (%lu MHz) (must be >= %lu)\n",
+			cr, cr / 1000000, cr_ahb);
+	else
+		dev_warn(&pdev->dev, "Clock bus %lu (%lu MHz) (must be >= %lu)\n",
+			 cr, cr / 1000000, cr_ahb);
+
+	cr = clk_get_rate(ss->ssclk);
+	if (cr <= cr_mod)
+		if (cr < cr_mod)
+			dev_warn(&pdev->dev, "Clock ss %lu (%lu MHz) (must be <= %lu)\n",
+				 cr, cr / 1000000, cr_mod);
+		else
+			dev_dbg(&pdev->dev, "Clock ss %lu (%lu MHz) (must be <= %lu)\n",
+				cr, cr / 1000000, cr_mod);
+	else
+		dev_warn(&pdev->dev, "Clock ss is at %lu (%lu MHz) (must be <= %lu)\n",
+			 cr, cr / 1000000, cr_mod);
+
+	/*
+	 * Datasheet named it "Die Bonding ID"
+	 * I expect to be a sort of Security System Revision number.
+	 * Since the A80 seems to have an other version of SS
+	 * this info could be useful
+	 */
+	writel(SS_ENABLED, ss->base + SS_CTL);
+	v = readl(ss->base + SS_CTL);
+	v >>= 16;
+	v &= 0x07;
+	dev_info(&pdev->dev, "Die ID %d\n", v);
+	writel(0, ss->base + SS_CTL);
+
+	ss->dev = &pdev->dev;
+
+	spin_lock_init(&ss->slock);
+
+	for (i = 0; i < ARRAY_SIZE(ss_algs); i++) {
+		ss_algs[i].ss = ss;
+		switch (ss_algs[i].type) {
+		case CRYPTO_ALG_TYPE_ABLKCIPHER:
+			err = crypto_register_alg(&ss_algs[i].alg.crypto);
+			if (err != 0) {
+				dev_err(ss->dev, "Fail to register %s\n",
+					ss_algs[i].alg.crypto.cra_name);
+				goto error_alg;
+			}
+			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			err = crypto_register_ahash(&ss_algs[i].alg.hash);
+			if (err != 0) {
+				dev_err(ss->dev, "Fail to register %s\n",
+					ss_algs[i].alg.hash.halg.base.cra_name);
+				goto error_alg;
+			}
+			break;
+		}
+	}
+	platform_set_drvdata(pdev, ss);
+	return 0;
+error_alg:
+	i--;
+	for (; i >= 0; i--) {
+		switch (ss_algs[i].type) {
+		case CRYPTO_ALG_TYPE_ABLKCIPHER:
+			crypto_unregister_alg(&ss_algs[i].alg.crypto);
+			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			crypto_unregister_ahash(&ss_algs[i].alg.hash);
+			break;
+		}
+	}
+	if (ss->reset)
+		reset_control_assert(ss->reset);
+error_clk:
+	clk_disable_unprepare(ss->ssclk);
+error_ssclk:
+	clk_disable_unprepare(ss->busclk);
+	return err;
+}
+
+static int sun4i_ss_remove(struct platform_device *pdev)
+{
+	int i;
+	struct sun4i_ss_ctx *ss = platform_get_drvdata(pdev);
+
+	for (i = 0; i < ARRAY_SIZE(ss_algs); i++) {
+		switch (ss_algs[i].type) {
+		case CRYPTO_ALG_TYPE_ABLKCIPHER:
+			crypto_unregister_alg(&ss_algs[i].alg.crypto);
+			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			crypto_unregister_ahash(&ss_algs[i].alg.hash);
+			break;
+		}
+	}
+
+	writel(0, ss->base + SS_CTL);
+	if (ss->reset)
+		reset_control_assert(ss->reset);
+	clk_disable_unprepare(ss->busclk);
+	clk_disable_unprepare(ss->ssclk);
+	return 0;
+}
+
+static const struct of_device_id a20ss_crypto_of_match_table[] = {
+	{ .compatible = "allwinner,sun4i-a10-crypto" },
+	{}
+};
+MODULE_DEVICE_TABLE(of, a20ss_crypto_of_match_table);
+
+static struct platform_driver sun4i_ss_driver = {
+	.probe          = sun4i_ss_probe,
+	.remove         = sun4i_ss_remove,
+	.driver         = {
+		.name           = "sun4i-ss",
+		.of_match_table	= a20ss_crypto_of_match_table,
+	},
+};
+
+module_platform_driver(sun4i_ss_driver);
+
+MODULE_DESCRIPTION("Allwinner Security System cryptographic accelerator");
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Corentin LABBE <clabbe.montjoie@gmail.com>");
diff --git a/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-hash.c b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-hash.c
new file mode 100644
index 000000000..0de2f62d5
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss-hash.c
@@ -0,0 +1,521 @@
+/*
+ * sun4i-ss-hash.c - hardware cryptographic accelerator for Allwinner A20 SoC
+ *
+ * Copyright (C) 2013-2015 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * This file add support for MD5 and SHA1.
+ *
+ * You could find the datasheet in Documentation/arm/sunxi/README
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include "sun4i-ss.h"
+#include <linux/scatterlist.h>
+
+/* This is a totally arbitrary value */
+#define SS_TIMEOUT 100
+
+int sun4i_hash_crainit(struct crypto_tfm *tfm)
+{
+	struct sun4i_tfm_ctx *op = crypto_tfm_ctx(tfm);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->__crt_alg);
+	struct sun4i_ss_alg_template *algt;
+
+	memset(op, 0, sizeof(struct sun4i_tfm_ctx));
+
+	algt = container_of(alg, struct sun4i_ss_alg_template, alg.hash);
+	op->ss = algt->ss;
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct sun4i_req_ctx));
+	return 0;
+}
+
+/* sun4i_hash_init: initialize request context */
+int sun4i_hash_init(struct ahash_request *areq)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sun4i_ss_alg_template *algt;
+
+	memset(op, 0, sizeof(struct sun4i_req_ctx));
+
+	algt = container_of(alg, struct sun4i_ss_alg_template, alg.hash);
+	op->mode = algt->mode;
+
+	return 0;
+}
+
+int sun4i_hash_export_md5(struct ahash_request *areq, void *out)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+	struct md5_state *octx = out;
+	int i;
+
+	octx->byte_count = op->byte_count + op->len;
+
+	memcpy(octx->block, op->buf, op->len);
+
+	if (op->byte_count > 0) {
+		for (i = 0; i < 4; i++)
+			octx->hash[i] = op->hash[i];
+	} else {
+		octx->hash[0] = SHA1_H0;
+		octx->hash[1] = SHA1_H1;
+		octx->hash[2] = SHA1_H2;
+		octx->hash[3] = SHA1_H3;
+	}
+
+	return 0;
+}
+
+int sun4i_hash_import_md5(struct ahash_request *areq, const void *in)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+	const struct md5_state *ictx = in;
+	int i;
+
+	sun4i_hash_init(areq);
+
+	op->byte_count = ictx->byte_count & ~0x3F;
+	op->len = ictx->byte_count & 0x3F;
+
+	memcpy(op->buf, ictx->block, op->len);
+
+	for (i = 0; i < 4; i++)
+		op->hash[i] = ictx->hash[i];
+
+	return 0;
+}
+
+int sun4i_hash_export_sha1(struct ahash_request *areq, void *out)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+	struct sha1_state *octx = out;
+	int i;
+
+	octx->count = op->byte_count + op->len;
+
+	memcpy(octx->buffer, op->buf, op->len);
+
+	if (op->byte_count > 0) {
+		for (i = 0; i < 5; i++)
+			octx->state[i] = op->hash[i];
+	} else {
+		octx->state[0] = SHA1_H0;
+		octx->state[1] = SHA1_H1;
+		octx->state[2] = SHA1_H2;
+		octx->state[3] = SHA1_H3;
+		octx->state[4] = SHA1_H4;
+	}
+
+	return 0;
+}
+
+int sun4i_hash_import_sha1(struct ahash_request *areq, const void *in)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+	const struct sha1_state *ictx = in;
+	int i;
+
+	sun4i_hash_init(areq);
+
+	op->byte_count = ictx->count & ~0x3F;
+	op->len = ictx->count & 0x3F;
+
+	memcpy(op->buf, ictx->buffer, op->len);
+
+	for (i = 0; i < 5; i++)
+		op->hash[i] = ictx->state[i];
+
+	return 0;
+}
+
+#define SS_HASH_UPDATE 1
+#define SS_HASH_FINAL 2
+
+/*
+ * sun4i_hash_update: update hash engine
+ *
+ * Could be used for both SHA1 and MD5
+ * Write data by step of 32bits and put then in the SS.
+ *
+ * Since we cannot leave partial data and hash state in the engine,
+ * we need to get the hash state at the end of this function.
+ * We can get the hash state every 64 bytes
+ *
+ * So the first work is to get the number of bytes to write to SS modulo 64
+ * The extra bytes will go to a temporary buffer op->buf storing op->len bytes
+ *
+ * So at the begin of update()
+ * if op->len + areq->nbytes < 64
+ * => all data will be written to wait buffer (op->buf) and end=0
+ * if not, write all data from op->buf to the device and position end to
+ * complete to 64bytes
+ *
+ * example 1:
+ * update1 60o => op->len=60
+ * update2 60o => need one more word to have 64 bytes
+ * end=4
+ * so write all data from op->buf and one word of SGs
+ * write remaining data in op->buf
+ * final state op->len=56
+ */
+static int sun4i_hash(struct ahash_request *areq)
+{
+	u32 v, ivmode = 0;
+	unsigned int i = 0;
+	/*
+	 * i is the total bytes read from SGs, to be compared to areq->nbytes
+	 * i is important because we cannot rely on SG length since the sum of
+	 * SG->length could be greater than areq->nbytes
+	 */
+
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sun4i_tfm_ctx *tfmctx = crypto_ahash_ctx(tfm);
+	struct sun4i_ss_ctx *ss = tfmctx->ss;
+	unsigned int in_i = 0; /* advancement in the current SG */
+	unsigned int end;
+	/*
+	 * end is the position when we need to stop writing to the device,
+	 * to be compared to i
+	 */
+	int in_r, err = 0;
+	unsigned int todo;
+	u32 spaces, rx_cnt = SS_RX_DEFAULT;
+	size_t copied = 0;
+	struct sg_mapping_iter mi;
+	unsigned int j = 0;
+	int zeros;
+	unsigned int index, padlen;
+	__be64 bits;
+	u32 bf[32];
+	u32 wb = 0;
+	unsigned int nwait, nbw = 0;
+	struct scatterlist *in_sg = areq->src;
+
+	dev_dbg(ss->dev, "%s %s bc=%llu len=%u mode=%x wl=%u h0=%0x",
+		__func__, crypto_tfm_alg_name(areq->base.tfm),
+		op->byte_count, areq->nbytes, op->mode,
+		op->len, op->hash[0]);
+
+	if (unlikely(areq->nbytes == 0) && (op->flags & SS_HASH_FINAL) == 0)
+		return 0;
+
+	/* protect against overflow */
+	if (unlikely(areq->nbytes > UINT_MAX - op->len)) {
+		dev_err(ss->dev, "Cannot process too large request\n");
+		return -EINVAL;
+	}
+
+	if (op->len + areq->nbytes < 64 && (op->flags & SS_HASH_FINAL) == 0) {
+		/* linearize data to op->buf */
+		copied = sg_pcopy_to_buffer(areq->src, sg_nents(areq->src),
+					    op->buf + op->len, areq->nbytes, 0);
+		op->len += copied;
+		return 0;
+	}
+
+	spin_lock_bh(&ss->slock);
+
+	/*
+	 * if some data have been processed before,
+	 * we need to restore the partial hash state
+	 */
+	if (op->byte_count > 0) {
+		ivmode = SS_IV_ARBITRARY;
+		for (i = 0; i < 5; i++)
+			writel(op->hash[i], ss->base + SS_IV0 + i * 4);
+	}
+	/* Enable the device */
+	writel(op->mode | SS_ENABLED | ivmode, ss->base + SS_CTL);
+
+	if ((op->flags & SS_HASH_UPDATE) == 0)
+		goto hash_final;
+
+	/* start of handling data */
+	if ((op->flags & SS_HASH_FINAL) == 0) {
+		end = ((areq->nbytes + op->len) / 64) * 64 - op->len;
+
+		if (end > areq->nbytes || areq->nbytes - end > 63) {
+			dev_err(ss->dev, "ERROR: Bound error %u %u\n",
+				end, areq->nbytes);
+			err = -EINVAL;
+			goto release_ss;
+		}
+	} else {
+		/* Since we have the flag final, we can go up to modulo 4 */
+		end = ((areq->nbytes + op->len) / 4) * 4 - op->len;
+	}
+
+	/* TODO if SGlen % 4 and op->len == 0 then DMA */
+	i = 1;
+	while (in_sg && i == 1) {
+		if ((in_sg->length % 4) != 0)
+			i = 0;
+		in_sg = sg_next(in_sg);
+	}
+	if (i == 1 && op->len == 0)
+		dev_dbg(ss->dev, "We can DMA\n");
+
+	i = 0;
+	sg_miter_start(&mi, areq->src, sg_nents(areq->src),
+		       SG_MITER_FROM_SG | SG_MITER_ATOMIC);
+	sg_miter_next(&mi);
+	in_i = 0;
+
+	do {
+		/*
+		 * we need to linearize in two case:
+		 * - the buffer is already used
+		 * - the SG does not have enough byte remaining ( < 4)
+		 */
+		if (op->len > 0 || (mi.length - in_i) < 4) {
+			/*
+			 * if we have entered here we have two reason to stop
+			 * - the buffer is full
+			 * - reach the end
+			 */
+			while (op->len < 64 && i < end) {
+				/* how many bytes we can read from current SG */
+				in_r = min3(mi.length - in_i, end - i,
+					    64 - op->len);
+				memcpy(op->buf + op->len, mi.addr + in_i, in_r);
+				op->len += in_r;
+				i += in_r;
+				in_i += in_r;
+				if (in_i == mi.length) {
+					sg_miter_next(&mi);
+					in_i = 0;
+				}
+			}
+			if (op->len > 3 && (op->len % 4) == 0) {
+				/* write buf to the device */
+				writesl(ss->base + SS_RXFIFO, op->buf,
+					op->len / 4);
+				op->byte_count += op->len;
+				op->len = 0;
+			}
+		}
+		if (mi.length - in_i > 3 && i < end) {
+			/* how many bytes we can read from current SG */
+			in_r = min3(mi.length - in_i, areq->nbytes - i,
+				    ((mi.length - in_i) / 4) * 4);
+			/* how many bytes we can write in the device*/
+			todo = min3((u32)(end - i) / 4, rx_cnt, (u32)in_r / 4);
+			writesl(ss->base + SS_RXFIFO, mi.addr + in_i, todo);
+			op->byte_count += todo * 4;
+			i += todo * 4;
+			in_i += todo * 4;
+			rx_cnt -= todo;
+			if (rx_cnt == 0) {
+				spaces = readl(ss->base + SS_FCSR);
+				rx_cnt = SS_RXFIFO_SPACES(spaces);
+			}
+			if (in_i == mi.length) {
+				sg_miter_next(&mi);
+				in_i = 0;
+			}
+		}
+	} while (i < end);
+
+	/*
+	 * Now we have written to the device all that we can,
+	 * store the remaining bytes in op->buf
+	 */
+	if ((areq->nbytes - i) < 64) {
+		while (i < areq->nbytes && in_i < mi.length && op->len < 64) {
+			/* how many bytes we can read from current SG */
+			in_r = min3(mi.length - in_i, areq->nbytes - i,
+				    64 - op->len);
+			memcpy(op->buf + op->len, mi.addr + in_i, in_r);
+			op->len += in_r;
+			i += in_r;
+			in_i += in_r;
+			if (in_i == mi.length) {
+				sg_miter_next(&mi);
+				in_i = 0;
+			}
+		}
+	}
+
+	sg_miter_stop(&mi);
+
+	/*
+	 * End of data process
+	 * Now if we have the flag final go to finalize part
+	 * If not, store the partial hash
+	 */
+	if ((op->flags & SS_HASH_FINAL) > 0)
+		goto hash_final;
+
+	writel(op->mode | SS_ENABLED | SS_DATA_END, ss->base + SS_CTL);
+	i = 0;
+	do {
+		v = readl(ss->base + SS_CTL);
+		i++;
+	} while (i < SS_TIMEOUT && (v & SS_DATA_END) > 0);
+	if (unlikely(i >= SS_TIMEOUT)) {
+		dev_err_ratelimited(ss->dev,
+				    "ERROR: hash end timeout %d>%d ctl=%x len=%u\n",
+				    i, SS_TIMEOUT, v, areq->nbytes);
+		err = -EIO;
+		goto release_ss;
+	}
+
+	for (i = 0; i < crypto_ahash_digestsize(tfm) / 4; i++)
+		op->hash[i] = readl(ss->base + SS_MD0 + i * 4);
+
+	goto release_ss;
+
+/*
+ * hash_final: finalize hashing operation
+ *
+ * If we have some remaining bytes, we write them.
+ * Then ask the SS for finalizing the hashing operation
+ *
+ * I do not check RX FIFO size in this function since the size is 32
+ * after each enabling and this function neither write more than 32 words.
+ * If we come from the update part, we cannot have more than
+ * 3 remaining bytes to write and SS is fast enough to not care about it.
+ */
+
+hash_final:
+
+	/* write the remaining words of the wait buffer */
+	if (op->len > 0) {
+		nwait = op->len / 4;
+		if (nwait > 0) {
+			writesl(ss->base + SS_RXFIFO, op->buf, nwait);
+			op->byte_count += 4 * nwait;
+		}
+		nbw = op->len - 4 * nwait;
+		wb = *(u32 *)(op->buf + nwait * 4);
+		wb &= (0xFFFFFFFF >> (4 - nbw) * 8);
+	}
+
+	/* write the remaining bytes of the nbw buffer */
+	if (nbw > 0) {
+		wb |= ((1 << 7) << (nbw * 8));
+		bf[j++] = wb;
+	} else {
+		bf[j++] = 1 << 7;
+	}
+
+	/*
+	 * number of space to pad to obtain 64o minus 8(size) minus 4 (final 1)
+	 * I take the operations from other MD5/SHA1 implementations
+	 */
+
+	/* we have already send 4 more byte of which nbw data */
+	if (op->mode == SS_OP_MD5) {
+		index = (op->byte_count + 4) & 0x3f;
+		op->byte_count += nbw;
+		if (index > 56)
+			zeros = (120 - index) / 4;
+		else
+			zeros = (56 - index) / 4;
+	} else {
+		op->byte_count += nbw;
+		index = op->byte_count & 0x3f;
+		padlen = (index < 56) ? (56 - index) : ((64 + 56) - index);
+		zeros = (padlen - 1) / 4;
+	}
+
+	memset(bf + j, 0, 4 * zeros);
+	j += zeros;
+
+	/* write the length of data */
+	if (op->mode == SS_OP_SHA1) {
+		bits = cpu_to_be64(op->byte_count << 3);
+		bf[j++] = bits & 0xffffffff;
+		bf[j++] = (bits >> 32) & 0xffffffff;
+	} else {
+		bf[j++] = (op->byte_count << 3) & 0xffffffff;
+		bf[j++] = (op->byte_count >> 29) & 0xffffffff;
+	}
+	writesl(ss->base + SS_RXFIFO, bf, j);
+
+	/* Tell the SS to stop the hashing */
+	writel(op->mode | SS_ENABLED | SS_DATA_END, ss->base + SS_CTL);
+
+	/*
+	 * Wait for SS to finish the hash.
+	 * The timeout could happen only in case of bad overclocking
+	 * or driver bug.
+	 */
+	i = 0;
+	do {
+		v = readl(ss->base + SS_CTL);
+		i++;
+	} while (i < SS_TIMEOUT && (v & SS_DATA_END) > 0);
+	if (unlikely(i >= SS_TIMEOUT)) {
+		dev_err_ratelimited(ss->dev,
+				    "ERROR: hash end timeout %d>%d ctl=%x len=%u\n",
+				    i, SS_TIMEOUT, v, areq->nbytes);
+		err = -EIO;
+		goto release_ss;
+	}
+
+	/* Get the hash from the device */
+	if (op->mode == SS_OP_SHA1) {
+		for (i = 0; i < 5; i++) {
+			v = cpu_to_be32(readl(ss->base + SS_MD0 + i * 4));
+			memcpy(areq->result + i * 4, &v, 4);
+		}
+	} else {
+		for (i = 0; i < 4; i++) {
+			v = readl(ss->base + SS_MD0 + i * 4);
+			memcpy(areq->result + i * 4, &v, 4);
+		}
+	}
+
+release_ss:
+	writel(0, ss->base + SS_CTL);
+	spin_unlock_bh(&ss->slock);
+	return err;
+}
+
+int sun4i_hash_final(struct ahash_request *areq)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+
+	op->flags = SS_HASH_FINAL;
+	return sun4i_hash(areq);
+}
+
+int sun4i_hash_update(struct ahash_request *areq)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+
+	op->flags = SS_HASH_UPDATE;
+	return sun4i_hash(areq);
+}
+
+/* sun4i_hash_finup: finalize hashing operation after an update */
+int sun4i_hash_finup(struct ahash_request *areq)
+{
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+
+	op->flags = SS_HASH_UPDATE | SS_HASH_FINAL;
+	return sun4i_hash(areq);
+}
+
+/* combo of init/update/final functions */
+int sun4i_hash_digest(struct ahash_request *areq)
+{
+	int err;
+	struct sun4i_req_ctx *op = ahash_request_ctx(areq);
+
+	err = sun4i_hash_init(areq);
+	if (err != 0)
+		return err;
+
+	op->flags = SS_HASH_UPDATE | SS_HASH_FINAL;
+	return sun4i_hash(areq);
+}
diff --git a/drivers/crypto/sunxi-ce/sun4i/sun4i-ss.h b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss.h
new file mode 100644
index 000000000..f04c0f8cf
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sun4i/sun4i-ss.h
@@ -0,0 +1,201 @@
+/*
+ * sun4i-ss.h - hardware cryptographic accelerator for Allwinner A20 SoC
+ *
+ * Copyright (C) 2013-2015 Corentin LABBE <clabbe.montjoie@gmail.com>
+ *
+ * Support AES cipher with 128,192,256 bits keysize.
+ * Support MD5 and SHA1 hash algorithms.
+ * Support DES and 3DES
+ *
+ * You could find the datasheet in Documentation/arm/sunxi/README
+ *
+ * Licensed under the GPL-2.
+ */
+
+#include <linux/clk.h>
+#include <linux/crypto.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <crypto/scatterwalk.h>
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <crypto/md5.h>
+#include <crypto/sha.h>
+#include <crypto/hash.h>
+#include <crypto/internal/hash.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/internal/rng.h>
+
+#define SS_CTL            0x00
+#define SS_KEY0           0x04
+#define SS_KEY1           0x08
+#define SS_KEY2           0x0C
+#define SS_KEY3           0x10
+#define SS_KEY4           0x14
+#define SS_KEY5           0x18
+#define SS_KEY6           0x1C
+#define SS_KEY7           0x20
+
+#define SS_IV0            0x24
+#define SS_IV1            0x28
+#define SS_IV2            0x2C
+#define SS_IV3            0x30
+
+#define SS_FCSR           0x44
+
+#define SS_MD0            0x4C
+#define SS_MD1            0x50
+#define SS_MD2            0x54
+#define SS_MD3            0x58
+#define SS_MD4            0x5C
+
+#define SS_RXFIFO         0x200
+#define SS_TXFIFO         0x204
+
+/* SS_CTL configuration values */
+
+/* PRNG generator mode - bit 15 */
+#define SS_PRNG_ONESHOT		(0 << 15)
+#define SS_PRNG_CONTINUE	(1 << 15)
+
+/* IV mode for hash */
+#define SS_IV_ARBITRARY		(1 << 14)
+
+/* SS operation mode - bits 12-13 */
+#define SS_ECB			(0 << 12)
+#define SS_CBC			(1 << 12)
+#define SS_CTS			(3 << 12)
+
+/* Counter width for CNT mode - bits 10-11 */
+#define SS_CNT_16BITS		(0 << 10)
+#define SS_CNT_32BITS		(1 << 10)
+#define SS_CNT_64BITS		(2 << 10)
+
+/* Key size for AES - bits 8-9 */
+#define SS_AES_128BITS		(0 << 8)
+#define SS_AES_192BITS		(1 << 8)
+#define SS_AES_256BITS		(2 << 8)
+
+/* Operation direction - bit 7 */
+#define SS_ENCRYPTION		(0 << 7)
+#define SS_DECRYPTION		(1 << 7)
+
+/* SS Method - bits 4-6 */
+#define SS_OP_AES		(0 << 4)
+#define SS_OP_DES		(1 << 4)
+#define SS_OP_3DES		(2 << 4)
+#define SS_OP_SHA1		(3 << 4)
+#define SS_OP_MD5		(4 << 4)
+#define SS_OP_PRNG		(5 << 4)
+
+/* Data end bit - bit 2 */
+#define SS_DATA_END		(1 << 2)
+
+/* PRNG start bit - bit 1 */
+#define SS_PRNG_START		(1 << 1)
+
+/* SS Enable bit - bit 0 */
+#define SS_DISABLED		(0 << 0)
+#define SS_ENABLED		(1 << 0)
+
+/* SS_FCSR configuration values */
+/* RX FIFO status - bit 30 */
+#define SS_RXFIFO_FREE		(1 << 30)
+
+/* RX FIFO empty spaces - bits 24-29 */
+#define SS_RXFIFO_SPACES(val)	(((val) >> 24) & 0x3f)
+
+/* TX FIFO status - bit 22 */
+#define SS_TXFIFO_AVAILABLE	(1 << 22)
+
+/* TX FIFO available spaces - bits 16-21 */
+#define SS_TXFIFO_SPACES(val)	(((val) >> 16) & 0x3f)
+
+#define SS_RX_MAX	32
+#define SS_RX_DEFAULT	SS_RX_MAX
+#define SS_TX_MAX	33
+
+#define SS_RXFIFO_EMP_INT_PENDING	(1 << 10)
+#define SS_TXFIFO_AVA_INT_PENDING	(1 << 8)
+#define SS_RXFIFO_EMP_INT_ENABLE	(1 << 2)
+#define SS_TXFIFO_AVA_INT_ENABLE	(1 << 0)
+
+struct sun4i_ss_ctx {
+	void __iomem *base;
+	int irq;
+	struct clk *busclk;
+	struct clk *ssclk;
+	struct reset_control *reset;
+	struct device *dev;
+	struct resource *res;
+	spinlock_t slock; /* control the use of the device */
+};
+
+struct sun4i_ss_alg_template {
+	u32 type;
+	u32 mode;
+	union {
+		struct crypto_alg crypto;
+		struct ahash_alg hash;
+	} alg;
+	struct sun4i_ss_ctx *ss;
+};
+
+struct sun4i_tfm_ctx {
+	u32 key[AES_MAX_KEY_SIZE / 4];/* divided by sizeof(u32) */
+	u32 keylen;
+	u32 keymode;
+	struct sun4i_ss_ctx *ss;
+};
+
+struct sun4i_cipher_req_ctx {
+	u32 mode;
+};
+
+struct sun4i_req_ctx {
+	u32 mode;
+	u64 byte_count; /* number of bytes "uploaded" to the device */
+	u32 hash[5]; /* for storing SS_IVx register */
+	char buf[64];
+	unsigned int len;
+	int flags;
+};
+
+int sun4i_hash_crainit(struct crypto_tfm *tfm);
+int sun4i_hash_init(struct ahash_request *areq);
+int sun4i_hash_update(struct ahash_request *areq);
+int sun4i_hash_final(struct ahash_request *areq);
+int sun4i_hash_finup(struct ahash_request *areq);
+int sun4i_hash_digest(struct ahash_request *areq);
+int sun4i_hash_export_md5(struct ahash_request *areq, void *out);
+int sun4i_hash_import_md5(struct ahash_request *areq, const void *in);
+int sun4i_hash_export_sha1(struct ahash_request *areq, void *out);
+int sun4i_hash_import_sha1(struct ahash_request *areq, const void *in);
+
+int sun4i_ss_cbc_aes_encrypt(struct ablkcipher_request *areq);
+int sun4i_ss_cbc_aes_decrypt(struct ablkcipher_request *areq);
+int sun4i_ss_ecb_aes_encrypt(struct ablkcipher_request *areq);
+int sun4i_ss_ecb_aes_decrypt(struct ablkcipher_request *areq);
+
+int sun4i_ss_cbc_des_encrypt(struct ablkcipher_request *areq);
+int sun4i_ss_cbc_des_decrypt(struct ablkcipher_request *areq);
+int sun4i_ss_ecb_des_encrypt(struct ablkcipher_request *areq);
+int sun4i_ss_ecb_des_decrypt(struct ablkcipher_request *areq);
+
+int sun4i_ss_cbc_des3_encrypt(struct ablkcipher_request *areq);
+int sun4i_ss_cbc_des3_decrypt(struct ablkcipher_request *areq);
+int sun4i_ss_ecb_des3_encrypt(struct ablkcipher_request *areq);
+int sun4i_ss_ecb_des3_decrypt(struct ablkcipher_request *areq);
+
+int sun4i_ss_cipher_init(struct crypto_tfm *tfm);
+int sun4i_ss_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			unsigned int keylen);
+int sun4i_ss_des_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			unsigned int keylen);
+int sun4i_ss_des3_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			 unsigned int keylen);
diff --git a/drivers/crypto/sunxi-ce/sunxi_ce.c b/drivers/crypto/sunxi-ce/sunxi_ce.c
new file mode 100644
index 000000000..4c7d292db
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sunxi_ce.c
@@ -0,0 +1,1527 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/clk.h>
+#include <linux/clk/sunxi.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/dmapool.h>
+#include <crypto/hash.h>
+#include <crypto/md5.h>
+#include <crypto/des.h>
+#include <crypto/internal/aead.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/reset.h>
+#include "sunxi_ce.h"
+#include "sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+#ifdef CONFIG_OF
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+
+static const struct of_device_id sunxi_ss_of_match[] = {
+		{.compatible = "allwinner,sunxi-ce",},
+		{},
+};
+MODULE_DEVICE_TABLE(of, sunxi_ss_of_match);
+#endif
+
+sunxi_ss_t *ss_dev;
+
+static DEFINE_MUTEX(ss_lock);
+
+void ss_dev_lock(void)
+{
+	mutex_lock(&ss_lock);
+}
+
+void ss_dev_unlock(void)
+{
+	mutex_unlock(&ss_lock);
+}
+
+void __iomem *ss_membase(void)
+{
+	return ss_dev->base_addr;
+}
+
+void ss_reset(void)
+{
+	SS_ENTER();
+	reset_control_assert(ss_dev->reset);
+	reset_control_deassert(ss_dev->reset);
+}
+
+#ifdef SS_RSA_CLK_ENABLE
+void ss_clk_set(u32 rate)
+{
+#ifdef CONFIG_EVB_PLATFORM
+	int ret = 0;
+
+	ret = clk_get_rate(ss_dev->ce_clk);
+	if (ret == rate)
+		return;
+
+	SS_DBG("Change the SS clk to %d MHz.\n", rate/1000000);
+	ret = clk_set_rate(ss_dev->ce_clk, rate);
+	if (ret != 0)
+		SS_ERR("clk_set_rate(%d) failed! return %d\n", rate, ret);
+#endif
+}
+#endif
+
+static int ss_aes_key_is_weak(const u8 *key, unsigned int keylen)
+{
+	s32 i;
+	u8 tmp = key[0];
+
+	for (i = 0; i < keylen; i++)
+		if (tmp != key[i])
+			return 0;
+
+	SS_ERR("The key is weak!\n");
+	return 1;
+}
+
+#ifdef SS_GCM_MODE_ENABLE
+static int sunxi_aes_gcm_setkey(struct crypto_aead *tfm, const u8 *key,
+				unsigned int keylen)
+{
+	ss_aead_ctx_t *ctx = crypto_aead_ctx(tfm);
+
+	if (keylen != AES_KEYSIZE_256 &&
+	    keylen != AES_KEYSIZE_192 &&
+	    keylen != AES_KEYSIZE_128) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+
+	memcpy(ctx->key, key, keylen);
+	ctx->key_size = keylen;
+
+	return 0;
+}
+
+static int sunxi_aes_gcm_setauthsize(struct crypto_aead *tfm,
+				     unsigned int authsize)
+{
+	/* Same as crypto_gcm_authsize() from crypto/gcm.c */
+	switch (authsize) {
+	case 4:
+	case 8:
+	case 12:
+	case 13:
+	case 14:
+	case 15:
+	case 16:
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+#endif
+
+static int ss_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+				unsigned int keylen)
+{
+	int ret = 0;
+	ss_aes_ctx_t *ctx = crypto_ablkcipher_ctx(tfm);
+
+	SS_DBG("keylen = %d\n", keylen);
+	if (ctx->comm.flags & SS_FLAG_NEW_KEY) {
+		SS_ERR("The key has already update.\n");
+		return -EBUSY;
+	}
+
+	ret = ss_aes_key_valid(tfm, keylen);
+	if (ret != 0)
+		return ret;
+
+	if (ss_aes_key_is_weak(key, keylen)) {
+		crypto_ablkcipher_tfm(tfm)->crt_flags
+					|= CRYPTO_TFM_REQ_FORBID_WEAK_KEYS;
+		/* testmgr.c need this, but we don't want to support it. */
+/*		return -EINVAL; */
+	}
+
+	ctx->key_size = keylen;
+	memcpy(ctx->key, key, keylen);
+	if (keylen < AES_KEYSIZE_256)
+		memset(&ctx->key[keylen], 0, AES_KEYSIZE_256 - keylen);
+
+	ctx->comm.flags |= SS_FLAG_NEW_KEY;
+	return 0;
+}
+
+static int ss_aes_ecb_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_ECB);
+}
+
+static int ss_aes_ecb_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_ECB);
+}
+
+static int ss_aes_cbc_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CBC);
+}
+
+static int ss_aes_cbc_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CBC);
+}
+
+#ifdef SS_CTR_MODE_ENABLE
+static int ss_aes_ctr_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CTR);
+}
+
+static int ss_aes_ctr_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CTR);
+}
+#endif
+
+#ifdef SS_CTS_MODE_ENABLE
+static int ss_aes_cts_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CTS);
+}
+
+static int ss_aes_cts_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CTS);
+}
+#endif
+
+#ifdef SS_XTS_MODE_ENABLE
+static int ss_aes_xts_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+			SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_XTS);
+}
+
+static int ss_aes_xts_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+			SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_XTS);
+}
+#endif
+
+#ifdef SS_OFB_MODE_ENABLE
+static int ss_aes_ofb_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_OFB);
+}
+
+static int ss_aes_ofb_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_OFB);
+}
+#endif
+
+#ifdef SS_CFB_MODE_ENABLE
+static int ss_aes_cfb1_encrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 1;
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb1_decrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 1;
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb8_encrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 8;
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb8_decrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 8;
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb64_encrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 64;
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb64_decrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 64;
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb128_encrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 128;
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+
+static int ss_aes_cfb128_decrypt(struct ablkcipher_request *req)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	req_ctx->bitwidth = 128;
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_CFB);
+}
+#endif
+
+#ifdef SS_GCM_MODE_ENABLE
+static int sunxi_aes_gcm_encrypt(struct aead_request *req)
+{
+	return ss_aead_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_AES, SS_AES_MODE_GCM);
+}
+
+static int sunxi_aes_gcm_decrypt(struct aead_request *req)
+{
+	return ss_aead_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_AES, SS_AES_MODE_GCM);
+}
+#endif
+
+static int ss_des_ecb_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_DES, SS_AES_MODE_ECB);
+}
+
+static int ss_des_ecb_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_DES, SS_AES_MODE_ECB);
+}
+
+static int ss_des_cbc_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_DES, SS_AES_MODE_CBC);
+}
+
+static int ss_des_cbc_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_DES, SS_AES_MODE_CBC);
+}
+
+static int ss_des3_ecb_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_3DES, SS_AES_MODE_ECB);
+}
+
+static int ss_des3_ecb_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_3DES, SS_AES_MODE_ECB);
+}
+
+static int ss_des3_cbc_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_3DES, SS_AES_MODE_CBC);
+}
+
+static int ss_des3_cbc_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_DECRYPT, SS_METHOD_3DES, SS_AES_MODE_CBC);
+}
+
+#ifdef SS_RSA_ENABLE
+static int ss_rsa_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_ENCRYPT,
+		SS_METHOD_RSA, CE_RSA_OP_M_EXP);
+}
+
+static int ss_rsa_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_DECRYPT,
+		SS_METHOD_RSA, CE_RSA_OP_M_EXP);
+}
+#endif
+
+#ifdef SS_DH_ENABLE
+static int ss_dh_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_ENCRYPT,
+		SS_METHOD_DH, CE_RSA_OP_M_EXP);
+}
+
+static int ss_dh_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_DECRYPT,
+		SS_METHOD_DH, CE_RSA_OP_M_EXP);
+}
+#endif
+
+#ifdef SS_ECC_ENABLE
+static int ss_ecdh_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_ENCRYPT, SS_METHOD_ECC,
+				CE_ECC_OP_POINT_MUL);
+}
+
+static int ss_ecdh_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_DECRYPT, SS_METHOD_ECC,
+				CE_ECC_OP_POINT_MUL);
+}
+
+static int ss_ecc_sign_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_ENCRYPT, SS_METHOD_ECC, CE_ECC_OP_SIGN);
+}
+
+static int ss_ecc_sign_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_DECRYPT, SS_METHOD_ECC, CE_ECC_OP_SIGN);
+}
+
+static int ss_ecc_verify_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_ENCRYPT, SS_METHOD_RSA,
+				CE_RSA_OP_M_MUL);
+}
+
+static int ss_ecc_verify_decrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req, SS_DIR_DECRYPT, SS_METHOD_RSA,
+				CE_RSA_OP_M_MUL);
+}
+
+#endif
+
+#ifdef SS_HMAC_SHA1_ENABLE
+static int ss_hmac_sha1_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_HMAC_SHA1, SS_AES_MODE_ECB);
+}
+#endif
+
+#ifdef SS_HMAC_SHA256_ENABLE
+static int ss_hmac_sha256_encrypt(struct ablkcipher_request *req)
+{
+	return ss_aes_crypt(req,
+		SS_DIR_ENCRYPT, SS_METHOD_HMAC_SHA256, SS_AES_MODE_ECB);
+}
+#endif
+
+int ss_rng_reset(struct crypto_rng *tfm, const u8 *seed, u32 slen)
+{
+	int len = slen > SS_PRNG_SEED_LEN ? SS_PRNG_SEED_LEN : slen;
+	ss_aes_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("Seed len: %d/%d, flags = %#x\n", len, slen, ctx->comm.flags);
+	ctx->key_size = len;
+	memset(ctx->key, 0, SS_PRNG_SEED_LEN);
+	memcpy(ctx->key, seed, len);
+	ctx->comm.flags |= SS_FLAG_NEW_KEY;
+
+	return 0;
+}
+
+#ifdef SS_DRBG_MODE_ENABLE
+int ss_drbg_reset(struct crypto_rng *tfm, const u8 *seed, u32 slen)
+{
+	int len = slen > SS_PRNG_SEED_LEN ? SS_PRNG_SEED_LEN : slen;
+	ss_drbg_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("Seed len: %d/%d, flags = %#x\n", len, slen, ctx->comm.flags);
+	ctx->person_size = len;
+	memset(ctx->person, 0, SS_PRNG_SEED_LEN);
+	memcpy(ctx->person, seed, slen);
+	ctx->comm.flags |= SS_FLAG_NEW_KEY;
+
+	return 0;
+}
+
+void ss_drbg_set_ent(struct crypto_rng *tfm, const u8 *entropy, u32 entropy_len)
+{
+	int len = entropy_len > SS_PRNG_SEED_LEN ? SS_PRNG_SEED_LEN : entropy_len;
+	ss_drbg_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("Seed len: %d / %d, flags = %#x\n", len, entropy_len, ctx->comm.flags);
+	ctx->entropt_size = entropy_len;
+	memset(ctx->entropt, 0, SS_PRNG_SEED_LEN);
+	memcpy(ctx->entropt, entropy, len);
+	ctx->comm.flags |= SS_FLAG_NEW_KEY;
+}
+#endif
+
+int ss_flow_request(ss_comm_ctx_t *comm)
+{
+	int i;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&ss_dev->lock, flags);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (ss_dev->flows[i].available == SS_FLOW_AVAILABLE) {
+			comm->flow = i;
+			ss_dev->flows[i].available = SS_FLOW_UNAVAILABLE;
+			SS_DBG("The flow %d is available.\n", i);
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&ss_dev->lock, flags);
+
+	if (i == SS_FLOW_NUM) {
+		SS_ERR("Failed to get an available flow.\n");
+		i = -1;
+	}
+	return i;
+}
+
+void ss_flow_release(ss_comm_ctx_t *comm)
+{
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&ss_dev->lock, flags);
+	ss_dev->flows[comm->flow].available = SS_FLOW_AVAILABLE;
+	spin_unlock_irqrestore(&ss_dev->lock, flags);
+}
+
+#ifdef SS_GCM_MODE_ENABLE
+static int sunxi_aes_gcm_init(struct crypto_aead *tfm)
+{
+	if (ss_flow_request(crypto_aead_ctx(tfm)) < 0)
+		return -1;
+
+	crypto_aead_set_reqsize(tfm, sizeof(ss_aes_req_ctx_t));
+	SS_DBG("reqsize = %d\n", tfm->reqsize);
+
+	return 0;
+}
+
+static void sunxi_aes_gcm_exit(struct crypto_aead *tfm)
+{
+	SS_ENTER();
+	ss_flow_release(crypto_aead_ctx(tfm));
+	/* sun8iw6 and sun9iw1 need reset SS controller after each operation. */
+#ifdef SS_IDMA_ENABLE
+	ss_reset();
+#endif
+
+}
+#endif
+
+static int sunxi_ss_cra_init(struct crypto_tfm *tfm)
+{
+	if (ss_flow_request(crypto_tfm_ctx(tfm)) < 0)
+		return -1;
+
+	tfm->crt_ablkcipher.reqsize = sizeof(ss_aes_req_ctx_t);
+	SS_DBG("reqsize = %d\n", tfm->crt_u.ablkcipher.reqsize);
+	return 0;
+}
+
+static int sunxi_ss_cra_rng_init(struct crypto_tfm *tfm)
+{
+	if (ss_flow_request(crypto_tfm_ctx(tfm)) < 0)
+		return -1;
+
+	return 0;
+}
+
+static int sunxi_ss_cra_hash_init(struct crypto_tfm *tfm)
+{
+	if (ss_flow_request(crypto_tfm_ctx(tfm)) < 0)
+		return -1;
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(ss_aes_req_ctx_t));
+
+	SS_DBG("reqsize = %zu\n", sizeof(ss_aes_req_ctx_t));
+	return 0;
+}
+
+static void sunxi_ss_cra_exit(struct crypto_tfm *tfm)
+{
+	SS_ENTER();
+	ss_flow_release(crypto_tfm_ctx(tfm));
+	/* sun8iw6 and sun9iw1 need reset SS controller after each operation. */
+#ifdef SS_IDMA_ENABLE
+	ss_reset();
+#endif
+}
+
+static int ss_hash_init(struct ahash_request *req, int type, int size, char *iv)
+{
+	ss_aes_req_ctx_t *req_ctx = ahash_request_ctx(req);
+	ss_hash_ctx_t *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));
+
+	SS_DBG("Method: %d\n", type);
+
+	memset(req_ctx, 0, sizeof(ss_aes_req_ctx_t));
+	req_ctx->type = type;
+
+	ctx->md_size = size;
+	memcpy(ctx->md, iv, size);
+
+	ctx->cnt = 0;
+	memset(ctx->pad, 0, SS_HASH_PAD_SIZE);
+	return 0;
+}
+
+static int ss_md5_init(struct ahash_request *req)
+{
+	int iv[MD5_DIGEST_SIZE/4] = {SHA1_H0, SHA1_H1, SHA1_H2, SHA1_H3};
+
+	return ss_hash_init(req, SS_METHOD_MD5, MD5_DIGEST_SIZE, (char *)iv);
+}
+
+static int ss_sha1_init(struct ahash_request *req)
+{
+	int iv[SHA1_DIGEST_SIZE/4] = {
+			SHA1_H0, SHA1_H1, SHA1_H2, SHA1_H3, SHA1_H4};
+
+#ifdef SS_SHA_SWAP_PRE_ENABLE
+#ifdef SS_SHA_NO_SWAP_IV4
+	ss_hash_swap((char *)iv, SHA1_DIGEST_SIZE - 4);
+#else
+	ss_hash_swap((char *)iv, SHA1_DIGEST_SIZE);
+#endif
+#endif
+
+	return ss_hash_init(req, SS_METHOD_SHA1, SHA1_DIGEST_SIZE, (char *)iv);
+}
+
+#ifdef SS_SHA224_ENABLE
+static int ss_sha224_init(struct ahash_request *req)
+{
+	int iv[SHA256_DIGEST_SIZE/4] = {
+			SHA224_H0, SHA224_H1, SHA224_H2, SHA224_H3,
+			SHA224_H4, SHA224_H5, SHA224_H6, SHA224_H7};
+
+#ifdef SS_SHA_SWAP_PRE_ENABLE
+	ss_hash_swap((char *)iv, SHA256_DIGEST_SIZE);
+#endif
+
+	return ss_hash_init(req,
+		SS_METHOD_SHA224, SHA256_DIGEST_SIZE, (char *)iv);
+}
+#endif
+
+#ifdef SS_SHA256_ENABLE
+static int ss_sha256_init(struct ahash_request *req)
+{
+	int iv[SHA256_DIGEST_SIZE/4] = {
+			SHA256_H0, SHA256_H1, SHA256_H2, SHA256_H3,
+			SHA256_H4, SHA256_H5, SHA256_H6, SHA256_H7};
+
+#ifdef SS_SHA_SWAP_PRE_ENABLE
+	ss_hash_swap((char *)iv, SHA256_DIGEST_SIZE);
+#endif
+
+	return ss_hash_init(req,
+			SS_METHOD_SHA256, SHA256_DIGEST_SIZE, (char *)iv);
+}
+#endif
+
+#define GET_U64_HIGH(data64)	(int)(data64 >> 32)
+#define GET_U64_LOW(data64)		(int)(data64 & 0xFFFFFFFF)
+
+#ifdef SS_SHA384_ENABLE
+static int ss_sha384_init(struct ahash_request *req)
+{
+	int iv[SHA512_DIGEST_SIZE/4] = {
+			GET_U64_HIGH(SHA384_H0), GET_U64_LOW(SHA384_H0),
+			GET_U64_HIGH(SHA384_H1), GET_U64_LOW(SHA384_H1),
+			GET_U64_HIGH(SHA384_H2), GET_U64_LOW(SHA384_H2),
+			GET_U64_HIGH(SHA384_H3), GET_U64_LOW(SHA384_H3),
+			GET_U64_HIGH(SHA384_H4), GET_U64_LOW(SHA384_H4),
+			GET_U64_HIGH(SHA384_H5), GET_U64_LOW(SHA384_H5),
+			GET_U64_HIGH(SHA384_H6), GET_U64_LOW(SHA384_H6),
+			GET_U64_HIGH(SHA384_H7), GET_U64_LOW(SHA384_H7)};
+
+#ifdef SS_SHA_SWAP_PRE_ENABLE
+	ss_hash_swap((char *)iv, SHA512_DIGEST_SIZE);
+#endif
+
+	return ss_hash_init(req,
+			SS_METHOD_SHA384, SHA512_DIGEST_SIZE, (char *)iv);
+}
+#endif
+
+#ifdef SS_SHA512_ENABLE
+static int ss_sha512_init(struct ahash_request *req)
+{
+	int iv[SHA512_DIGEST_SIZE/4] = {
+			GET_U64_HIGH(SHA512_H0), GET_U64_LOW(SHA512_H0),
+			GET_U64_HIGH(SHA512_H1), GET_U64_LOW(SHA512_H1),
+			GET_U64_HIGH(SHA512_H2), GET_U64_LOW(SHA512_H2),
+			GET_U64_HIGH(SHA512_H3), GET_U64_LOW(SHA512_H3),
+			GET_U64_HIGH(SHA512_H4), GET_U64_LOW(SHA512_H4),
+			GET_U64_HIGH(SHA512_H5), GET_U64_LOW(SHA512_H5),
+			GET_U64_HIGH(SHA512_H6), GET_U64_LOW(SHA512_H6),
+			GET_U64_HIGH(SHA512_H7), GET_U64_LOW(SHA512_H7)};
+
+#ifdef SS_SHA_SWAP_PRE_ENABLE
+	ss_hash_swap((char *)iv, SHA512_DIGEST_SIZE);
+#endif
+
+	return ss_hash_init(req,
+			SS_METHOD_SHA512, SHA512_DIGEST_SIZE, (char *)iv);
+}
+#endif
+
+#define DES_MIN_KEY_SIZE	DES_KEY_SIZE
+#define DES_MAX_KEY_SIZE	DES_KEY_SIZE
+#define DES3_MIN_KEY_SIZE	DES3_EDE_KEY_SIZE
+#define DES3_MAX_KEY_SIZE	DES3_EDE_KEY_SIZE
+
+#define DECLARE_SS_AES_ALG(utype, ltype, lmode, block_size, iv_size) \
+{ \
+	.cra_name	 = #lmode"("#ltype")", \
+	.cra_driver_name = "ss-"#lmode"-"#ltype, \
+	.cra_flags	 = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC, \
+	.cra_type	 = &crypto_ablkcipher_type, \
+	.cra_blocksize	 = block_size, \
+	.cra_alignmask	 = 3, \
+	.cra_u.ablkcipher = { \
+		.setkey      = ss_aes_setkey, \
+		.encrypt     = ss_##ltype##_##lmode##_encrypt, \
+		.decrypt     = ss_##ltype##_##lmode##_decrypt, \
+		.min_keysize = utype##_MIN_KEY_SIZE, \
+		.max_keysize = utype##_MAX_KEY_SIZE, \
+		.ivsize	     = iv_size, \
+	} \
+}
+
+#ifdef SS_XTS_MODE_ENABLE
+#define DECLARE_SS_AES_XTS_ALG(utype, ltype, lmode, block_size, iv_size) \
+{ \
+	.cra_name	 = #lmode"("#ltype")", \
+	.cra_driver_name = "ss-"#lmode"-"#ltype, \
+	.cra_flags	 = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC, \
+	.cra_type	 = &crypto_ablkcipher_type, \
+	.cra_blocksize	 = block_size, \
+	.cra_alignmask	 = 3, \
+	.cra_u.ablkcipher = { \
+		.setkey      = ss_aes_setkey, \
+		.encrypt     = ss_##ltype##_##lmode##_encrypt, \
+		.decrypt     = ss_##ltype##_##lmode##_decrypt, \
+		.min_keysize = utype##_MAX_KEY_SIZE, \
+		.max_keysize = utype##_MAX_KEY_SIZE * 2, \
+		.ivsize	     = iv_size, \
+	} \
+}
+#endif
+
+#define DECLARE_SS_ASYM_ALG(type, bitwidth, key_size, iv_size) \
+{ \
+	.cra_name	 = #type"("#bitwidth")", \
+	.cra_driver_name = "ss-"#type"-"#bitwidth, \
+	.cra_flags	 = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC, \
+	.cra_type	 = &crypto_ablkcipher_type, \
+	.cra_blocksize	 = key_size%AES_BLOCK_SIZE == 0 ? AES_BLOCK_SIZE : 4, \
+	.cra_alignmask	 = key_size%AES_BLOCK_SIZE == 0 ? 31 : 3, \
+	.cra_u.ablkcipher = { \
+		.setkey      = ss_aes_setkey, \
+		.encrypt     = ss_##type##_encrypt, \
+		.decrypt     = ss_##type##_decrypt, \
+		.min_keysize = key_size, \
+		.max_keysize = key_size, \
+		.ivsize      = iv_size, \
+	}, \
+}
+#ifndef SS_SUPPORT_CE_V3_2
+#define DECLARE_SS_RSA_ALG(type, bitwidth) \
+		DECLARE_SS_ASYM_ALG(type, bitwidth, (bitwidth/8), (bitwidth/8))
+#else
+#define DECLARE_SS_RSA_ALG(type, bitwidth) \
+		DECLARE_SS_ASYM_ALG(type, bitwidth, (bitwidth/8), 0)
+#endif
+#define DECLARE_SS_DH_ALG(type, bitwidth) DECLARE_SS_RSA_ALG(type, bitwidth)
+
+#ifdef SS_GCM_MODE_ENABLE
+static struct aead_alg sunxi_aes_gcm_alg = {
+	.setkey		= sunxi_aes_gcm_setkey,
+	.setauthsize	= sunxi_aes_gcm_setauthsize,
+	.encrypt	= sunxi_aes_gcm_encrypt,
+	.decrypt	= sunxi_aes_gcm_decrypt,
+	.init		= sunxi_aes_gcm_init,
+	.exit		= sunxi_aes_gcm_exit,
+	.ivsize		= AES_MIN_KEY_SIZE,
+	.maxauthsize	= AES_BLOCK_SIZE,
+
+	.base = {
+		.cra_name		= "gcm(aes)",
+		.cra_driver_name	= "ss-gcm-aes",
+		.cra_priority		= SS_ALG_PRIORITY,
+		.cra_flags		= CRYPTO_ALG_ASYNC,
+		.cra_blocksize		= AES_BLOCK_SIZE,
+		.cra_ctxsize		= sizeof(ss_aead_ctx_t),
+		.cra_alignmask		= 31,
+		.cra_module		= THIS_MODULE,
+	},
+};
+#endif
+
+static struct crypto_alg sunxi_ss_algs[] = {
+	DECLARE_SS_AES_ALG(AES, aes, ecb, AES_BLOCK_SIZE, 0),
+	DECLARE_SS_AES_ALG(AES, aes, cbc, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+#ifdef SS_CTR_MODE_ENABLE
+	DECLARE_SS_AES_ALG(AES, aes, ctr, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+#endif
+#ifdef SS_CTS_MODE_ENABLE
+	DECLARE_SS_AES_ALG(AES, aes, cts, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+#endif
+#ifdef SS_XTS_MODE_ENABLE
+	DECLARE_SS_AES_XTS_ALG(AES, aes, xts, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+#endif
+#ifdef SS_OFB_MODE_ENABLE
+	DECLARE_SS_AES_ALG(AES, aes, ofb, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+#endif
+#ifdef SS_CFB_MODE_ENABLE
+	DECLARE_SS_AES_ALG(AES, aes, cfb1, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+	DECLARE_SS_AES_ALG(AES, aes, cfb8, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+	DECLARE_SS_AES_ALG(AES, aes, cfb64, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+	DECLARE_SS_AES_ALG(AES, aes, cfb128, AES_BLOCK_SIZE, AES_MIN_KEY_SIZE),
+#endif
+	DECLARE_SS_AES_ALG(DES, des, ecb, DES_BLOCK_SIZE, 0),
+	DECLARE_SS_AES_ALG(DES, des, cbc, DES_BLOCK_SIZE, DES_KEY_SIZE),
+	DECLARE_SS_AES_ALG(DES3, des3, ecb, DES3_EDE_BLOCK_SIZE, 0),
+	DECLARE_SS_AES_ALG(DES3, des3, cbc, DES3_EDE_BLOCK_SIZE, DES_KEY_SIZE),
+#ifdef SS_RSA512_ENABLE
+	DECLARE_SS_RSA_ALG(rsa, 512),
+#endif
+#ifdef SS_RSA1024_ENABLE
+	DECLARE_SS_RSA_ALG(rsa, 1024),
+#endif
+#ifdef SS_RSA2048_ENABLE
+	DECLARE_SS_RSA_ALG(rsa, 2048),
+#endif
+#ifdef SS_RSA3072_ENABLE
+	DECLARE_SS_RSA_ALG(rsa, 3072),
+#endif
+#ifdef SS_RSA4096_ENABLE
+	DECLARE_SS_RSA_ALG(rsa, 4096),
+#endif
+#ifdef SS_DH512_ENABLE
+	DECLARE_SS_DH_ALG(dh, 512),
+#endif
+#ifdef SS_DH1024_ENABLE
+	DECLARE_SS_DH_ALG(dh, 1024),
+#endif
+#ifdef SS_DH2048_ENABLE
+	DECLARE_SS_DH_ALG(dh, 2048),
+#endif
+#ifdef SS_DH3072_ENABLE
+	DECLARE_SS_DH_ALG(dh, 3072),
+#endif
+#ifdef SS_DH4096_ENABLE
+	DECLARE_SS_DH_ALG(dh, 4096),
+#endif
+#ifdef SS_ECC_ENABLE
+#ifndef SS_SUPPORT_CE_V3_2
+	DECLARE_SS_ASYM_ALG(ecdh, 160, 160/8, 160/8),
+	DECLARE_SS_ASYM_ALG(ecdh, 224, 224/8, 224/8),
+	DECLARE_SS_ASYM_ALG(ecdh, 256, 256/8, 256/8),
+	DECLARE_SS_ASYM_ALG(ecdh, 521, ((521+31)/32)*4, ((521+31)/32)*4),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 160, 160/8, (160/8)*2),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 224, 224/8, (224/8)*2),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 256, 256/8, (256/8)*2),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 521, ((521+31)/32)*4, ((521+31)/32)*4*2),
+#else
+	DECLARE_SS_ASYM_ALG(ecdh, 160, 160/8, 0),
+	DECLARE_SS_ASYM_ALG(ecdh, 224, 224/8, 0),
+	DECLARE_SS_ASYM_ALG(ecdh, 256, 256/8, 0),
+	DECLARE_SS_ASYM_ALG(ecdh, 521, ((521+31)/32)*4, 0),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 160, 160/8, 0),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 224, 224/8, 0),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 256, 256/8, 0),
+	DECLARE_SS_ASYM_ALG(ecc_sign, 521, ((521+31)/32)*4, 0),
+#endif
+	DECLARE_SS_RSA_ALG(ecc_verify, 512),
+	DECLARE_SS_RSA_ALG(ecc_verify, 1024),
+#endif
+
+#ifdef SS_HMAC_SHA1_ENABLE
+	{
+		.cra_name	 = "hmac-sha1",
+		.cra_driver_name = "ss-hmac-sha1",
+		.cra_flags	= CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+		.cra_type	 = &crypto_ablkcipher_type,
+		.cra_blocksize	 = 4,
+		.cra_alignmask	 = 3,
+		.cra_u.ablkcipher = {
+			.setkey	     = ss_aes_setkey,
+			.encrypt     = ss_hmac_sha1_encrypt,
+			.decrypt     = NULL,
+			.min_keysize = SHA1_BLOCK_SIZE,
+			.max_keysize = SHA1_BLOCK_SIZE,
+			.ivsize	     = 0,
+		}
+	},
+#endif
+#ifdef SS_HMAC_SHA256_ENABLE
+	{
+		.cra_name	 = "hmac-sha256",
+		.cra_driver_name = "ss-hmac-sha256",
+		.cra_flags	 = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+		.cra_type	 = &crypto_ablkcipher_type,
+		.cra_blocksize	 = 4,
+		.cra_alignmask	 = 3,
+		.cra_u.ablkcipher = {
+			.setkey      = ss_aes_setkey,
+			.encrypt     = ss_hmac_sha256_encrypt,
+			.decrypt     = NULL,
+			.min_keysize = SHA256_BLOCK_SIZE,
+			.max_keysize = SHA256_BLOCK_SIZE,
+			.ivsize	     = 0,
+		}
+	},
+#endif
+};
+
+#define DECLARE_SS_RNG_ALG(ltype) \
+{ \
+	.generate = ss_##ltype##_get_random, \
+	.seed	  = ss_rng_reset, \
+	.seedsize = SS_SEED_SIZE, \
+	.base	  = { \
+		.cra_name = #ltype, \
+		.cra_driver_name = "ss-"#ltype, \
+		.cra_flags = CRYPTO_ALG_TYPE_RNG, \
+		.cra_priority = SS_ALG_PRIORITY, \
+		.cra_ctxsize = sizeof(ss_aes_ctx_t), \
+		.cra_module = THIS_MODULE, \
+		.cra_init = sunxi_ss_cra_rng_init, \
+		.cra_exit = sunxi_ss_cra_exit, \
+	} \
+}
+
+#ifdef SS_DRBG_MODE_ENABLE
+#define DECLARE_SS_DRBG_ALG(ltype) \
+{ \
+	.generate = ss_drbg_##ltype##_get_random, \
+	.seed	  = ss_drbg_reset, \
+	.set_ent	= ss_drbg_set_ent, \
+	.seedsize = SS_SEED_SIZE, \
+	.base	  = { \
+		.cra_name = "drbg-"#ltype, \
+		.cra_driver_name = "ss-drbg-"#ltype, \
+		.cra_flags = CRYPTO_ALG_TYPE_RNG, \
+		.cra_priority = SS_ALG_PRIORITY, \
+		.cra_ctxsize = sizeof(ss_drbg_ctx_t), \
+		.cra_module = THIS_MODULE, \
+		.cra_init = sunxi_ss_cra_rng_init, \
+		.cra_exit = sunxi_ss_cra_exit, \
+	} \
+}
+#endif
+
+static struct rng_alg sunxi_ss_algs_rng[] = {
+#ifdef SS_TRNG_ENABLE
+	DECLARE_SS_RNG_ALG(trng),
+#endif
+	DECLARE_SS_RNG_ALG(prng),
+#ifdef SS_DRBG_MODE_ENABLE
+	DECLARE_SS_DRBG_ALG(sha1),
+	DECLARE_SS_DRBG_ALG(sha256),
+	DECLARE_SS_DRBG_ALG(sha512),
+#endif
+};
+
+#define MD5_BLOCK_SIZE	MD5_HMAC_BLOCK_SIZE
+#define sha224_state   sha256_state
+#define sha384_state   sha512_state
+#define DECLARE_SS_AHASH_ALG(ltype, utype) \
+	{ \
+		.init		= ss_##ltype##_init, \
+		.update		= ss_hash_update, \
+		.final		= ss_hash_final, \
+		.finup		= ss_hash_finup, \
+		.digest		= ss_hash_digest, \
+		.halg = { \
+			.digestsize = utype##_DIGEST_SIZE, \
+			.statesize = sizeof(struct ltype##_state), \
+			.base	= { \
+			.cra_name        = #ltype, \
+			.cra_driver_name = "ss-"#ltype, \
+			.cra_priority = SS_ALG_PRIORITY, \
+			.cra_flags  = CRYPTO_ALG_TYPE_AHASH|CRYPTO_ALG_ASYNC, \
+			.cra_blocksize	 = utype##_BLOCK_SIZE, \
+			.cra_ctxsize	 = sizeof(ss_hash_ctx_t), \
+			.cra_alignmask	 = 3, \
+			.cra_module	 = THIS_MODULE, \
+			.cra_init	 = sunxi_ss_cra_hash_init, \
+			.cra_exit	 = sunxi_ss_cra_exit, \
+			} \
+		} \
+	}
+
+static struct ahash_alg sunxi_ss_algs_hash[] = {
+	DECLARE_SS_AHASH_ALG(md5, MD5),
+	DECLARE_SS_AHASH_ALG(sha1, SHA1),
+#ifdef SS_SHA224_ENABLE
+	DECLARE_SS_AHASH_ALG(sha224, SHA224),
+#endif
+#ifdef SS_SHA256_ENABLE
+	DECLARE_SS_AHASH_ALG(sha256, SHA256),
+#endif
+#ifdef SS_SHA384_ENABLE
+	DECLARE_SS_AHASH_ALG(sha384, SHA384),
+#endif
+#ifdef SS_SHA512_ENABLE
+	DECLARE_SS_AHASH_ALG(sha512, SHA512),
+#endif
+};
+
+/* Requeset the resource: IRQ, mem */
+static int sunxi_ss_res_request(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device_node *pnode = pdev->dev.of_node;
+	sunxi_ss_t *sss = platform_get_drvdata(pdev);
+#ifdef SS_IDMA_ENABLE
+	int i;
+
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		sss->flows[i].buf_src =	kmalloc(SS_DMA_BUF_SIZE, GFP_KERNEL);
+		if (sss->flows[i].buf_src == NULL) {
+			SS_ERR("Can not allocate DMA source buffer\n");
+			return -ENOMEM;
+		}
+		sss->flows[i].buf_src_dma = virt_to_phys(sss->flows[i].buf_src);
+
+		sss->flows[i].buf_dst = kmalloc(SS_DMA_BUF_SIZE, GFP_KERNEL);
+		if (sss->flows[i].buf_dst == NULL) {
+			SS_ERR("Can not allocate DMA source buffer\n");
+			return -ENOMEM;
+		}
+		sss->flows[i].buf_dst_dma = virt_to_phys(sss->flows[i].buf_dst);
+		init_completion(&sss->flows[i].done);
+	}
+#endif
+	sss->irq = irq_of_parse_and_map(pnode, SS_RES_INDEX);
+	if (sss->irq == 0) {
+		SS_ERR("Failed to get the SS IRQ.\n");
+		return -EINVAL;
+	}
+
+	ret = request_irq(sss->irq,
+			sunxi_ss_irq_handler, 0, sss->dev_name, sss);
+	if (ret != 0) {
+		SS_ERR("Cannot request IRQ\n");
+		return ret;
+	}
+
+#ifdef CONFIG_OF
+	sss->base_addr = of_iomap(pnode, SS_RES_INDEX);
+	if (sss->base_addr == NULL) {
+		SS_ERR("Unable to remap IO\n");
+		return -ENXIO;
+	}
+#endif
+
+	return 0;
+}
+
+/* Release the resource: IRQ, mem */
+static int sunxi_ss_res_release(sunxi_ss_t *sss)
+{
+#ifdef SS_IDMA_ENABLE
+	int i;
+#endif
+
+	iounmap(sss->base_addr);
+
+#ifdef SS_IDMA_ENABLE
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		kfree(sss->flows[i].buf_src);
+		kfree(sss->flows[i].buf_dst);
+	}
+#endif
+
+	free_irq(sss->irq, sss);
+	return 0;
+}
+
+static int sunxi_get_ce_clk(sunxi_ss_t *sss)
+{
+	struct platform_device *pdev = sss->pdev;
+
+	if (sss->suspend == 1) {
+		return 0;
+	}
+
+	sss->pll_clk = devm_clk_get(&pdev->dev, "pll_periph0_2x");
+	if (IS_ERR(sss->pll_clk)) {
+		SS_ERR("Fail to get pll clk, ret %x\n", PTR_RET(sss->pll_clk));
+		return PTR_RET(sss->pll_clk);
+	}
+	sss->ce_clk = devm_clk_get(&pdev->dev, "ce_clk");
+	if (IS_ERR(sss->ce_clk)) {
+		SS_ERR("Fail to get module clk, ret %x\n", PTR_RET(sss->ce_clk));
+		return PTR_RET(sss->ce_clk);
+	}
+
+	sss->bus_clk = devm_clk_get(&pdev->dev, "bus_ce");
+	if (IS_ERR(sss->bus_clk)) {
+		SS_ERR("Fail to get bus_ce clk, ret %x\n", PTR_RET(sss->bus_clk));
+		return PTR_RET(sss->bus_clk);
+	}
+
+	sss->mbus_clk = devm_clk_get(&pdev->dev, "mbus_ce");
+	if (IS_ERR(sss->mbus_clk)) {
+		SS_ERR("Fail to get mbus clk, ret %x\n", PTR_RET(sss->mbus_clk));
+		return PTR_RET(sss->mbus_clk);
+	}
+
+	sss->reset = devm_reset_control_get(&pdev->dev, NULL);
+	if (IS_ERR(sss->reset)) {
+		SS_ERR("Fail to get reset clk, ret %x\n", PTR_RET(sss->reset));
+		return PTR_RET(sss->reset);
+	}
+
+	return 0;
+}
+
+static int sunxi_ss_hw_init(sunxi_ss_t *sss)
+{
+#ifdef CONFIG_EVB_PLATFORM
+	int ret = 0;
+#endif
+	struct device_node *pnode = sss->pdev->dev.of_node;
+
+	if (sunxi_get_ce_clk(sss) != 0) {
+		return -1;
+	}
+
+	/*deassert ce reset*/
+	if (reset_control_deassert(sss->reset)) {
+		SS_ERR("Couldn't deassert reset\n");
+		return -EBUSY;
+	}
+	/*enable ce gating*/
+	if (clk_prepare_enable(sss->bus_clk)) {
+		SS_ERR("Couldn't enable bus gating\n");
+		return -EBUSY;
+	}
+
+#ifdef SS_RSA_CLK_ENABLE
+	if (of_property_read_u32_array(pnode, "clock-frequency",
+		&sss->gen_clkrate, 2)) {
+#else
+	if (of_property_read_u32(pnode, "clock-frequency", &sss->gen_clkrate)) {
+#endif
+		SS_ERR("Unable to get clock-frequency.\n");
+		return -EINVAL;
+	}
+	SS_DBG("The clk freq: %d, %d\n", sss->gen_clkrate, sss->rsa_clkrate);
+
+#ifdef CONFIG_EVB_PLATFORM
+	ret = clk_set_parent(sss->ce_clk, sss->pll_clk);
+	if (ret != 0) {
+		SS_ERR("clk_set_parent() failed! return %d\n", ret);
+		return ret;
+	}
+
+	ret = clk_set_rate(sss->ce_clk, sss->gen_clkrate);
+	if (ret != 0) {
+		SS_ERR("Set rate(%d) failed! ret %d\n", sss->gen_clkrate, ret);
+		return ret;
+	}
+#endif
+	SS_DBG("SS ce_clk%luMHz, pclk %luMHz\n", clk_get_rate(sss->ce_clk)/1000000,
+			clk_get_rate(sss->pll_clk)/1000000);
+
+	/*enable ce clock*/
+	if (clk_prepare_enable(sss->ce_clk)) {
+		SS_ERR("Couldn't enable module clock\n");
+		return -EBUSY;
+	}
+
+	/*enable ce mbus_clock*/
+	if (clk_prepare_enable(sss->mbus_clk)) {
+		SS_ERR("Couldn't enable ce mbus clock\n");
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static int sunxi_ss_hw_exit(sunxi_ss_t *sss)
+{
+	clk_disable_unprepare(sss->ce_clk);
+	clk_disable_unprepare(sss->bus_clk);
+	reset_control_assert(sss->reset);
+	return 0;
+}
+
+static int sunxi_ss_alg_register(void)
+{
+	int i;
+	int ret = 0;
+
+	for (i = 0; i < ARRAY_SIZE(sunxi_ss_algs); i++) {
+		INIT_LIST_HEAD(&sunxi_ss_algs[i].cra_list);
+		SS_DBG("Add %s...\n", sunxi_ss_algs[i].cra_name);
+		sunxi_ss_algs[i].cra_priority = SS_ALG_PRIORITY;
+		sunxi_ss_algs[i].cra_ctxsize = sizeof(ss_aes_ctx_t);
+		sunxi_ss_algs[i].cra_module = THIS_MODULE;
+		sunxi_ss_algs[i].cra_exit = sunxi_ss_cra_exit;
+		sunxi_ss_algs[i].cra_init = sunxi_ss_cra_init;
+
+		ret = crypto_register_alg(&sunxi_ss_algs[i]);
+		if (ret != 0) {
+			SS_ERR("crypto_register_alg(%s) failed! return %d\n",
+				sunxi_ss_algs[i].cra_name, ret);
+			return ret;
+		}
+	}
+
+	for (i = 0; i < ARRAY_SIZE(sunxi_ss_algs_rng); i++) {
+		SS_DBG("Add %s...\n", sunxi_ss_algs_rng[i].base.cra_name);
+		ret = crypto_register_rng(&sunxi_ss_algs_rng[i]);
+		if (ret != 0) {
+			SS_ERR("crypto_register_rng(%s) failed! return %d\n",
+				sunxi_ss_algs_rng[i].base.cra_name, ret);
+			return ret;
+		}
+	}
+
+	for (i = 0; i < ARRAY_SIZE(sunxi_ss_algs_hash); i++) {
+		SS_DBG("Add %s...\n", sunxi_ss_algs_hash[i].halg.base.cra_name);
+		ret = crypto_register_ahash(&sunxi_ss_algs_hash[i]);
+		if (ret != 0) {
+			SS_ERR("crypto_register_ahash(%s) failed! return %d\n",
+				sunxi_ss_algs_hash[i].halg.base.cra_name, ret);
+			return ret;
+		}
+	}
+#if 1
+#ifdef SS_GCM_MODE_ENABLE
+	ret = crypto_register_aead(&sunxi_aes_gcm_alg);
+	if (ret != 0) {
+		SS_ERR("crypto_register_aead(%s) failed! return %d\n",
+			sunxi_aes_gcm_alg.base.cra_name, ret);
+		return ret;
+	}
+#endif
+#endif
+	return 0;
+}
+
+static void sunxi_ss_alg_unregister(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(sunxi_ss_algs); i++)
+		crypto_unregister_alg(&sunxi_ss_algs[i]);
+
+	for (i = 0; i < ARRAY_SIZE(sunxi_ss_algs_rng); i++)
+		crypto_unregister_rng(&sunxi_ss_algs_rng[i]);
+
+	for (i = 0; i < ARRAY_SIZE(sunxi_ss_algs_hash); i++)
+		crypto_unregister_ahash(&sunxi_ss_algs_hash[i]);
+}
+
+static ssize_t sunxi_ss_info_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct platform_device *pdev =
+		container_of(dev, struct platform_device, dev);
+	sunxi_ss_t *sss = platform_get_drvdata(pdev);
+
+	return snprintf(buf, PAGE_SIZE,
+		"pdev->id   = %d\n"
+		"pdev->name = %s\n"
+		"pdev->num_resources = %u\n"
+		"pdev->resource.irq = %d\n"
+		"SS module clk rate = %ld Mhz\n"
+		"IO membase = 0x%px\n",
+		pdev->id, pdev->name, pdev->num_resources,
+		sss->irq,
+		(clk_get_rate(sss->ce_clk)/1000000), sss->base_addr);
+}
+static struct device_attribute sunxi_ss_info_attr =
+	__ATTR(info, S_IRUGO, sunxi_ss_info_show, NULL);
+
+static ssize_t sunxi_ss_status_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	int i;
+	struct platform_device *pdev =
+			container_of(dev, struct platform_device, dev);
+	sunxi_ss_t *sss = platform_get_drvdata(pdev);
+	static char const *avail[] = {"Available", "Unavailable"};
+
+	if (sss == NULL)
+		return snprintf(buf, PAGE_SIZE, "%s\n", "sunxi_ss is NULL!");
+
+	buf[0] = 0;
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		snprintf(buf+strlen(buf), PAGE_SIZE-strlen(buf),
+			"The flow %d state: %s\n"
+#ifdef SS_IDMA_ENABLE
+			"    Src: 0x%px / 0x%08x\n"
+			"    Dst: 0x%px / 0x%08x\n"
+#endif
+			, i, avail[sss->flows[i].available]
+#ifdef SS_IDMA_ENABLE
+			, sss->flows[i].buf_src, sss->flows[i].buf_src_dma
+			, sss->flows[i].buf_dst, sss->flows[i].buf_dst_dma
+#endif
+		);
+	}
+
+	return strlen(buf)
+		+ ss_reg_print(buf + strlen(buf), PAGE_SIZE - strlen(buf));
+}
+
+static struct device_attribute sunxi_ss_status_attr =
+	__ATTR(status, S_IRUGO, sunxi_ss_status_show, NULL);
+
+static void sunxi_ss_sysfs_create(struct platform_device *_pdev)
+{
+	device_create_file(&_pdev->dev, &sunxi_ss_info_attr);
+	device_create_file(&_pdev->dev, &sunxi_ss_status_attr);
+}
+
+static void sunxi_ss_sysfs_remove(struct platform_device *_pdev)
+{
+	device_remove_file(&_pdev->dev, &sunxi_ss_info_attr);
+	device_remove_file(&_pdev->dev, &sunxi_ss_status_attr);
+}
+
+static u64 sunxi_ss_dma_mask = DMA_BIT_MASK(64);
+
+static int sunxi_ss_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	sunxi_ss_t *sss = NULL;
+
+	sss = devm_kzalloc(&pdev->dev, sizeof(sunxi_ss_t), GFP_KERNEL);
+	if (sss == NULL) {
+		SS_ERR("Unable to allocate sunxi_ss_t\n");
+		return -ENOMEM;
+	}
+
+#ifdef TASK_DMA_POOL
+	sss->task_pool = dma_pool_create("task_pool", &pdev->dev,
+			sizeof(struct ce_task_desc), 4, 0);
+	if (sss->task_pool == NULL)
+		return -ENOMEM;
+#endif
+
+#ifdef CONFIG_OF
+	pdev->dev.dma_mask = &sunxi_ss_dma_mask;
+	pdev->dev.coherent_dma_mask = DMA_BIT_MASK(64);
+#endif
+
+	snprintf(sss->dev_name, sizeof(sss->dev_name), SUNXI_SS_DEV_NAME);
+	platform_set_drvdata(pdev, sss);
+
+	ret = sunxi_ss_res_request(pdev);
+	if (ret != 0)
+		goto err0;
+
+	sss->pdev = pdev;
+
+	ret = sunxi_ss_hw_init(sss);
+	if (ret != 0) {
+		SS_ERR("SS hw init failed!\n");
+		goto err1;
+	}
+
+	ss_dev = sss;
+	ret = sunxi_ss_alg_register();
+	if (ret != 0) {
+		SS_ERR("sunxi_ss_alg_register() failed! return %d\n", ret);
+		goto err2;
+	}
+
+	sunxi_ss_sysfs_create(pdev);
+
+	SS_DBG("SS is inited, base 0x%px, irq %d!\n", sss->base_addr, sss->irq);
+	return 0;
+
+err2:
+	sunxi_ss_hw_exit(sss);
+err1:
+	sunxi_ss_res_release(sss);
+err0:
+	platform_set_drvdata(pdev, NULL);
+#ifdef SS_SCATTER_ENABLE
+	if (sss->task_pool)
+		dma_pool_destroy(sss->task_pool);
+#endif
+	return ret;
+}
+
+static int sunxi_ss_remove(struct platform_device *pdev)
+{
+	sunxi_ss_t *sss = platform_get_drvdata(pdev);
+
+	ss_wait_idle();
+	sunxi_ss_sysfs_remove(pdev);
+
+	sunxi_ss_alg_unregister();
+	sunxi_ss_hw_exit(sss);
+	sunxi_ss_res_release(sss);
+
+#ifdef SS_SCATTER_ENABLE
+	if (sss->task_pool)
+		dma_pool_destroy(sss->task_pool);
+#endif
+	platform_set_drvdata(pdev, NULL);
+	ss_dev = NULL;
+	return 0;
+}
+
+#ifdef CONFIG_PM
+static int sunxi_ss_suspend(struct device *dev)
+{
+#ifdef CONFIG_EVB_PLATFORM
+	struct platform_device *pdev = to_platform_device(dev);
+	sunxi_ss_t *sss = platform_get_drvdata(pdev);
+	unsigned long flags = 0;
+
+	SS_ENTER();
+
+	/* Wait for the completion of SS operation. */
+	ss_dev_lock();
+
+	spin_lock_irqsave(&ss_dev->lock, flags);
+	sss->suspend = 1;
+	spin_unlock_irqrestore(&sss->lock, flags);
+
+	sunxi_ss_hw_exit(sss);
+	ss_dev_unlock();
+#endif
+
+	return 0;
+}
+
+static int sunxi_ss_resume(struct device *dev)
+{
+	int ret = 0;
+#ifdef CONFIG_EVB_PLATFORM
+	struct platform_device *pdev = to_platform_device(dev);
+	sunxi_ss_t *sss = platform_get_drvdata(pdev);
+	unsigned long flags = 0;
+
+	SS_ENTER();
+	ret = sunxi_ss_hw_init(sss);
+	spin_lock_irqsave(&ss_dev->lock, flags);
+	sss->suspend = 0;
+	spin_unlock_irqrestore(&sss->lock, flags);
+#endif
+	return ret;
+}
+
+static const struct dev_pm_ops sunxi_ss_dev_pm_ops = {
+	.suspend = sunxi_ss_suspend,
+	.resume  = sunxi_ss_resume,
+};
+
+#define SUNXI_SS_DEV_PM_OPS (&sunxi_ss_dev_pm_ops)
+#else
+#define SUNXI_SS_DEV_PM_OPS NULL
+#endif /* CONFIG_PM */
+
+static struct platform_driver sunxi_ss_driver = {
+	.probe   = sunxi_ss_probe,
+	.remove  = sunxi_ss_remove,
+	.driver = {
+	.name	= SUNXI_SS_DEV_NAME,
+		.owner	= THIS_MODULE,
+		.pm		= SUNXI_SS_DEV_PM_OPS,
+		.of_match_table = sunxi_ss_of_match,
+	},
+};
+
+static int __init sunxi_ss_init(void)
+{
+	int ret = 0;
+
+	ret = platform_driver_register(&sunxi_ss_driver);
+	if (ret < 0) {
+		SS_ERR("platform_driver_register() failed, return %d\n", ret);
+		return ret;
+	}
+
+	return ret;
+}
+
+static void __exit sunxi_ss_exit(void)
+{
+	platform_driver_unregister(&sunxi_ss_driver);
+}
+
+module_init(sunxi_ss_init);
+module_exit(sunxi_ss_exit);
+
+MODULE_AUTHOR("mintow");
+MODULE_DESCRIPTION("SUNXI SS Controller Driver");
+MODULE_ALIAS("platform:"SUNXI_SS_DEV_NAME);
+MODULE_LICENSE("GPL");
diff --git a/drivers/crypto/sunxi-ce/sunxi_ce.h b/drivers/crypto/sunxi-ce/sunxi_ce.h
new file mode 100644
index 000000000..22de0dc2b
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sunxi_ce.h
@@ -0,0 +1,491 @@
+/*
+ * Some macro and struct of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_SECURITY_SYSTEM_H_
+#define _SUNXI_SECURITY_SYSTEM_H_
+
+#include <crypto/aes.h>
+#include <crypto/sha.h>
+#include <crypto/hash.h>
+#include <crypto/algapi.h>
+
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+
+/* flag for sunxi_ss_t.flags */
+#define SS_FLAG_MODE_MASK		0xFF
+#define SS_FLAG_NEW_KEY			BIT(0)
+#define SS_FLAG_NEW_IV			BIT(1)
+#define SS_FLAG_INIT			BIT(2)
+#define SS_FLAG_FAST			BIT(3)
+#define SS_FLAG_BUSY			BIT(4)
+#define SS_FLAG_TRNG			BIT(8)
+
+/* flag for crypto_async_request.flags */
+#define SS_FLAG_AES				BIT(16)
+#define SS_FLAG_HASH			BIT(17)
+
+/* Define the capability of SS controller. */
+#ifdef CONFIG_ARCH_SUN8IW6
+#define SS_CTR_MODE_ENABLE		1
+#define SS_CTS_MODE_ENABLE		1
+#define SS_SHA224_ENABLE		1
+#define SS_SHA256_ENABLE		1
+#define SS_TRNG_ENABLE			1
+#define SS_TRNG_POSTPROCESS_ENABLE	1
+#define SS_RSA512_ENABLE		1
+#define SS_RSA1024_ENABLE		1
+#define SS_RSA2048_ENABLE		1
+#define SS_RSA3072_ENABLE		1
+#define SS_IDMA_ENABLE			1
+#define SS_MULTI_FLOW_ENABLE	1
+
+#define SS_SHA_SWAP_PRE_ENABLE	1 /* The initial IV need to be converted. */
+
+#define SS_DMA_BUF_SIZE			SZ_8K
+
+#define SS_RSA_MIN_SIZE			(512/8)  /* in Bytes. 512 bits */
+#define SS_RSA_MAX_SIZE			(3072/8) /* in Bytes. 3072 bits */
+
+#define SS_FLOW_NUM				2
+#endif
+
+
+/*define CE_V1_X---->CE_DRIVER_V3_1*/
+#if defined(CONFIG_ARCH_SUN20IW1) || defined(CONFIG_ARCH_SUN8IW11)\
+	|| defined(CONFIG_ARCH_SUN50IW1) || defined(CONFIG_ARCH_SUN50IW2)\
+	|| defined(CONFIG_ARCH_SUN8IW12) || defined(CONFIG_ARCH_SUN8IW15)\
+	|| defined(CONFIG_ARCH_SUN8IW7) || defined(CONFIG_ARCH_SUN50IW8)\
+	|| defined(CONFIG_ARCH_SUN8IW17) || defined(CONFIG_ARCH_SUN8IW18)\
+	|| defined(CONFIG_ARCH_SUN8IW16) || defined(CONFIG_ARCH_SUN8IW19)\
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW11)\
+
+#define SS_SUPPORT_CE_V3_1		1
+#define SS_SCATTER_ENABLE		1
+#define	TASK_DMA_POOL			1
+#define SS_SHA_SWAP_PRE_ENABLE	1 /* The initial IV need to be converted. */
+#endif
+
+/*define CE_V2_X---->CE_DRIVER_V3_2*/
+#if defined(CONFIG_ARCH_SUN50IW3) || defined(CONFIG_ARCH_SUN50IW6) || \
+	defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW8) || \
+	defined(CONFIG_ARCH_SUN8IW16) || defined(CONFIG_ARCH_SUN50IW9) || \
+	defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) || \
+	defined(CONFIG_ARCH_SUN50IW12)
+
+#define SS_SUPPORT_CE_V3_2		1
+#define SS_SCATTER_ENABLE		1
+
+#define SS_SHA_SWAP_PRE_ENABLE	1 /* The initial IV need to be converted. */
+
+#if defined(CONFIG_ARCH_SUN50IW12)
+#define	CE_BYTE_ADDR_ENABLE
+#endif
+
+#endif
+
+#ifdef CONFIG_EVB_PLATFORM
+#define SS_TRNG_ENABLE				1
+#define SS_TRNG_POSTPROCESS_ENABLE	1
+#endif
+
+#if defined(SS_SUPPORT_CE_V3_1) || defined(SS_SUPPORT_CE_V3_2)
+#define SS_CTR_MODE_ENABLE		1
+#define SS_CTS_MODE_ENABLE		1
+#define SS_OFB_MODE_ENABLE		1
+#define SS_CFB_MODE_ENABLE		1
+
+#define SS_SHA224_ENABLE		1
+#define SS_SHA256_ENABLE		1
+#define SS_SHA384_ENABLE		1
+#define SS_SHA512_ENABLE		1
+#define SS_HMAC_SHA1_ENABLE		1
+#define SS_HMAC_SHA256_ENABLE	1
+
+#define SS_RSA512_ENABLE		1
+#define SS_RSA1024_ENABLE		1
+#define SS_RSA2048_ENABLE		1
+
+#define SS_DH512_ENABLE			1
+#define SS_DH1024_ENABLE		1
+#define SS_DH2048_ENABLE		1
+
+#define SS_ECC_ENABLE			1
+#endif	/*SS_SUPPORT_CE_V3_1*/
+
+
+#if defined(SS_SUPPORT_CE_V3_2)
+#define SS_XTS_MODE_ENABLE		1
+
+#define SS_RSA3072_ENABLE		1
+#define SS_RSA4096_ENABLE		1
+
+#define SS_DH3072_ENABLE		1
+#define SS_DH4096_ENABLE		1
+
+#define SS_HASH_HW_PADDING		1
+#define SS_HASH_HW_PADDING_ALIGN_CASE	1
+
+#if defined(CONFIG_ARCH_SUN50IW8) || defined(CONFIG_ARCH_SUN50IW10) || \
+	defined(CONFIG_ARCH_SUN50IW12)
+
+#define SS_GCM_MODE_ENABLE	1
+#define SS_DRBG_MODE_ENABLE	1
+#endif
+
+#undef SS_TRNG_POSTPROCESS_ENABLE
+
+#endif	/*SS_SUPPORT_CE_V3_2*/
+
+
+#define SS_RSA_MIN_SIZE			(512/8)  /* in Bytes. 512 bits */
+
+#if defined (CONFIG_ARCH_SUN8IW7)
+#define SS_RSA_MAX_SIZE			(4096/8) /*in Bytes. 4096 bits*/
+#else
+#define SS_RSA_MAX_SIZE			(2048/8) /* in Bytes. 2048 bits */
+#endif
+
+#ifdef CONFIG_ARCH_SUN8IW7
+#define SS_FLOW_NUM			1
+#else
+#define SS_FLOW_NUM				4
+#endif
+
+#if defined(CONFIG_ARCH_SUN8IW7)
+#define SS_ECC_ENABLE                   1
+#define SS_RSA_PREPROCESS_ENABLE        1
+#define SS_RSA_CLK_ENABLE		1
+#endif
+
+
+#if defined(SS_RSA512_ENABLE) || defined(SS_RSA1024_ENABLE) \
+	|| defined(SS_RSA2048_ENABLE) || defined(SS_RSA3072_ENABLE) \
+	|| defined(SS_RSA4096_ENABLE)
+#define SS_RSA_ENABLE			1
+#endif
+
+#if defined(SS_DH512_ENABLE) || defined(SS_DH1024_ENABLE) \
+	|| defined(SS_DH2048_ENABLE) || defined(SS_DH3072_ENABLE) \
+	|| defined(SS_DH4096_ENABLE)
+#define SS_DH_ENABLE			1
+#endif
+
+#define SS_PRNG_SEED_LEN		(192/8) /* 192 bits */
+#define SS_RNG_MAX_LEN			SZ_8K
+
+#define SUNXI_SS_DEV_NAME	"ss"
+
+#if defined(SS_SHA384_ENABLE) || defined(SS_SHA512_ENABLE)
+#define SS_DIGEST_SIZE		SHA512_DIGEST_SIZE
+#define SS_HASH_PAD_SIZE	(SHA512_BLOCK_SIZE * 2)
+#else
+#define SS_DIGEST_SIZE		SHA256_DIGEST_SIZE
+#define SS_HASH_PAD_SIZE	(SHA1_BLOCK_SIZE * 2)
+#endif
+
+#ifdef CONFIG_EVB_PLATFORM
+#define SS_WAIT_TIME	2000 /* 2s, used in wait_for_completion_timeout() */
+#else
+#define SS_WAIT_TIME	40000 /* 40s, used in wait_for_completion_timeout() */
+#endif
+
+#define SS_ALG_PRIORITY		260
+
+#if defined(CONFIG_ARCH_SUN50IW9) || defined(CONFIG_ARCH_SUN50IW10)
+#define WORD_ALGIN		(2)
+#else
+#define WORD_ALGIN		(0)
+#endif
+
+
+/* For debug */
+/*#define SUNXI_CE_DEBUG*/
+#ifdef SUNXI_CE_DEBUG
+#define SS_DBG(fmt, arg...) pr_err("%s()%d - "fmt, __func__, __LINE__, ##arg)
+#else
+#define SS_DBG(fmt, arg...) pr_debug("%s()%d - "fmt, __func__, __LINE__, ##arg)
+#endif
+#define SS_ERR(fmt, arg...) pr_err("%s()%d - "fmt, __func__, __LINE__, ##arg)
+#define SS_EXIT()	    SS_DBG("%s\n", "Exit")
+#define SS_ENTER()	    SS_DBG("%s\n", "Enter ...")
+
+#define SS_FLOW_AVAILABLE	0
+#define SS_FLOW_UNAVAILABLE	1
+
+#define SS_RES_NS_INDEX	0
+#define SS_RES_S_INDEX	1
+#ifdef CONFIG_EVB_PLATFORM
+#define SS_RES_INDEX	SS_RES_NS_INDEX
+#else
+#define SS_RES_INDEX	SS_RES_NS_INDEX
+#endif
+
+#define CE_ADDR_MASK	0xffffffffff
+#ifdef SS_SCATTER_ENABLE
+
+/* CE: Crypto Engine, start using CE from sun8iw7/sun8iw9 */
+#define CE_SCATTERS_PER_TASK		8
+
+/* The descriptor of a CE task. */
+#ifdef CE_BYTE_ADDR_ENABLE
+typedef struct {
+	u8 src_addr[5];
+	u8 dst_addr[5];
+	u8 pad[2];
+	u32 src_len;
+	u32 dst_len;
+} ce_scatter_t;
+
+typedef struct ce_task_desc {
+	u32 chan_id;
+	u32 comm_ctl;
+	u32 sym_ctl;
+	u32 asym_ctl;
+
+	u8 key_addr[5];
+	u8 iv_addr[5];
+	u8 ctr_addr[5];
+	u8 pad;
+	u32 data_len; /* in word(4 byte). Exception: in byte for AES_CTS */
+
+	ce_scatter_t ce_sg[CE_SCATTERS_PER_TASK];
+
+	u8 next_sg_addr[5];
+	u8 next_task_addr[5];
+	u8 pad2[2];
+	u32 reserved[3];
+	dma_addr_t task_phy_addr;
+} ce_task_desc_t;
+
+/* The descriptor of a CE hash and rng task. */
+typedef struct ce_new_task_desc {
+	u32 comm_ctl;
+	u32 main_cmd;
+	u8 data_len[5];
+	u8 key_addr[5];
+	u8 iv_addr[5];
+	u8 pad;
+
+	ce_scatter_t ce_sg[CE_SCATTERS_PER_TASK];
+
+	u8 next_sg_addr[5];
+	u8 next_task_addr[5];
+	u8 pad2[2];
+	u32 reserved[3];
+	dma_addr_t task_phy_addr;
+} ce_new_task_desc_t;
+#else
+
+typedef struct {
+	u32 addr;
+	u32 len; /* in word (4 bytes). Exception: in byte for AES_CTS */
+} ce_scatter_t;
+
+typedef struct ce_task_desc {
+	u32 chan_id;
+	u32 comm_ctl;
+	u32 sym_ctl;
+	u32 asym_ctl;
+
+	u32 key_addr;
+	u32 iv_addr;
+	u32 ctr_addr;
+	u32 data_len; /* in word(4 byte). Exception: in byte for AES_CTS */
+
+	ce_scatter_t src[CE_SCATTERS_PER_TASK];
+	ce_scatter_t dst[CE_SCATTERS_PER_TASK];
+
+	u32 next_addr;
+	u32 reserved[3];
+	dma_addr_t task_phy_addr;
+} ce_task_desc_t;
+
+/* The descriptor of a CE hash and rng task. */
+typedef struct ce_new_task_desc {
+	u32 common_ctl;
+	u32 main_cmd;
+	u32 data_len;
+	u32 key_addr;
+	u32 iv_addr;
+
+	ce_scatter_t src[CE_SCATTERS_PER_TASK];
+	ce_scatter_t dst[CE_SCATTERS_PER_TASK];
+	u32 next_addr;
+	u32 reserved[3];
+	dma_addr_t task_phy_addr;
+} ce_new_task_desc_t;
+#endif
+
+
+#endif
+struct ce_scatter_mapping {
+	void *virt_addr;
+	u32 data_len;
+};
+
+typedef struct {
+	u32 dir;
+	u32 nents;
+	struct dma_chan *chan;
+	struct scatterlist *sg;
+#ifdef SS_IDMA_ENABLE
+	struct sg_table sgt_for_cp; /* Used to copy data from/to user space. */
+#endif
+#ifdef SS_SCATTER_ENABLE
+	u32 has_padding;
+	u8 *padding;
+	struct scatterlist *last_sg;
+	struct ce_scatter_mapping mapping[CE_SCATTERS_PER_TASK];
+#endif
+} ss_dma_info_t;
+
+typedef struct {
+	u32 dir;
+	u32 type;
+	u32 mode;
+#ifdef SS_CFB_MODE_ENABLE
+	u32 bitwidth;	/* the bitwidth of CFB mode */
+#endif
+	struct completion done;
+	ss_dma_info_t dma_src;
+	ss_dma_info_t dma_dst;
+} ss_aes_req_ctx_t;
+
+/* The common context of AES and HASH */
+typedef struct {
+	u32 flow;
+	u32 flags;
+} ss_comm_ctx_t;
+
+typedef struct {
+	ss_comm_ctx_t comm; /* must be in the front. */
+
+#ifdef SS_RSA_ENABLE
+	u8 key[SS_RSA_MAX_SIZE];
+	u8 iv[SS_RSA_MAX_SIZE];
+#else
+	u8 key[AES_MAX_KEY_SIZE];
+	u8 iv[AES_MAX_KEY_SIZE];
+#endif
+#ifdef SS_SCATTER_ENABLE
+	u8 next_iv[AES_MAX_KEY_SIZE]; /* the next IV/Counter in continue mode */
+#endif
+
+	u32 key_size;
+	u32 iv_size;
+	u32 cnt;	/* in Byte */
+} ss_aes_ctx_t;
+
+#ifdef SS_DRBG_MODE_ENABLE
+typedef struct {
+	ss_comm_ctx_t comm; /* must be in the front. */
+
+	u8 person[AES_MAX_KEY_SIZE];	/* in drbg, means personalization input */
+	u32 person_size;
+	u8 nonce[AES_MAX_KEY_SIZE];
+	u32 nonce_size;
+	u8 entropt[AES_MAX_KEY_SIZE];
+	u32 entropt_size;
+
+	u32 cnt;	/* in Byte */
+} ss_drbg_ctx_t;
+
+#endif
+
+#ifdef SS_GCM_MODE_ENABLE
+typedef struct {
+	ss_comm_ctx_t comm; /* must be in the front. */
+
+	u8 key[AES_MAX_KEY_SIZE];
+	u8 iv[AES_MAX_KEY_SIZE];
+
+#ifdef SS_SCATTER_ENABLE
+	u8 next_iv[AES_MAX_KEY_SIZE]; /* the next IV/Counter in continue mode */
+#endif
+
+	u8 task_iv[88];	/* max size: pt_iv(4 words) || tag_iv(4 words) ||
+					   ghash_y(4 words) || tag(4 words) || iv_size(2 words)
+					   || aad_size(2 words) || pt_size(2 words)*/
+	u8 task_ctr[48];	/* max size when not last package: pt_iv(4 words) ||
+						   tag_iv(4 words) || ghash_y(4 words) */
+	u8 tag[16];		/* to store the tag data */
+
+	struct scatterlist *aad_addr;
+	u32 key_size;
+	u32 iv_size;
+	u32 tag_len;
+	u32 assoclen;
+	u32 cryptlen;
+
+	u32 cnt;	/* in Byte */
+} ss_aead_ctx_t;
+#endif
+
+typedef struct {
+	ss_comm_ctx_t comm; /* must be in the front. */
+
+	u8  md[SS_DIGEST_SIZE];
+	u8  pad[SS_HASH_PAD_SIZE];
+	u32 tail_len;
+	u32 md_size;
+	u32 cnt;	/* in Byte */
+} ss_hash_ctx_t;
+
+typedef struct {
+#ifdef SS_IDMA_ENABLE
+	char *buf_src;
+	dma_addr_t buf_src_dma;
+	char *buf_dst;
+	dma_addr_t buf_dst_dma;
+#endif
+#ifdef SS_SCATTER_ENABLE
+	ce_task_desc_t task;
+#endif
+	struct completion done;
+	u32 available;
+} ss_flow_t;
+
+typedef struct {
+	struct platform_device *pdev;
+	void __iomem *base_addr; /* for register */
+
+	ss_flow_t flows[SS_FLOW_NUM];
+	struct dma_pool		*task_pool;
+
+	struct clk *ce_clk;  /* module clock */
+	struct clk *bus_clk;
+	struct clk *mbus_clk;
+	struct clk *pll_clk;
+	struct reset_control *reset;
+	u32 gen_clkrate;
+	u32 rsa_clkrate;
+	u32 irq;
+	s8  dev_name[8];
+
+	spinlock_t lock;
+	s32 suspend;
+} sunxi_ss_t;
+
+/* Global variable */
+
+extern sunxi_ss_t *ss_dev;
+
+/* Inner functions declaration */
+
+void ss_dev_lock(void);
+void ss_dev_unlock(void);
+void __iomem *ss_membase(void);
+void ss_reset(void);
+void ss_clk_set(u32 rate);
+
+#endif /* end of _SUNXI_SECURITY_SYSTEM_H_ */
diff --git a/drivers/crypto/sunxi-ce/sunxi_ce_cdev.c b/drivers/crypto/sunxi-ce/sunxi_ce_cdev.c
new file mode 100644
index 000000000..7ed6a0383
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sunxi_ce_cdev.c
@@ -0,0 +1,539 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/clk.h>
+#include <linux/clk/sunxi.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+
+#include "sunxi_ce_cdev.h"
+
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+
+
+sunxi_ce_cdev_t	*ce_cdev;
+
+static DEFINE_MUTEX(ss_lock);
+
+void ce_dev_lock(void)
+{
+	mutex_lock(&ss_lock);
+}
+
+void ce_dev_unlock(void)
+{
+	mutex_unlock(&ss_lock);
+}
+
+void __iomem *ss_membase(void)
+{
+	return ce_cdev->base_addr;
+}
+
+void ce_reset(void)
+{
+	SS_ENTER();
+	reset_control_assert(ce_cdev->reset);
+	reset_control_deassert(ce_cdev->reset);
+}
+
+/* Requeset the resource: IRQ, mem */
+static int sunxi_ce_res_request(sunxi_ce_cdev_t *p_cdev)
+{
+	int ret = 0;
+	struct device_node *pnode = p_cdev->pnode;
+
+	p_cdev->irq = irq_of_parse_and_map(pnode, SS_RES_INDEX);
+	if (p_cdev->irq == 0) {
+		SS_ERR("Failed to get the CE IRQ.\n");
+		return -EINVAL;
+	}
+
+	ret = request_irq(p_cdev->irq,
+			sunxi_ce_irq_handler, 0, p_cdev->dev_name, p_cdev);
+	if (ret != 0) {
+		SS_ERR("Cannot request IRQ\n");
+		return ret;
+	}
+
+#ifdef CONFIG_OF
+	p_cdev->base_addr = of_iomap(pnode, SS_RES_INDEX);
+	if (p_cdev->base_addr == NULL) {
+		SS_ERR("Unable to remap IO\n");
+		return -ENXIO;
+	}
+#endif
+
+	return 0;
+}
+
+/* Release the resource: IRQ, mem */
+static int sunxi_ce_res_release(sunxi_ce_cdev_t *p_cdev)
+{
+	if (p_cdev->base_addr)
+		iounmap(p_cdev->base_addr);
+	if (p_cdev->irq)
+		free_irq(p_cdev->irq, p_cdev);
+	if (p_cdev->pnode)
+		of_node_put(p_cdev->pnode);
+	return 0;
+}
+
+static s32 sunxi_get_ce_device_node(struct device_node **pnode, void __iomem **base,
+		s8 *compatible)
+{
+	*pnode = of_find_compatible_node(NULL, NULL, compatible);
+	if (IS_ERR_OR_NULL(*pnode)) {
+		SS_ERR("Failed to find \"%s\" in dts.\n", compatible);
+		return -1;
+	}
+
+	*base = of_iomap(*pnode, 0); /* reg[0] must be accessible. */
+	if (*base == NULL) {
+		SS_ERR("Unable to remap IO\n");
+		return -2;
+	}
+	SS_DBG("Base addr of \"%s\" is %p\n", compatible, *base);
+	return 0;
+}
+
+static int sunxi_ce_hw_init(sunxi_ce_cdev_t *p_cdev)
+{
+	int ret = 0;
+	struct clk *pclk = NULL;
+	u32 gen_clkrate;
+
+	ret = sunxi_get_ce_device_node(&p_cdev->pnode, &p_cdev->base_addr, SUNXI_CE_DEV_NODE_NAME);
+	if (ret) {
+		SS_ERR("sunxi_get_ce_device_node fail, return %x\n", ret);
+		return -1;
+	}
+
+	/*get periph0 clk reg*/
+	pclk = of_clk_get(p_cdev->pnode, 1);
+	if (IS_ERR_OR_NULL(pclk)) {
+		SS_ERR("Unable to get pll clock, return %x\n", PTR_RET(pclk));
+		return PTR_RET(pclk);
+	}
+
+	/*get ce clk reg*/
+	p_cdev->ce_clk = of_clk_get(p_cdev->pnode, 0);
+	if (IS_ERR_OR_NULL(p_cdev->ce_clk)) {
+		SS_ERR("Fail to get module clk, ret %x\n", PTR_RET(p_cdev->ce_clk));
+		return PTR_RET(p_cdev->ce_clk);
+	}
+
+	/*get ce need configure clk*/
+	if (of_property_read_u32(p_cdev->pnode, "clock-frequency", &gen_clkrate)) {
+
+		SS_ERR("Unable to get clock-frequency.\n");
+		return -EINVAL;
+	}
+	SS_DBG("The clk freq: %d\n", gen_clkrate);
+
+	p_cdev->reset = devm_reset_control_get(p_cdev->pdevice, NULL);
+	if (IS_ERR(p_cdev->reset)) {
+		SS_ERR("Fail to get reset clk, ret %x\n", PTR_RET(p_cdev->reset));
+		return PTR_RET(p_cdev->reset);
+	}
+
+	/*configure ce clk form periph0*/
+#ifdef CONFIG_EVB_PLATFORM
+	ret = clk_set_parent(p_cdev->ce_clk, pclk);
+	if (ret != 0) {
+		SS_ERR("clk_set_parent() failed! return %d\n", ret);
+		return ret;
+	}
+
+	ret = clk_set_rate(p_cdev->ce_clk, gen_clkrate);
+	if (ret != 0) {
+		SS_ERR("Set rate(%d) failed! ret %d\n", gen_clkrate, ret);
+		return ret;
+	}
+#endif
+	SS_DBG("SS mclk %luMHz, pclk %luMHz\n", clk_get_rate(p_cdev->ce_clk)/1000000,
+			clk_get_rate(pclk)/1000000);
+
+	/*enable ce clk*/
+	if (clk_prepare_enable(p_cdev->ce_clk)) {
+		SS_ERR("Couldn't enable module clock\n");
+		return -EBUSY;
+	}
+
+	clk_put(pclk);
+
+	sunxi_ce_res_request(p_cdev);
+
+	return 0;
+}
+
+static int sunxi_ce_hw_exit(void)
+{
+	clk_disable_unprepare(ce_cdev->ce_clk);
+	clk_put(ce_cdev->ce_clk);
+	ce_cdev->ce_clk = NULL;
+	sunxi_ce_res_release(ce_cdev);
+	return 0;
+}
+
+static u64 sunxi_ss_dma_mask = DMA_BIT_MASK(64);
+
+
+static int sunxi_ce_open(struct inode *inode, struct file *file)
+{
+	file->private_data = ce_cdev;
+	return 0;
+}
+
+static int sunxi_ce_release(struct inode *inode, struct file *file)
+{
+	file->private_data = NULL;
+	return 0;
+}
+
+static ssize_t sunxi_ce_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)
+{
+	return 0;
+}
+
+static ssize_t sunxi_ce_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)
+{
+	return 0;
+}
+
+static int sunxi_ce_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	return 0;
+}
+
+static int sunxi_copy_from_user(u8 **src, u32 size)
+{
+	int ret = -1;
+	u8 *tmp = NULL;
+
+	tmp = kzalloc(size, GFP_KERNEL);
+	if (!tmp) {
+		SS_ERR("kzalloc fail\n");
+		return -ENOMEM;
+	}
+
+	ret = copy_from_user(tmp, *src, size);
+	if (ret) {
+		SS_ERR("copy_from_user fail\n");
+		return -EAGAIN;
+	}
+
+	*src = tmp;
+
+	return 0;
+}
+
+static int sunxi_ce_channel_request(void)
+{
+	int i;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ce_cdev->lock, flags);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (ce_cdev->flows[i].available == SS_FLOW_AVAILABLE) {
+			ce_cdev->flows[i].available = SS_FLOW_UNAVAILABLE;
+			SS_DBG("The flow %d is available.\n", i);
+			break;
+		}
+	}
+	ce_cdev->flag = 1;
+	spin_unlock_irqrestore(&ce_cdev->lock, flags);
+
+	if (i == SS_FLOW_NUM) {
+		SS_ERR("Failed to get an available flow.\n");
+		i = -1;
+	}
+	return i;
+}
+
+static void sunxi_ce_channel_free(int id)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&ce_cdev->lock, flags);
+	ce_cdev->flows[id].available = SS_FLOW_AVAILABLE;
+	ce_cdev->flag = 0;
+	SS_DBG("The flow %d is available.\n", id);
+	spin_unlock_irqrestore(&ce_cdev->lock, flags);
+}
+
+static long sunxi_ce_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	crypto_aes_req_ctx_t *aes_req_ctx;
+	int channel_id = -1;
+	int ret;
+	phys_addr_t usr_key_addr;
+	phys_addr_t usr_dst_addr;
+
+	SS_DBG("cmd = %u\n", cmd);
+	switch (cmd) {
+	case CE_IOC_REQUEST:
+	{
+		channel_id = sunxi_ce_channel_request();
+		if (channel_id < 0) {
+			SS_ERR("Failed to sunxi_ce_channel_request\n");
+			return -EAGAIN;
+		}
+		SS_DBG("channel_id = %d\n", channel_id);
+		ret = put_user(channel_id, (int *)arg);
+		if (ret < 0) {
+			SS_ERR("Failed to put_user\n");
+			return -EAGAIN;
+		}
+
+		break;
+	}
+
+	case CE_IOC_FREE:
+	{
+		ret = get_user(channel_id, (int *)arg);
+		if (ret < 0) {
+			SS_ERR("Failed to get_user\n");
+			return -EAGAIN;
+		}
+		sunxi_ce_channel_free(channel_id);
+
+		break;
+	}
+
+	case CE_IOC_AES_CRYPTO:
+	{
+		SS_DBG("arg_size = %d\n", _IOC_SIZE(cmd));
+		if (_IOC_SIZE(cmd) != sizeof(crypto_aes_req_ctx_t)) {
+			SS_DBG("arg_size != sizeof(crypto_aes_req_ctx_t)\n");
+			return -EINVAL;
+		}
+
+		aes_req_ctx = kzalloc(sizeof(crypto_aes_req_ctx_t), GFP_KERNEL);
+		if (!aes_req_ctx) {
+			SS_ERR("kzalloc aes_req_ctx fail\n");
+			return -ENOMEM;
+		}
+
+		ret = copy_from_user(aes_req_ctx, (crypto_aes_req_ctx_t *)arg,
+				sizeof(crypto_aes_req_ctx_t));
+		if (ret) {
+			SS_ERR("copy_from_user fail\n");
+			kfree(aes_req_ctx);
+			return -EAGAIN;
+		}
+		usr_key_addr = (phys_addr_t)aes_req_ctx->key_buffer;
+		usr_dst_addr = (phys_addr_t)aes_req_ctx->dst_buffer;
+		SS_DBG("usr_key_addr = 0x%px usr_dst_addr = 0x%px\n",
+				(void *)usr_key_addr, (void *)usr_dst_addr);
+		ret = sunxi_copy_from_user(&aes_req_ctx->key_buffer, aes_req_ctx->key_length);
+		if (ret) {
+			SS_ERR("key_buffer copy_from_user fail\n");
+			return -EAGAIN;
+		}
+
+		SS_DBG("aes_req_ctx->key_buffer = 0x%px\n", aes_req_ctx->key_buffer);
+		ret = sunxi_copy_from_user(&aes_req_ctx->iv_buf, aes_req_ctx->iv_length);
+		if (ret) {
+			SS_ERR("iv_buffer copy_from_user fail\n");
+			return -EAGAIN;
+		}
+
+		if (aes_req_ctx->ion_flag) {
+			SS_DBG("src_phy = 0x%lx, dst_phy = 0x%lx\n", aes_req_ctx->src_phy, aes_req_ctx->dst_phy);
+		} else {
+			ret = sunxi_copy_from_user(&aes_req_ctx->src_buffer, aes_req_ctx->src_length);
+			if (ret) {
+				SS_ERR("src_buffer copy_from_user fail\n");
+				return -EAGAIN;
+			}
+
+			ret = sunxi_copy_from_user(&aes_req_ctx->dst_buffer, aes_req_ctx->dst_length);
+			if (ret) {
+				SS_ERR("dst_buffer copy_from_user fail\n");
+				return -EAGAIN;
+			}
+		}
+
+		SS_ERR("do_aes_crypto start\n");
+		ret = do_aes_crypto(aes_req_ctx);
+		if (ret) {
+			kfree(aes_req_ctx);
+			SS_ERR("do_aes_crypto fail\n");
+			return -3;
+		}
+
+		if (aes_req_ctx->ion_flag) {
+		} else {
+			ret = copy_to_user((u8 *)usr_dst_addr, aes_req_ctx->dst_buffer, aes_req_ctx->dst_length);
+			if (ret) {
+				SS_ERR(" dst_buffer copy_from_user fail\n");
+				kfree(aes_req_ctx);
+				return -EAGAIN;
+			}
+		}
+		break;
+	}
+
+	default:
+		return -EINVAL;
+		break;
+
+	}
+	return 0;
+}
+
+static const struct file_operations ce_fops = {
+	.owner          = THIS_MODULE,
+	.open           = sunxi_ce_open,
+	.release        = sunxi_ce_release,
+	.write          = sunxi_ce_write,
+	.read           = sunxi_ce_read,
+	.unlocked_ioctl = sunxi_ce_ioctl,
+	.mmap           = sunxi_ce_mmap,
+};
+
+
+static int sunxi_ce_setup_cdev(void)
+{
+	int ret = 0;
+
+	ce_cdev = kzalloc(sizeof(sunxi_ce_cdev_t), GFP_KERNEL);
+	if (!ce_cdev) {
+		SS_ERR("kzalloc fail\n");
+		return -ENOMEM;
+	}
+
+	/*alloc devid*/
+	ret = alloc_chrdev_region(&ce_cdev->devid, 0, 1, SUNXI_SS_DEV_NAME);
+	if (ret < 0) {
+		SS_ERR("alloc_chrdev_region fail\n");
+		kfree(ce_cdev);
+		return -1;
+	}
+
+	SS_DBG("ce_cdev->devid = %d\n", ce_cdev->devid & 0xfff00000);
+	/*alloc pcdev*/
+	ce_cdev->pcdev = cdev_alloc();
+	if (!ce_cdev->pcdev) {
+		SS_ERR("cdev_alloc fail\n");
+		ret = -ENOMEM;
+		goto err0;
+	}
+
+	/*bing ce_fops*/
+	cdev_init(ce_cdev->pcdev, &ce_fops);
+	ce_cdev->pcdev->owner = THIS_MODULE;
+
+	/*register cdev*/
+	ret = cdev_add(ce_cdev->pcdev, ce_cdev->devid, 1);
+	if (ret) {
+		SS_ERR("cdev_add fail\n");
+		ret = -1;
+		goto err0;
+	}
+
+	/*create device note*/
+	ce_cdev->pclass = class_create(THIS_MODULE, SUNXI_SS_DEV_NAME);
+	if (IS_ERR(ce_cdev->pclass)) {
+		SS_ERR("class_create fail\n");
+		ret = -1;
+		goto err0;
+	}
+	ce_cdev->pdevice = device_create(ce_cdev->pclass, NULL, ce_cdev->devid, NULL,
+						SUNXI_SS_DEV_NAME);
+
+	/*init task_pool*/
+	ce_cdev->task_pool = dma_pool_create("task_pool", ce_cdev->pdevice,
+			sizeof(struct ce_task_desc), 4, 0);
+	if (ce_cdev->task_pool == NULL)
+		return -ENOMEM;
+
+#ifdef CONFIG_OF
+	ce_cdev->pdevice->dma_mask = &sunxi_ss_dma_mask;
+	ce_cdev->pdevice->coherent_dma_mask = DMA_BIT_MASK(32);
+#endif
+
+	return 0;
+
+err0:
+	if (ce_cdev)
+		kfree(ce_cdev);
+	unregister_chrdev_region(ce_cdev->devid, 1);
+	return ret;
+
+}
+
+static int sunxi_ce_exit_cdev(void)
+{
+	device_destroy(ce_cdev->pclass, ce_cdev->devid);
+	class_destroy(ce_cdev->pclass);
+
+	unregister_chrdev_region(ce_cdev->devid, 1);
+
+	cdev_del(ce_cdev->pcdev);
+	kfree(ce_cdev);
+
+	if (ce_cdev->task_pool)
+		dma_pool_destroy(ce_cdev->task_pool);
+
+	return 0;
+}
+
+static int __init sunxi_ce_module_init(void)
+{
+	int ret = 0;
+
+	SS_DBG("sunxi_ce_cdev_init\n");
+	ret = sunxi_ce_setup_cdev();
+	if (ret < 0) {
+		SS_ERR("sunxi_ce_setup_cdev() failed, return %d\n", ret);
+		return ret;
+	}
+
+	ret = sunxi_ce_hw_init(ce_cdev);
+	if (ret < 0) {
+		SS_ERR("sunxi_ce_hw_init failed, return %d\n", ret);
+		return ret;
+	}
+	return ret;
+}
+
+static void __exit sunxi_ce_module_exit(void)
+{
+	SS_DBG("sunxi_ce_module_exit\n");
+	sunxi_ce_exit_cdev();
+	sunxi_ce_hw_exit();
+}
+
+module_init(sunxi_ce_module_init);
+module_exit(sunxi_ce_module_exit);
+
+MODULE_AUTHOR("mintow");
+MODULE_DESCRIPTION("SUNXI CE Controller Driver");
+MODULE_ALIAS("platform:"SUNXI_SS_DEV_NAME);
+MODULE_LICENSE("GPL");
diff --git a/drivers/crypto/sunxi-ce/sunxi_ce_cdev.h b/drivers/crypto/sunxi-ce/sunxi_ce_cdev.h
new file mode 100644
index 000000000..4baf31c73
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sunxi_ce_cdev.h
@@ -0,0 +1,473 @@
+/*
+ * Some macro and struct of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_CE_CDEV_H_
+#define _SUNXI_CE_CDEV_H_
+
+#include <crypto/aes.h>
+#include <crypto/sha.h>
+#include <crypto/hash.h>
+
+#include <linux/scatterlist.h>
+#include <linux/interrupt.h>
+#include <linux/cdev.h>
+
+#define SUNXI_SS_DEV_NAME		"ce"
+#define SUNXI_CE_DEV_NODE_NAME	"allwinner,sunxi-ce"
+
+/* flag for sunxi_ss_t.flags */
+#define SS_FLAG_MODE_MASK		0xFF
+#define SS_FLAG_NEW_KEY			BIT(0)
+#define SS_FLAG_NEW_IV			BIT(1)
+#define SS_FLAG_INIT			BIT(2)
+#define SS_FLAG_FAST			BIT(3)
+#define SS_FLAG_BUSY			BIT(4)
+#define SS_FLAG_TRNG			BIT(8)
+
+/* flag for crypto_async_request.flags */
+#define SS_FLAG_AES				BIT(16)
+#define SS_FLAG_HASH			BIT(17)
+
+/* Define the capability of SS controller. */
+#ifdef CONFIG_ARCH_SUN8IW6
+#define SS_CTR_MODE_ENABLE		1
+#define SS_CTS_MODE_ENABLE		1
+#define SS_SHA224_ENABLE		1
+#define SS_SHA256_ENABLE		1
+#define SS_TRNG_ENABLE			1
+#define SS_TRNG_POSTPROCESS_ENABLE	1
+#define SS_RSA512_ENABLE		1
+#define SS_RSA1024_ENABLE		1
+#define SS_RSA2048_ENABLE		1
+#define SS_RSA3072_ENABLE		1
+#define SS_IDMA_ENABLE			1
+#define SS_MULTI_FLOW_ENABLE	1
+
+#define SS_SHA_SWAP_PRE_ENABLE	1 /* The initial IV need to be converted. */
+
+#define SS_DMA_BUF_SIZE			SZ_8K
+
+#define SS_RSA_MIN_SIZE			(512/8)  /* in Bytes. 512 bits */
+#define SS_RSA_MAX_SIZE			(3072/8) /* in Bytes. 3072 bits */
+
+#define SS_FLOW_NUM				2
+#endif
+
+/*define CE_V1_X---->CE_DRIVER_V3_1*/
+#if defined(CONFIG_ARCH_SUN20IW1) || defined(CONFIG_ARCH_SUN8IW11)\
+	|| defined(CONFIG_ARCH_SUN50IW1) || defined(CONFIG_ARCH_SUN50IW2)\
+	|| defined(CONFIG_ARCH_SUN8IW12) || defined(CONFIG_ARCH_SUN8IW15)\
+	|| defined(CONFIG_ARCH_SUN8IW7) || defined(CONFIG_ARCH_SUN50IW8)\
+	|| defined(CONFIG_ARCH_SUN8IW17) || defined(CONFIG_ARCH_SUN8IW18)\
+	|| defined(CONFIG_ARCH_SUN8IW16) || defined(CONFIG_ARCH_SUN8IW19)\
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW11)\
+
+#define SS_SUPPORT_CE_V3_1		1
+#define SS_SCATTER_ENABLE		1
+#define	TASK_DMA_POOL			1
+#define SS_SHA_SWAP_PRE_ENABLE	1 /* The initial IV need to be converted. */
+#endif
+
+/*define CE_V2_X---->CE_DRIVER_V3_2*/
+#if defined(CONFIG_ARCH_SUN50IW3) || defined(CONFIG_ARCH_SUN50IW6) || \
+	defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW8) || \
+	defined(CONFIG_ARCH_SUN8IW16) || defined(CONFIG_ARCH_SUN50IW9) || \
+	defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) || \
+	defined(CONFIG_ARCH_SUN50IW12) || defined(CONFIG_ARCH_SUN8IW21)
+
+#define SS_SUPPORT_CE_V3_2		1
+#define SS_SCATTER_ENABLE		1
+#define	TASK_DMA_POOL			1
+
+#define SS_SHA_SWAP_PRE_ENABLE	1 /* The initial IV need to be converted. */
+
+#if defined(CONFIG_ARCH_SUN50IW12) || defined(CONFIG_ARCH_SUN8IW21)
+#define	CE_BYTE_ADDR_ENABLE
+#endif
+
+#endif
+
+#ifdef CONFIG_EVB_PLATFORM
+#define SS_TRNG_ENABLE				1
+#define SS_TRNG_POSTPROCESS_ENABLE	1
+#endif
+
+#if defined(SS_SUPPORT_CE_V3_1) || defined(SS_SUPPORT_CE_V3_2)
+#define SS_CTR_MODE_ENABLE		1
+#define SS_CTS_MODE_ENABLE		1
+#define SS_OFB_MODE_ENABLE		1
+#define SS_CFB_MODE_ENABLE		1
+
+#define SS_SHA224_ENABLE		1
+#define SS_SHA256_ENABLE		1
+#define SS_SHA384_ENABLE		1
+#define SS_SHA512_ENABLE		1
+#define SS_HMAC_SHA1_ENABLE		1
+#define SS_HMAC_SHA256_ENABLE	1
+
+#define SS_RSA512_ENABLE		1
+#define SS_RSA1024_ENABLE		1
+#define SS_RSA2048_ENABLE		1
+
+#define SS_DH512_ENABLE			1
+#define SS_DH1024_ENABLE		1
+#define SS_DH2048_ENABLE		1
+
+#define SS_ECC_ENABLE			1
+#endif	/*SS_SUPPORT_CE_V3_1*/
+
+
+#if defined(SS_SUPPORT_CE_V3_2)
+#define SS_XTS_MODE_ENABLE		1
+
+#define SS_RSA3072_ENABLE		1
+#define SS_RSA4096_ENABLE		1
+
+#define SS_DH3072_ENABLE		1
+#define SS_DH4096_ENABLE		1
+
+#define SS_HASH_HW_PADDING		1
+#define SS_HASH_HW_PADDING_ALIGN_CASE	1
+
+#if defined(CONFIG_ARCH_SUN50IW8) || defined(CONFIG_ARCH_SUN50IW10) || \
+	defined(CONFIG_ARCH_SUN50IW12) || defined(CONFIG_ARCH_SUN8IW21)
+
+#define SS_GCM_MODE_ENABLE	1
+#define SS_DRBG_MODE_ENABLE	1
+#endif
+
+#undef SS_TRNG_POSTPROCESS_ENABLE
+
+#endif	/*SS_SUPPORT_CE_V3_2*/
+
+
+#define SS_RSA_MIN_SIZE			(512/8)  /* in Bytes. 512 bits */
+
+#if defined(CONFIG_ARCH_SUN8IW7)
+#define SS_RSA_MAX_SIZE			(4096/8) /*in Bytes. 4096 bits*/
+#else
+#define SS_RSA_MAX_SIZE			(2048/8) /* in Bytes. 2048 bits */
+#endif
+
+#ifdef CONFIG_ARCH_SUN8IW7
+#define SS_FLOW_NUM			1
+#else
+#define SS_FLOW_NUM				4
+#endif
+
+#if defined(CONFIG_ARCH_SUN8IW7)
+#define SS_ECC_ENABLE                   1
+#define SS_RSA_PREPROCESS_ENABLE        1
+#define SS_RSA_CLK_ENABLE		1
+#endif
+
+#if defined(SS_RSA512_ENABLE) || defined(SS_RSA1024_ENABLE) \
+	|| defined(SS_RSA2048_ENABLE) || defined(SS_RSA3072_ENABLE) \
+	|| defined(SS_RSA4096_ENABLE)
+#define SS_RSA_ENABLE			1
+#endif
+
+#if defined(SS_DH512_ENABLE) || defined(SS_DH1024_ENABLE) \
+	|| defined(SS_DH2048_ENABLE) || defined(SS_DH3072_ENABLE) \
+	|| defined(SS_DH4096_ENABLE)
+#define SS_DH_ENABLE			1
+#endif
+
+#define SS_PRNG_SEED_LEN		(192/8) /* 192 bits */
+#define SS_RNG_MAX_LEN			SZ_8K
+
+
+#if defined(SS_SHA384_ENABLE) || defined(SS_SHA512_ENABLE)
+#define SS_DIGEST_SIZE		SHA512_DIGEST_SIZE
+#define SS_HASH_PAD_SIZE	(SHA512_BLOCK_SIZE * 2)
+#else
+#define SS_DIGEST_SIZE		SHA256_DIGEST_SIZE
+#define SS_HASH_PAD_SIZE	(SHA1_BLOCK_SIZE * 2)
+#endif
+
+#ifdef CONFIG_EVB_PLATFORM
+#define SS_WAIT_TIME	2000 /* 2s, used in wait_for_completion_timeout() */
+#else
+#define SS_WAIT_TIME	40000 /* 40s, used in wait_for_completion_timeout() */
+#endif
+
+#define SS_ALG_PRIORITY		260
+
+#if defined(CONFIG_ARCH_SUN50IW9) || defined(CONFIG_ARCH_SUN50IW10)
+#define WORD_ALGIN		(2)
+#else
+#define WORD_ALGIN		(0)
+#endif
+
+
+/* For debug */
+#define SS_DBG(fmt, arg...) pr_debug("%s()%d - "fmt, __func__, __LINE__, ##arg)
+#define SS_ERR(fmt, arg...) pr_err("%s()%d - "fmt, __func__, __LINE__, ##arg)
+#define SS_EXIT()	    SS_DBG("%s\n", "Exit")
+#define SS_ENTER()	    SS_DBG("%s\n", "Enter ...")
+
+#define SS_FLOW_AVAILABLE	0
+#define SS_FLOW_UNAVAILABLE	1
+
+#define SS_RES_NS_INDEX	0
+#define SS_RES_S_INDEX	1
+#ifdef CONFIG_EVB_PLATFORM
+#define SS_RES_INDEX	SS_RES_NS_INDEX
+#else
+#define SS_RES_INDEX	SS_RES_NS_INDEX
+#endif
+
+
+#ifdef SS_SCATTER_ENABLE
+/* CE: Crypto Engine, start using CE from sun8iw7/sun8iw9 */
+#define CE_SCATTERS_PER_TASK		8
+
+#define CE_ADDR_MASK	0xffffffffff
+
+/* The descriptor of a CE task. */
+#ifdef CE_BYTE_ADDR_ENABLE
+typedef struct {
+	u8 src_addr[5];
+	u8 dst_addr[5];
+	u8 pad[2];
+	u32 src_len;
+	u32 dst_len;
+} ce_scatter_t;
+
+typedef struct ce_task_desc {
+	u32 chan_id;
+	u32 comm_ctl;
+	u32 sym_ctl;
+	u32 asym_ctl;
+
+	u8 key_addr[5];
+	u8 iv_addr[5];
+	u8 ctr_addr[5];
+	u8 pad;
+	u32 data_len; /* in word(4 byte). Exception: in byte for AES_CTS */
+
+	ce_scatter_t ce_sg[CE_SCATTERS_PER_TASK];
+
+	u8 next_sg_addr[5];
+	u8 next_task_addr[5];
+	u8 pad2[2];
+	u32 reserved[3];
+	struct ce_task_desc *next_virt;
+	dma_addr_t task_phy_addr;
+} ce_task_desc_t;
+
+/* The descriptor of a CE hash and rng task. */
+typedef struct ce_new_task_desc {
+	u32 comm_ctl;
+	u32 main_cmd;
+	u8 data_len[5];
+	u8 key_addr[5];
+	u8 iv_addr[5];
+	u8 pad;
+
+	ce_scatter_t ce_sg[CE_SCATTERS_PER_TASK];
+
+	u8 next_sg_addr[5];
+	u8 next_task_addr[5];
+	u8 pad2[2];
+	u32 reserved[3];
+	struct ce_task_desc *next_virt;
+	dma_addr_t task_phy_addr;
+} ce_new_task_desc_t;
+#else
+typedef struct {
+	u32 addr;
+	u32 len; /* in word (4 bytes). Exception: in byte for AES_CTS */
+} ce_scatter_t;
+
+/* The descriptor of a CE task. */
+typedef struct ce_task_desc {
+	u32 chan_id;
+	u32 comm_ctl;
+	u32 sym_ctl;
+	u32 asym_ctl;
+
+	u32 key_addr;
+	u32 iv_addr;
+	u32 ctr_addr;
+	u32 data_len; /* in word(4 byte). Exception: in byte for AES_CTS */
+
+	ce_scatter_t src[CE_SCATTERS_PER_TASK];
+	ce_scatter_t dst[CE_SCATTERS_PER_TASK];
+
+	//struct ce_task_desc *next;
+	u32 next_task_addr;
+	u32 reserved[3];
+	struct ce_task_desc *next_virt;
+	dma_addr_t task_phy_addr;
+} ce_task_desc_t;
+
+/* The descriptor of a CE hash and rng task. */
+typedef struct ce_new_task_desc {
+	u32 common_ctl;
+	u32 main_cmd;
+	u32 data_len;
+	u32 key_addr;
+	u32 iv_addr;
+
+	ce_scatter_t src[CE_SCATTERS_PER_TASK];
+	ce_scatter_t dst[CE_SCATTERS_PER_TASK];
+	struct ce_new_task_desc *next;
+	u32 reserved[3];
+} ce_new_task_desc_t;
+#endif
+#endif
+
+struct ce_scatter_mapping {
+	void *virt_addr;
+	u32 data_len;
+};
+
+typedef struct {
+	u32 dir;
+	u32 nents;
+	struct dma_chan *chan;
+	struct scatterlist *sg;
+#ifdef SS_IDMA_ENABLE
+	struct sg_table sgt_for_cp; /* Used to copy data from/to user space. */
+#endif
+#ifdef SS_SCATTER_ENABLE
+	u32 has_padding;
+	u8 *padding;
+	struct scatterlist *last_sg;
+	struct ce_scatter_mapping mapping[CE_SCATTERS_PER_TASK];
+#endif
+} ss_dma_info_t;
+
+typedef struct {
+	u32 dir;
+	u32 type;
+	u32 mode;
+#ifdef SS_CFB_MODE_ENABLE
+	u32 bitwidth;	/* the bitwidth of CFB mode */
+#endif
+	struct completion done;
+	ss_dma_info_t dma_src;
+	ss_dma_info_t dma_dst;
+} ss_aes_req_ctx_t;
+
+/* The common context of AES and HASH */
+typedef struct {
+	u32 flow;
+	u32 flags;
+} ss_comm_ctx_t;
+
+typedef struct {
+	ss_comm_ctx_t comm; /* must be in the front. */
+
+#ifdef SS_RSA_ENABLE
+	u8 key[SS_RSA_MAX_SIZE];
+	u8 iv[SS_RSA_MAX_SIZE];
+#else
+	u8 key[AES_MAX_KEY_SIZE];
+	u8 iv[AES_MAX_KEY_SIZE];
+#endif
+#ifdef SS_SCATTER_ENABLE
+	u8 next_iv[AES_MAX_KEY_SIZE]; /* the next IV/Counter in continue mode */
+#endif
+
+	u32 key_size;
+	u32 iv_size;
+	u32 cnt;	/* in Byte */
+} ss_aes_ctx_t;
+
+typedef struct {
+	ss_comm_ctx_t comm; /* must be in the front. */
+
+	u8  md[SS_DIGEST_SIZE];
+	u8  pad[SS_HASH_PAD_SIZE];
+	u32 tail_len;
+	u32 md_size;
+	u32 cnt;	/* in Byte */
+} ss_hash_ctx_t;
+
+typedef struct {
+#ifdef SS_IDMA_ENABLE
+	char *buf_src;
+	dma_addr_t buf_src_dma;
+	char *buf_dst;
+	dma_addr_t buf_dst_dma;
+#endif
+#ifdef SS_SCATTER_ENABLE
+	ce_task_desc_t task;
+#endif
+	struct completion done;
+	u32 available;
+} ss_flow_t;
+
+typedef struct {
+#ifdef SS_SCATTER_ENABLE
+	ce_task_desc_t *task;
+#endif
+	struct completion done;
+	u32 available;
+	u32 buf_pendding;
+} ce_channel_t;
+
+typedef struct {
+	struct cdev *pcdev;
+	dev_t devid;
+	struct class *pclass;
+	struct device *pdevice;
+	struct device_node *pnode;
+	void __iomem *base_addr;
+	ce_channel_t flows[SS_FLOW_NUM];
+	struct clk *ce_clk;
+	struct reset_control *reset;
+	s8  dev_name[8];
+	spinlock_t lock;
+	u32 flag;
+	u32 irq;
+	struct dma_pool	*task_pool;
+} sunxi_ce_cdev_t;
+
+extern sunxi_ce_cdev_t	*ce_cdev;
+
+typedef struct {
+	u8 *src_buffer;
+	u32 src_length;
+	u8 *dst_buffer;
+	u32 dst_length;
+	u8 *key_buffer;
+	u32 key_length;
+	u8 *iv_buf;
+	u32 iv_length;
+	u32 aes_mode;
+	u32 dir;
+	u32 ion_flag;
+	unsigned long src_phy;
+	unsigned long dst_phy;
+	unsigned long iv_phy;
+	unsigned long key_phy;
+	s32 channel_id;
+} crypto_aes_req_ctx_t;
+
+/*ioctl cmd*/
+#define CE_IOC_MAGIC			'C'
+#define CE_IOC_REQUEST			_IOR(CE_IOC_MAGIC, 0, int)
+#define CE_IOC_FREE				_IOW(CE_IOC_MAGIC, 1, int)
+#define CE_IOC_AES_CRYPTO		_IOW(CE_IOC_MAGIC, 2, crypto_aes_req_ctx_t)
+
+/* Inner functions declaration */
+void ce_dev_lock(void);
+void ce_dev_unlock(void);
+void ce_reset(void);
+void __iomem *ss_membase(void);
+int do_aes_crypto(crypto_aes_req_ctx_t *req);
+irqreturn_t sunxi_ce_irq_handler(int irq, void *dev_id);
+
+#endif /* end of _SUNXI_CE_CDEV_H_ */
diff --git a/drivers/crypto/sunxi-ce/sunxi_ce_proc.h b/drivers/crypto/sunxi-ce/sunxi_ce_proc.h
new file mode 100644
index 000000000..349995a56
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sunxi_ce_proc.h
@@ -0,0 +1,79 @@
+/*
+ * Declare the function interface of SUNXI SS process.
+ *
+ * Copyright (C) 2014 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_SECURITY_SYSTEM_PROC_H_
+#define _SUNXI_SECURITY_SYSTEM_PROC_H_
+
+#include <crypto/aes.h>
+#include <crypto/sha.h>
+#include <crypto/algapi.h>
+#include <linux/scatterlist.h>
+
+#define SRC_FLAG	(0x0)
+#define DST_FLAG	(0x1)
+/* Inner functions declaration, defined in vx/sunxi_ss_proc.c */
+
+int ss_aes_key_valid(struct crypto_ablkcipher *tfm, int len);
+int ss_aes_one_req(sunxi_ss_t *sss, struct ablkcipher_request *req);
+
+#ifdef SS_GCM_MODE_ENABLE
+int ss_aead_crypt(struct aead_request *req, int dir, int method, int mode);
+int ss_aead_one_req(sunxi_ss_t *sss, struct aead_request *req);
+#endif
+
+#ifdef SS_DRBG_MODE_ENABLE
+int ss_drbg_get_random(struct crypto_rng *tfm, const u8 *src, u32 slen,
+				u8 *rdata, u32 dlen, u32 mode);
+int ss_drbg_sha1_get_random(struct crypto_rng *tfm, const u8 *src,
+				unsigned int slen, u8 *dst, unsigned int dlen);
+int ss_drbg_sha256_get_random(struct crypto_rng *tfm, const u8 *src,
+				unsigned int slen, u8 *dst, unsigned int dlen);
+int ss_drbg_sha512_get_random(struct crypto_rng *tfm, const u8 *src,
+				unsigned int slen, u8 *dst, unsigned int dlen);
+#endif
+
+int ss_rng_get_random(struct crypto_rng *tfm, u8 *rdata, u32 dlen, u32 trng);
+
+u32 ss_hash_start(ss_hash_ctx_t *ctx,
+		ss_aes_req_ctx_t *req_ctx, u32 len, u32 last);
+
+irqreturn_t sunxi_ss_irq_handler(int irq, void *dev_id);
+
+/* defined in sunxi_ss_proc_comm.c */
+
+void ss_print_hex(char *_data, int _len, void *_addr);
+#ifdef SS_SCATTER_ENABLE
+void ss_print_task_info(ce_task_desc_t *task);
+#endif
+int ss_sg_cnt(struct scatterlist *sg, int total);
+
+int ss_prng_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+		u8 *dst, unsigned int dlen);
+#ifdef SS_TRNG_ENABLE
+int ss_trng_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+		u8 *dst, unsigned int dlen);
+#endif
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+void ss_trng_postprocess(u8 *out, u32 outlen, u8 *in, u32 inlen);
+#endif
+
+int ss_aes_crypt(struct ablkcipher_request *req, int dir, int method, int mode);
+
+void ss_hash_swap(char *data, int len);
+int ss_hash_blk_size(int type);
+void ss_hash_padding_sg_prepare(struct scatterlist *last, int total);
+int ss_hash_update(struct ahash_request *req);
+int ss_hash_final(struct ahash_request *req);
+int ss_hash_finup(struct ahash_request *req);
+int ss_hash_digest(struct ahash_request *req);
+
+#endif /* end of _SUNXI_SECURITY_SYSTEM_PROC_H_ */
diff --git a/drivers/crypto/sunxi-ce/sunxi_ce_proc_comm.c b/drivers/crypto/sunxi-ce/sunxi_ce_proc_comm.c
new file mode 100644
index 000000000..c04b46327
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/sunxi_ce_proc_comm.c
@@ -0,0 +1,416 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2014 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/internal/aead.h>
+#include <crypto/hash.h>
+
+#include "sunxi_ce.h"
+#include "sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+void ss_print_hex(char *_data, int _len, void *_addr)
+{
+	int i;
+
+	pr_debug("---------------- The valid len = %d ----------------\n",
+		_len);
+	for (i = 0; i < _len/8; i++) {
+		pr_debug("0x%px: %02X %02X %02X %02X %02X %02X %02X %02X\n",
+			i*8 + _addr,
+			_data[i*8+0], _data[i*8+1], _data[i*8+2], _data[i*8+3],
+			_data[i*8+4], _data[i*8+5], _data[i*8+6], _data[i*8+7]);
+	}
+	pr_debug("----------------------------------------------------\n");
+}
+
+int ss_sg_cnt(struct scatterlist *sg, int total)
+{
+	int cnt = 0;
+	int nbyte = 0;
+	struct scatterlist *cur = sg;
+	struct scatterlist *prev = sg;
+
+	while (cur != NULL) {
+		cnt++;
+		prev = cur;
+		SS_DBG("cnt: %d, cur: %px, len: %d, is_last: %ld\n", cnt, cur,
+				cur->length, sg_is_last(cur));
+		nbyte += cur->length;
+		if (nbyte >= total)
+			break;
+
+		cur = sg_next(cur);
+	}
+
+	if (!sg_is_last(prev))
+		sg_mark_end(prev);
+	return cnt;
+}
+
+int ss_aes_crypt(struct ablkcipher_request *req, int dir, int method, int mode)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+
+	SS_DBG("nbytes: %d, dec: %d, method: %d, mode: %d\n",
+		req->nbytes, dir, method, mode);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	req_ctx->dir  = dir;
+	req_ctx->type = method;
+	req_ctx->mode = mode;
+	req->base.flags |= SS_FLAG_AES;
+
+	return ss_aes_one_req(ss_dev, req);
+}
+
+#ifdef SS_GCM_MODE_ENABLE
+int ss_aead_crypt(struct aead_request *req, int dir, int method, int mode)
+{
+	ss_aes_req_ctx_t *req_ctx = aead_request_ctx(req);
+
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	req_ctx->dir  = dir;
+	req_ctx->type = method;
+	req_ctx->mode = mode;
+	req->base.flags |= SS_FLAG_AES;
+
+	return ss_aead_one_req(ss_dev, req);
+}
+#endif
+
+int ss_prng_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+				u8 *dst, unsigned int dlen)
+{
+	return ss_rng_get_random(tfm, dst, dlen, 0);
+}
+
+#ifdef SS_TRNG_ENABLE
+int ss_trng_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+				u8 *dst, unsigned int dlen)
+{
+	return ss_rng_get_random(tfm, dst, dlen, 1);
+}
+#endif
+
+#ifdef SS_DRBG_MODE_ENABLE
+int ss_drbg_sha1_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+				u8 *dst, unsigned int dlen)
+{
+	return ss_drbg_get_random(tfm, src, slen, dst, dlen, SS_METHOD_SHA1);
+}
+
+int ss_drbg_sha256_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+				u8 *dst, unsigned int dlen)
+{
+	return ss_drbg_get_random(tfm, src, slen, dst, dlen, SS_METHOD_SHA256);
+}
+
+int ss_drbg_sha512_get_random(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
+				u8 *dst, unsigned int dlen)
+{
+	return ss_drbg_get_random(tfm, src, slen, dst, dlen, SS_METHOD_SHA512);
+}
+#endif
+
+#if defined(SS_SHA_SWAP_PRE_ENABLE) || defined(SS_SHA_SWAP_FINAL_ENABLE)
+void ss_hash_swap(char *data, int len)
+{
+	int i;
+	int temp = 0;
+	int *cur = (int *)data;
+
+	SS_DBG("Convert the byter-order of digest. len %d\n", len);
+	for (i = 0; i < len/4; i++, cur++) {
+		temp = cpu_to_be32(*cur);
+		*cur = temp;
+	}
+}
+#endif
+
+int ss_hash_blk_size(int type)
+{
+#if defined(SS_SHA384_ENABLE) || defined(SS_SHA512_ENABLE)
+	if ((type == SS_METHOD_SHA384) || (type == SS_METHOD_SHA512))
+		return SHA512_BLOCK_SIZE;
+#endif
+	return SHA1_BLOCK_SIZE;
+}
+
+/* The tail data will be processed later. */
+void ss_hash_padding_sg_prepare(struct scatterlist *last, int total)
+{
+	if (total%SHA1_BLOCK_SIZE != 0) {
+		SS_DBG("sg len: %d, total: %d\n", sg_dma_len(last), total);
+		WARN(sg_dma_len(last) < total%SHA1_BLOCK_SIZE,
+			"sg len: %d, total: %d\n", sg_dma_len(last), total);
+		sg_dma_len(last) = sg_dma_len(last) - total%SHA1_BLOCK_SIZE;
+	}
+	WARN_ON(sg_dma_len(last) > total);
+}
+
+int ss_hash_hw_padding(ss_hash_ctx_t *ctx, int type)
+{
+	SS_DBG("type: %d, n: %d, cnt: %d\n", type, ctx->tail_len, ctx->cnt);
+	return SS_HASH_PAD_SIZE;
+}
+
+int ss_hash_sw_padding(ss_hash_ctx_t *ctx, int type)
+{
+	int blk_size = ss_hash_blk_size(type);
+	int len_threshold = blk_size == SHA512_BLOCK_SIZE ? 112 : 56;
+	int n = ctx->tail_len;
+	u8 *p = ctx->pad;
+	int len_l = ctx->cnt << 3;  /* total len, in bits. */
+	int len_h = ctx->cnt >> 29;
+	int big_endian = type == SS_METHOD_MD5 ? 0 : 1;
+
+	SS_DBG("type = %d, n = %d, ctx->cnt = %d\n", type, n, ctx->cnt);
+	p[n] = 0x80;
+	n++;
+
+	if (n > len_threshold) { /* The pad data need two blocks. */
+		memset(p+n, 0, blk_size*2 - n);
+		p += blk_size*2 - 8;
+	} else {
+		memset(p+n, 0, blk_size - n);
+		p += blk_size - 8;
+	}
+
+	if (big_endian == 1) {
+#if 0
+		/* The length should use bit64 in SHA384/512 case.
+		 * The OpenSSL package is always small than 8K,
+		 * so we use still bit32.
+		 */
+		if (blk_size == SHA512_BLOCK_SIZE) {
+			int len_hh = ctx->cnt >> 61;
+			*(int *)(p-4) = swab32(len_hh);
+		}
+#endif
+		*(int *)p = swab32(len_h);
+		*(int *)(p+4) = swab32(len_l);
+	} else {
+		*(int *)p = len_l;
+		*(int *)(p+4) = len_h;
+	}
+
+	SS_DBG("After padding %d: %02x %02x %02x %02x   %02x %02x %02x %02x\n",
+			(s32)(p + 8 - ctx->pad),
+			p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7]);
+
+	return p + 8 - ctx->pad;
+}
+
+int ss_hash_padding(ss_hash_ctx_t *ctx, int type)
+{
+#ifdef SS_HASH_HW_PADDING
+#ifdef SS_HASH_HW_PADDING_ALIGN_CASE
+	if (ctx->tail_len == 0)
+		return ss_hash_sw_padding(ctx, type);
+#endif
+	return ss_hash_hw_padding(ctx, type);
+#else
+	return ss_hash_sw_padding(ctx, type);
+#endif
+}
+
+static int ss_hash_one_req(sunxi_ss_t *sss, struct ahash_request *req)
+{
+	int ret = 0;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+	ss_hash_ctx_t *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));
+
+	SS_ENTER();
+	if (!req->src) {
+		SS_ERR("Invalid sg: src = %px\n", req->src);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	req_ctx = ahash_request_ctx(req);
+	req_ctx->dma_src.sg = req->src;
+
+	ret = ss_hash_start(ctx, req_ctx, req->nbytes, 0);
+	if (ret < 0)
+		SS_ERR("ss_hash_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+	return ret;
+}
+
+/* Backup the tail data to req_ctx->pad[]. */
+void ss_hash_save_tail(struct ahash_request *req)
+{
+	s8 *buf = NULL;
+	s32 sg_cnt = 0;
+	s32 taillen = 0;
+	ss_aes_req_ctx_t *req_ctx = ahash_request_ctx(req);
+	ss_hash_ctx_t *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));
+
+	taillen = req->nbytes % ss_hash_blk_size(req_ctx->type);
+	SS_DBG("type: %d, mode: %d, len: %d, tail: %d\n", req_ctx->type,
+		req_ctx->mode, req->nbytes, taillen);
+	ctx->tail_len = taillen;
+	if (taillen == 0) /* The package don't need to backup. */
+		return;
+
+	buf = vmalloc(req->nbytes);
+	if (unlikely(buf == NULL)) {
+		SS_ERR("Fail to vmalloc(%d)\n", req->nbytes);
+		return;
+	}
+
+	sg_cnt = ss_sg_cnt(req->src, req->nbytes);
+	sg_copy_to_buffer(req->src, sg_cnt, buf, req->nbytes);
+
+	memcpy(ctx->pad, buf + req->nbytes - taillen, taillen);
+	vfree(buf);
+}
+
+int ss_hash_update(struct ahash_request *req)
+{
+	if (!req->nbytes) {
+		SS_ERR("Invalid length: %d.\n", req->nbytes);
+		return 0;
+	}
+	ss_hash_save_tail(req);
+
+	SS_DBG("Flags: %#x, len = %d\n", req->base.flags, req->nbytes);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	req->base.flags |= SS_FLAG_HASH;
+	return ss_hash_one_req(ss_dev, req);
+}
+
+int ss_hash_final(struct ahash_request *req)
+{
+	int pad_len = 0;
+	ss_aes_req_ctx_t *req_ctx = ahash_request_ctx(req);
+	ss_hash_ctx_t *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));
+	struct scatterlist last = {0}; /* make a sg struct for padding data. */
+
+	if (req->result == NULL) {
+		SS_ERR("Invalid result porinter.\n");
+		return -EINVAL;
+	}
+	SS_DBG("Method: %d, cnt: %d\n", req_ctx->type, ctx->cnt);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	/* Process the padding data. */
+	pad_len = ss_hash_padding(ctx, req_ctx->type);
+	SS_DBG("Pad len: %d\n", pad_len);
+	req_ctx->dma_src.sg = &last;
+	sg_init_table(&last, 1);
+	sg_set_buf(&last, ctx->pad, pad_len);
+	SS_DBG("Padding data:\n");
+	ss_print_hex((s8 *)ctx->pad, 128, ctx->pad);
+
+	ss_dev_lock();
+#ifdef SS_HASH_HW_PADDING_ALIGN_CASE
+	if (ctx->tail_len == 0)
+		ss_hash_start(ctx, req_ctx, pad_len, 0);
+	else
+#endif
+		ss_hash_start(ctx, req_ctx, pad_len, 1);
+
+	ss_sha_final();
+
+	SS_DBG("Method: %d, cnt: %d\n", req_ctx->type, ctx->cnt);
+
+	ss_check_sha_end();
+	ss_md_get(req->result, ctx->md, ctx->md_size);
+	ss_ctrl_stop();
+	ss_dev_unlock();
+
+#ifdef SS_SHA_SWAP_FINAL_ENABLE
+	if (req_ctx->type != SS_METHOD_MD5)
+		ss_hash_swap(req->result, ctx->md_size);
+#endif
+
+	return 0;
+}
+
+int ss_hash_finup(struct ahash_request *req)
+{
+	ss_hash_update(req);
+	return ss_hash_final(req);
+}
+
+int ss_hash_digest(struct ahash_request *req)
+{
+	crypto_ahash_reqtfm(req)->init(req);
+	ss_hash_update(req);
+	return ss_hash_final(req);
+}
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+void ss_trng_postprocess(u8 *out, u32 outlen, u8 *in, u32 inlen)
+{
+	s32 i;
+	int ret;
+	u32 left = inlen;
+	s8 result[SHA256_DIGEST_SIZE] = "";
+	struct scatterlist sg = {0};
+	struct crypto_ahash *tfm = NULL;
+	struct ahash_request *req = NULL;
+
+	tfm = crypto_alloc_ahash("sha256", CRYPTO_ALG_TYPE_AHASH, CRYPTO_ALG_TYPE_AHASH_MASK);
+	if (IS_ERR(tfm)) {
+		SS_ERR("Fail to alloc ahash tfm!\n");
+	}
+
+	req = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!req) {
+		SS_ERR("Fail to alloc ahash request!\n");
+	}
+
+	ahash_request_set_callback(req, 0, NULL, NULL);
+
+	for (i = 0; i < inlen/SHA256_BLOCK_SIZE; i++) {
+		sg_init_one(&sg, in+i*SHA256_BLOCK_SIZE, SHA256_BLOCK_SIZE);
+		ahash_request_set_crypt(req, &sg, result, SHA256_BLOCK_SIZE);
+		ret = crypto_ahash_digest(req);
+		if (ret) {
+			SS_ERR("Fail to do %d SHA256(), inlen: %d\n", i, inlen);
+			break;
+		}
+		if (left < SHA256_DIGEST_SIZE) {
+			memcpy(&out[i*SHA256_DIGEST_SIZE], req->result, inlen);
+			break;
+		}
+		memcpy(&out[i*SHA256_DIGEST_SIZE], req->result, SHA256_DIGEST_SIZE);
+		left -= SHA256_DIGEST_SIZE;
+	}
+
+	ahash_request_free(req);
+	crypto_free_ahash(tfm);
+}
+#endif
diff --git a/drivers/crypto/sunxi-ce/v2/sunxi_ce_proc.c b/drivers/crypto/sunxi-ce/v2/sunxi_ce_proc.c
new file mode 100644
index 000000000..4e1500823
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v2/sunxi_ce_proc.c
@@ -0,0 +1,618 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+
+#include "../sunxi_ce.h"
+#include "../sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+/* Callback of DMA completion. */
+static void ss_dma_cb(void *data)
+{
+	ss_aes_req_ctx_t *req_ctx = (ss_aes_req_ctx_t *)data;
+
+	SS_DBG("DMA transfer data complete!\n");
+
+	complete(&req_ctx->done);
+}
+
+/* request dma channel and set callback function */
+static int ss_dma_prepare(ss_dma_info_t *info)
+{
+	dma_cap_mask_t mask;
+
+	/* Try to acquire a generic DMA engine slave channel */
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_SG, mask);
+	dma_cap_set(DMA_MEMCPY, mask);
+	info->chan = dma_request_channel(mask, NULL, NULL);
+	if (info->chan == NULL) {
+		SS_ERR("Request DMA() failed!\n");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/* set dma start flag, if queue, it will auto restart to transfer next queue */
+static void ss_dma_start(ss_dma_info_t *info)
+{
+	dma_async_issue_pending(info->chan);
+}
+
+/* Make a sg_table based on sg[] of crypto request. */
+static int ss_sg_table_init(struct sg_table *sgt,
+	struct scatterlist *sg, int len, char *vbase, dma_addr_t pbase)
+{
+	int i;
+	int npages = 0;
+	int offset = 0;
+	struct scatterlist *src_sg = sg;
+	struct scatterlist *dst_sg = NULL;
+
+	npages = ss_sg_cnt(sg, len);
+	WARN_ON(npages == 0);
+
+	if (sg_alloc_table(sgt, npages, GFP_KERNEL)) {
+		SS_ERR("sg_alloc_table(%d) failed!\n", npages);
+		WARN_ON(1);
+	}
+
+	dst_sg = sgt->sgl;
+	for (i = 0; i < npages; i++) {
+		sg_set_buf(dst_sg, vbase + offset, sg_dma_len(src_sg));
+		offset += sg_dma_len(src_sg);
+		src_sg = sg_next(src_sg);
+		dst_sg = sg_next(dst_sg);
+	}
+	return 0;
+}
+
+static int ss_dma_dst_config(sunxi_ss_t *sss,
+	void *ctx, ss_aes_req_ctx_t *req_ctx, int len, int cb)
+{
+	int flow = ((ss_comm_ctx_t *)ctx)->flow;
+	ss_dma_info_t *info = &req_ctx->dma_dst;
+	struct dma_slave_config dma_conf = {0};
+	struct dma_async_tx_descriptor *dma_desc = NULL;
+
+	info->dir = DMA_MEM_TO_MEM;
+	dma_conf.direction = info->dir;
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR)
+		dma_conf.src_addr_width = dma_conf.dst_addr_width
+			= DMA_SLAVE_BUSWIDTH_1_BYTE;
+	else
+#endif
+	dma_conf.src_addr_width = dma_conf.dst_addr_width
+			= DMA_SLAVE_BUSWIDTH_4_BYTES;
+
+	dma_conf.src_maxburst = 1;
+	dma_conf.dst_maxburst = 1;
+	dma_conf.slave_id = sunxi_slave_id(DRQDST_SDRAM, DRQSRC_SDRAM);
+	dmaengine_slave_config(info->chan, &dma_conf);
+
+	ss_sg_table_init(&info->sgt_for_cp, info->sg, len,
+		sss->flows[flow].buf_dst, sss->flows[flow].buf_dst_dma);
+
+	info->nents = info->sgt_for_cp.nents;
+	SS_DBG("flow: %d, sg num: %d, total len: %d\n", flow, info->nents, len);
+
+	dma_map_sg(&sss->pdev->dev, info->sg, info->nents, info->dir);
+	dma_map_sg(&sss->pdev->dev, info->sgt_for_cp.sgl,
+		info->nents, info->dir);
+
+	dma_desc = info->chan->device->device_prep_dma_sg(info->chan,
+			info->sg, info->nents,
+			info->sgt_for_cp.sgl, info->nents,
+			DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!dma_desc) {
+		SS_ERR("dmaengine_prep_slave_sg() failed!\n");
+		return -1;
+	}
+
+	if (cb == 1) {
+		dma_desc->callback = ss_dma_cb;
+		dma_desc->callback_param = (void *)req_ctx;
+	}
+	dmaengine_submit(dma_desc);
+
+	return 0;
+}
+
+static int ss_dma_src_config(sunxi_ss_t *sss,
+	void *ctx, ss_aes_req_ctx_t *req_ctx, int len, int cb)
+{
+	int flow = ((ss_comm_ctx_t *)ctx)->flow;
+	ss_dma_info_t *info = &req_ctx->dma_src;
+	struct dma_slave_config dma_conf = {0};
+	struct dma_async_tx_descriptor *dma_desc = NULL;
+
+	info->dir = DMA_MEM_TO_MEM;
+	dma_conf.direction = info->dir;
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR)
+		dma_conf.src_addr_width = dma_conf.dst_addr_width
+			= DMA_SLAVE_BUSWIDTH_1_BYTE;
+	else
+#endif
+	dma_conf.src_addr_width = dma_conf.dst_addr_width
+		= DMA_SLAVE_BUSWIDTH_4_BYTES;
+
+	dma_conf.src_maxburst = 1;
+	dma_conf.dst_maxburst = 1;
+	dma_conf.slave_id = sunxi_slave_id(DRQDST_SDRAM, DRQSRC_SDRAM);
+	dmaengine_slave_config(info->chan, &dma_conf);
+
+	ss_sg_table_init(&info->sgt_for_cp, info->sg, len,
+		sss->flows[flow].buf_src, sss->flows[flow].buf_src_dma);
+	SS_DBG("chan: 0x%p, info->sgt_for_cp.sgl: 0x%p\n",
+		info->chan, info->sgt_for_cp.sgl);
+
+	info->nents = info->sgt_for_cp.nents;
+	SS_DBG("flow: %d, sg num: %d, total len: %d\n", flow, info->nents, len);
+
+	dma_map_sg(&sss->pdev->dev, info->sg, info->nents, info->dir);
+	dma_map_sg(&sss->pdev->dev, info->sgt_for_cp.sgl,
+		info->nents, info->dir);
+
+	if (SS_METHOD_IS_HASH(req_ctx->type)) {
+		/* Total len is too small, so there is no data for DMA. */
+		if (len < SHA1_BLOCK_SIZE)
+			return 1;
+
+		ss_hash_padding_sg_prepare(&info->sg[info->nents-1], len);
+		ss_hash_padding_sg_prepare(&info->sgt_for_cp.sgl[info->nents-1],
+			len);
+	}
+
+	dma_desc = info->chan->device->device_prep_dma_sg(info->chan,
+			info->sgt_for_cp.sgl, info->nents,
+			info->sg, info->nents, DMA_PREP_INTERRUPT|DMA_CTRL_ACK);
+	if (!dma_desc) {
+		SS_ERR("dmaengine_prep_slave_sg() failed!\n");
+		return -1;
+	}
+
+	if (cb == 1) {
+		dma_desc->callback = ss_dma_cb;
+		dma_desc->callback_param = (void *)req_ctx;
+	}
+	dmaengine_submit(dma_desc);
+	return 0;
+}
+
+static void ss_dma_release(sunxi_ss_t *sss, ss_dma_info_t *info)
+{
+	dma_unmap_sg(&sss->pdev->dev, info->sgt_for_cp.sgl,
+		info->nents, info->dir);
+	sg_free_table(&info->sgt_for_cp);
+	dma_unmap_sg(&sss->pdev->dev, info->sg, info->nents, info->dir);
+	dma_release_channel(info->chan);
+}
+
+static int ss_aes_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+	ss_flow_enable(flow);
+
+	ss_method_set(req_ctx->dir, req_ctx->type);
+	ss_aes_mode_set(req_ctx->mode);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n",
+		flow, req_ctx->dir, req_ctx->type, req_ctx->mode, len);
+
+	init_completion(&req_ctx->done);
+
+	/* 1. Copy data from user space to sss->flows[flow].buf_src. */
+	if (ss_dma_prepare(&req_ctx->dma_src))
+		return -EBUSY;
+#ifdef SS_CTR_MODE_ENABLE
+	if ((req_ctx->mode == SS_AES_MODE_CTR) && ((len%AES_BLOCK_SIZE) != 0))
+		memset(&ss_dev->flows[flow].buf_src[len], 0, AES_BLOCK_SIZE);
+#endif
+	ss_dma_src_config(ss_dev, ctx, req_ctx, len, 1);
+	ss_dma_start(&req_ctx->dma_src);
+	ret = wait_for_completion_timeout(&req_ctx->done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	/* 2. Start the SS. */
+	ss_data_src_set(ss_dev->flows[flow].buf_src_dma);
+	ss_data_dst_set(ss_dev->flows[flow].buf_dst_dma);
+	SS_DBG("ss_dev->buf_dst_dma = %#x\n", ss_dev->flows[flow].buf_dst_dma);
+#ifdef SS_CTS_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTS) {
+		ss_data_len_set(len);
+		/* A bad way to determin the last packet of CTS mode. */
+		/* if (len < SZ_4K) */
+			ss_cts_last();
+	} else
+#endif
+	ss_data_len_set(DIV_ROUND_UP(len, AES_BLOCK_SIZE)*4);
+
+	ss_ctrl_start();
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		return -ETIMEDOUT;
+	}
+
+	/* 3. Copy the result from sss->flows[flow].buf_dst to user space. */
+	if (ss_dma_prepare(&req_ctx->dma_dst))
+		return -EBUSY;
+	ss_dma_dst_config(ss_dev, ctx, req_ctx, len, 1);
+	ss_dma_start(&req_ctx->dma_dst);
+	ret = wait_for_completion_timeout(&req_ctx->done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	ss_ctrl_stop();
+	ss_irq_disable(flow);
+	ss_dma_release(ss_dev, &req_ctx->dma_src);
+	ss_dma_release(ss_dev, &req_ctx->dma_dst);
+
+	return 0;
+}
+
+int ss_aes_key_valid(struct crypto_ablkcipher *tfm, int len)
+{
+	if (unlikely(len > SS_RSA_MAX_SIZE)) {
+		SS_ERR("Unsupported key size: %d\n", len);
+		tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int ss_rng_start(ss_aes_ctx_t *ctx, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	u32 len = dlen;
+
+	if (len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", len);
+		len = SS_RNG_MAX_LEN;
+	}
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+	ss_flow_enable(flow);
+
+	if (trng) {
+		ss_method_set(SS_DIR_ENCRYPT, SS_METHOD_TRNG);
+		ss_trng_osc_enable();
+	} else
+		ss_method_set(SS_DIR_ENCRYPT, SS_METHOD_PRNG);
+
+	ss_rng_mode_set(SS_RNG_MODE_CONTINUE);
+
+	ss_data_dst_set(ss_dev->flows[flow].buf_dst_dma);
+	if (trng)
+		/* align with 32 Bytes */
+		ss_data_len_set(DIV_ROUND_UP(len, 32)*(32>>2));
+	else
+		/* align with 20 Bytes */
+		ss_data_len_set(DIV_ROUND_UP(len, 20)*(20>>2));
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n",
+		flow, len, DIV_ROUND_UP(len, 20)*5);
+	dma_map_single(&ss_dev->pdev->dev, ss_dev->flows[flow].buf_dst,
+		SS_DMA_BUF_SIZE, DMA_DEV_TO_MEM);
+
+	ss_ctrl_start();
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		return -ETIMEDOUT;
+	}
+
+	dma_unmap_single(&ss_dev->pdev->dev, ss_dev->flows[flow].buf_dst_dma,
+		SS_DMA_BUF_SIZE, DMA_DEV_TO_MEM);
+	memcpy(rdata, ss_dev->flows[flow].buf_dst, len);
+
+	if (trng)
+		ss_trng_osc_disable();
+
+	ss_irq_disable(flow);
+	ss_ctrl_stop();
+	ret = len;
+	return ret;
+}
+
+int ss_rng_get_random(struct crypto_rng *tfm, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	ss_aes_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, data = %p, len = %d, trng = %d\n",
+		ctx->comm.flow, data, len, trng);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+	if (trng) {
+		len = DIV_ROUND_UP(dlen, SHA256_DIGEST_SIZE)*SHA256_BLOCK_SIZE;
+		data = kzalloc(len, GFP_KERNEL);
+		if (data == NULL) {
+			SS_ERR("Failed to malloc(%d)\n", len);
+			return -ENOMEM;
+		}
+		SS_DBG("In fact, flow = %d, data = %p, len = %d\n",
+			ctx->comm.flow, data, len);
+	}
+#endif
+
+	ss_dev_lock();
+
+	/* Must set the seed addr in PRNG/TRNG. */
+	ss_key_set(ctx->key, ctx->key_size);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev, ctx->key, ctx->key_size,
+		DMA_MEM_TO_DEV);
+
+	ret = ss_rng_start(ctx, data, len, trng);
+	ss_dev_unlock();
+
+	SS_DBG("Get %d byte random.\n", ret);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+	if (trng) {
+		ss_trng_postprocess(rdata, dlen, data, len);
+		ret = dlen;
+		kfree(data);
+	}
+#endif
+
+	return ret;
+}
+
+u32 ss_hash_start(ss_hash_ctx_t *ctx,
+		ss_aes_req_ctx_t *req_ctx, u32 len, u32 last)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	int md_map_flag = 0;
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+	ss_flow_enable(flow);
+	ss_method_set(req_ctx->dir, req_ctx->type);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d/%d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+
+	SS_DBG("IV address = 0x%p, size = %d\n", ctx->md, ctx->md_size);
+	ss_iv_set(ctx->md, ctx->md_size);
+	ss_iv_mode_set(SS_IV_MODE_ARBITRARY);
+
+	init_completion(&req_ctx->done);
+
+	if (ss_dma_prepare(&req_ctx->dma_src))
+		return -EBUSY;
+	ret = ss_dma_src_config(ss_dev, ctx, req_ctx, len, 1);
+	if (ret == 0) {
+		/* 1. Copy data from user space to sss->flows[flow].buf_src. */
+		ss_dma_start(&req_ctx->dma_src);
+		ret = wait_for_completion_timeout(&req_ctx->done,
+			msecs_to_jiffies(SS_WAIT_TIME));
+		if (ret == 0) {
+			SS_ERR("Timed out\n");
+			return -ETIMEDOUT;
+		}
+		/* 2. Start the SS. */
+		ss_data_src_set(ss_dev->flows[flow].buf_src_dma);
+		ss_data_dst_set(ss_dev->flows[flow].buf_dst_dma);
+		SS_DBG("buf_dst_dma: %#x\n", ss_dev->flows[flow].buf_dst_dma);
+		ss_data_len_set((len - len%SHA1_BLOCK_SIZE)/4);
+
+#ifdef SS_SHA_SWAP_MID_ENABLE
+		if (req_ctx->type != SS_METHOD_MD5)
+			ss_hash_swap(ctx->md, ctx->md_size);
+#endif
+
+		dma_map_single(&ss_dev->pdev->dev, ctx->md, ctx->md_size,
+			DMA_MEM_TO_DEV);
+		md_map_flag = 1;
+
+		SS_DBG("Before SS, CTRL: 0x%08x\n", ss_reg_rd(SS_REG_CTL));
+		memset(ss_dev->flows[flow].buf_dst, 0xF0, ctx->md_size);
+		dma_map_single(&ss_dev->pdev->dev, ss_dev->flows[flow].buf_dst,
+			ctx->md_size, DMA_DEV_TO_MEM);
+		ss_ctrl_start();
+
+		ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+			msecs_to_jiffies(SS_WAIT_TIME));
+		if (ret == 0) {
+			SS_ERR("Timed out\n");
+			ss_reset();
+			return -ETIMEDOUT;
+		}
+		SS_DBG("After SS, CTRL: 0x%08x\n", ss_reg_rd(SS_REG_CTL));
+		SS_DBG("After SS, dst data:\n");
+		ss_print_hex(ss_dev->flows[flow].buf_dst, 32,
+			ss_dev->flows[flow].buf_dst);
+
+		/* 3. Copy the MD from sss->buf_dst to ctx->md. */
+		memcpy(ctx->md, ss_dev->flows[flow].buf_dst, ctx->md_size);
+	}
+
+	ss_ctrl_stop();
+	ss_irq_disable(flow);
+	if (md_map_flag == 1) {
+		dma_unmap_single(&ss_dev->pdev->dev,
+			ss_dev->flows[flow].buf_dst_dma, ctx->md_size,
+			DMA_DEV_TO_MEM);
+		dma_unmap_single(&ss_dev->pdev->dev,
+			virt_to_phys(ctx->md), ctx->md_size, DMA_MEM_TO_DEV);
+	}
+	ss_dma_release(ss_dev, &req_ctx->dma_src);
+
+	ctx->cnt += len;
+	return 0;
+}
+
+int ss_aes_one_req(sunxi_ss_t *sss, struct ablkcipher_request *req)
+{
+	int ret = 0;
+	struct crypto_ablkcipher *tfm = NULL;
+	ss_aes_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+	int key_map_flag = 0;
+	int iv_map_flag = 0;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = %p, dst = %p\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_ablkcipher_reqtfm(req);
+	req_ctx = ablkcipher_request_ctx(req);
+	ctx = crypto_ablkcipher_ctx(tfm);
+
+	/* A31 SS need update key each cycle in decryption. */
+	if ((ctx->comm.flags & SS_FLAG_NEW_KEY)
+		|| (req_ctx->dir == SS_DIR_DECRYPT)) {
+		SS_DBG("KEY address: %p, size: %d\n", ctx->key, ctx->key_size);
+		ss_key_set(ctx->key, ctx->key_size);
+		dma_map_single(&sss->pdev->dev, ctx->key,
+			ctx->key_size, DMA_MEM_TO_DEV);
+		key_map_flag = 1;
+		ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	}
+
+#ifdef SS_CTS_MODE_ENABLE
+	if (((req_ctx->mode == SS_AES_MODE_CBC)
+		|| (req_ctx->mode == SS_AES_MODE_CTS)) && (req->info != NULL)) {
+#else
+	if ((req_ctx->mode == SS_AES_MODE_CBC) && (req->info != NULL)) {
+#endif
+		SS_DBG("IV address = %p, size = %d\n", req->info,
+			crypto_ablkcipher_ivsize(tfm));
+		memcpy(ctx->iv, req->info, crypto_ablkcipher_ivsize(tfm));
+		ss_iv_set(ctx->iv, crypto_ablkcipher_ivsize(tfm));
+		dma_map_single(&sss->pdev->dev, ctx->iv,
+			crypto_ablkcipher_ivsize(tfm), DMA_MEM_TO_DEV);
+		iv_map_flag = 1;
+	}
+
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR) {
+		SS_DBG("Cnt address = %p, size = %d\n", req->info,
+			crypto_ablkcipher_ivsize(tfm));
+		if (ctx->cnt == 0)
+			memcpy(ctx->iv, req->info,
+				crypto_ablkcipher_ivsize(tfm));
+
+		SS_DBG("CNT: %08x %08x %08x %08x\n",
+			*(int *)&ctx->iv[0], *(int *)&ctx->iv[4],
+			*(int *)&ctx->iv[8], *(int *)&ctx->iv[12]);
+		ss_cnt_set(ctx->iv, crypto_ablkcipher_ivsize(tfm));
+		dma_map_single(&sss->pdev->dev, ctx->iv,
+			crypto_ablkcipher_ivsize(tfm), DMA_MEM_TO_DEV);
+		iv_map_flag = 1;
+	}
+#endif
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, 16, ctx->iv);
+
+	if (req_ctx->type == SS_METHOD_RSA)
+		ss_rsa_width_set(crypto_ablkcipher_ivsize(tfm));
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+	ret = ss_aes_start(ctx, req_ctx, req->nbytes);
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+	if (key_map_flag == 1)
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->key),
+			ctx->key_size, DMA_MEM_TO_DEV);
+	if (iv_map_flag == 1)
+		dma_unmap_single(&sss->pdev->dev, virt_to_phys(ctx->iv),
+			crypto_ablkcipher_ivsize(tfm), DMA_MEM_TO_DEV);
+
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR) {
+		ss_cnt_get(ctx->comm.flow,
+			ctx->iv, crypto_ablkcipher_ivsize(tfm));
+		SS_DBG("CNT: %08x %08x %08x %08x\n",
+			*(int *)&ctx->iv[0], *(int *)&ctx->iv[4],
+			*(int *)&ctx->iv[8], *(int *)&ctx->iv[12]);
+	}
+#endif
+
+	ctx->cnt += req->nbytes;
+	return ret;
+}
+
+irqreturn_t sunxi_ss_irq_handler(int irq, void *dev_id)
+{
+	sunxi_ss_t *sss = (sunxi_ss_t *)dev_id;
+	unsigned long flags = 0;
+	int pending = 0;
+
+	spin_lock_irqsave(&sss->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("SS pending %#x\n", pending);
+	if (pending&SS_REG_ICR_FLOW0_PENDING_MASK) {
+		ss_pending_clear(0);
+		complete(&sss->flows[0].done);
+	}
+	if (pending&SS_REG_ICR_FLOW1_PENDING_MASK) {
+		ss_pending_clear(1);
+		complete(&sss->flows[1].done);
+	}
+	spin_unlock_irqrestore(&sss->lock, flags);
+	SS_DBG("SS pending %#x, CTRL: 0x%08x\n",
+		ss_pending_get(), ss_reg_rd(SS_REG_CTL));
+
+	return IRQ_HANDLED;
+}
+
diff --git a/drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.c b/drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.c
new file mode 100644
index 000000000..4f062a7d9
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.c
@@ -0,0 +1,382 @@
+/*
+ * The interface function of controlling the SS register.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <asm/io.h>
+
+#include "../sunxi_ce.h"
+#include "sunxi_ce_reg.h"
+
+static int gs_ss_osc_prev_state;
+
+inline u32 ss_readl(u32 offset)
+{
+	return readl(ss_membase() + offset);
+}
+
+inline void ss_writel(u32 offset, u32 val)
+{
+	writel(val, ss_membase() + offset);
+}
+
+u32 ss_reg_rd(u32 offset)
+{
+	return ss_readl(offset);
+}
+
+void ss_reg_wr(u32 offset, u32 val)
+{
+	ss_writel(offset, val);
+}
+
+void ss_keyselect_set(int select)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~SS_REG_CTL_KEY_SELECT_MASK;
+	val |= select << SS_REG_CTL_KEY_SELECT_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_keysize_set(int size)
+{
+	int val = ss_readl(SS_REG_CTL);
+	int type = SS_AES_KEY_SIZE_128;
+
+	switch (size) {
+	case AES_KEYSIZE_192:
+		type = SS_AES_KEY_SIZE_192;
+		break;
+	case AES_KEYSIZE_256:
+		type = SS_AES_KEY_SIZE_256;
+		break;
+	default:
+/*		type = SS_AES_KEY_SIZE_128; */
+		break;
+	}
+
+	val &= ~(SS_REG_CTL_KEY_SIZE_MASK);
+	val |= (type << SS_REG_CTL_KEY_SIZE_SHIFT);
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_flow_enable(int flow)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~(SS_STREM1_SELECT|SS_STREM0_SELECT);
+	if (flow == 0)
+		val |= SS_STREM0_SELECT;
+	else
+		val |= SS_STREM1_SELECT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_flow_mode_set(int mode)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~(SS_REG_CTL_FLOW_MODE_MASK);
+	val |= mode;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_pending_clear(int flow)
+{
+	int val = ss_readl(SS_REG_ISR);
+
+	val &= ~(SS_REG_ICR_FLOW0_PENDING_MASK|SS_REG_ICR_FLOW1_PENDING_MASK);
+	val |= SS_FLOW_PENDING << flow;
+	ss_writel(SS_REG_ISR, val);
+}
+
+int ss_pending_get(void)
+{
+	return ss_readl(SS_REG_ISR);
+}
+
+void ss_irq_enable(int flow)
+{
+	int val = ss_readl(SS_REG_ICR);
+
+	val |= SS_FLOW_END_INT_ENABLE << flow;
+	ss_writel(SS_REG_ICR, val);
+}
+
+void ss_irq_disable(int flow)
+{
+	int val = ss_readl(SS_REG_ICR);
+
+	val &= ~(SS_FLOW_END_INT_ENABLE << flow);
+	ss_writel(SS_REG_ICR, val);
+}
+
+/* key: phsical address. */
+void ss_key_set(char *key, int size)
+{
+	int i = 0;
+	int key_sel = CE_KEY_SELECT_INPUT;
+	struct {
+		int type;
+		char desc[AES_MIN_KEY_SIZE];
+	} key_select[] = {
+		{CE_KEY_SELECT_SSK,			CE_KS_SSK},
+		{CE_KEY_SELECT_HUK,			CE_KS_HUK},
+		{CE_KEY_SELECT_RSSK,		CE_KS_RSSK},
+		{CE_KEY_SELECT_INTERNAL_0,	CE_KS_INTERNAL_0},
+		{CE_KEY_SELECT_INTERNAL_1,	CE_KS_INTERNAL_1},
+		{CE_KEY_SELECT_INTERNAL_2,	CE_KS_INTERNAL_2},
+		{CE_KEY_SELECT_INTERNAL_3,	CE_KS_INTERNAL_3},
+		{CE_KEY_SELECT_INTERNAL_4,	CE_KS_INTERNAL_4},
+		{CE_KEY_SELECT_INTERNAL_5,	CE_KS_INTERNAL_5},
+		{CE_KEY_SELECT_INTERNAL_6,	CE_KS_INTERNAL_6},
+		{CE_KEY_SELECT_INTERNAL_7,	CE_KS_INTERNAL_7},
+		{CE_KEY_SELECT_INPUT, ""}
+	};
+
+	while (key_select[i].type != CE_KEY_SELECT_INPUT) {
+		if (strncasecmp(key, key_select[i].desc, AES_MIN_KEY_SIZE) == 0) {
+			key_sel = key_select[i].type;
+			memset(key, 0, size);
+			break;
+		}
+		i++;
+	}
+	SS_DBG("The key select: %d\n", key_sel);
+
+	ss_keyselect_set(key_sel);
+	ss_keysize_set(size);
+	ss_writel(SS_REG_KEY_L, virt_to_phys(key));
+}
+
+/* iv: phsical address. */
+void ss_iv_set(char *iv, int size)
+{
+	ss_writel(SS_REG_IV_L, virt_to_phys(iv));
+}
+
+void ss_cntsize_set(int size)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~SS_REG_CTL_CTR_SIZE_MASK;
+	val |= size << SS_REG_CTL_CTR_SIZE_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_cnt_set(char *cnt, int size)
+{
+	ss_cntsize_set(SS_CTR_SIZE_128);
+	ss_writel(SS_REG_IV_L, virt_to_phys(cnt));
+}
+
+void ss_cnt_get(int flow, char *cnt, int size)
+{
+	int i;
+	int *val = (int *)cnt;
+	int base = SS_REG_CNT_BASE;
+
+	if (flow == 1)
+		base = SS_REG_FLOW1_CNT_BASE;
+
+	for (i = 0; i < size/4; i++, val++)
+		*val = ss_readl(base + i*4);
+}
+
+void ss_md_get(char *dst, char *src, int size)
+{
+	memcpy(dst, src, size);
+}
+
+void ss_iv_mode_set(int mode)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~SS_REG_CTL_IV_MODE_MASK;
+	val |= mode << SS_REG_CTL_IV_MODE_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_cts_last(void)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val |= SS_REG_CTL_AES_CTS_LAST;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_method_set(int dir, int type)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~(SS_REG_CTL_OP_DIR_MASK|SS_REG_CTL_METHOD_MASK);
+	val |= dir << SS_REG_CTL_OP_DIR_SHIFT;
+	val |= type << SS_REG_CTL_METHOD_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_aes_mode_set(int mode)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~SS_REG_CTL_OP_MODE_MASK;
+	val |= mode << SS_REG_CTL_OP_MODE_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_rng_mode_set(int mode)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~SS_REG_CTL_PRNG_MODE_MASK;
+	val |= mode << SS_REG_CTL_PRNG_MODE_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_trng_osc_enable(void)
+{
+	int val = readl(SS_TRNG_OSC_ADDR);
+
+	gs_ss_osc_prev_state = 1;
+	if (val & 1)
+		return;
+
+	val |= 1;
+	writel(val, SS_TRNG_OSC_ADDR);
+}
+
+void ss_trng_osc_disable(void)
+{
+	int val = 0;
+
+	if (gs_ss_osc_prev_state == 1)
+		return;
+
+	val = readl(SS_TRNG_OSC_ADDR);
+	val &= ~1;
+	writel(val, SS_TRNG_OSC_ADDR);
+}
+
+void ss_sha_final(void)
+{
+	/* unsupported. */
+}
+
+void ss_check_sha_end(void)
+{
+	/* unsupported. */
+}
+
+void ss_rsa_width_set(int size)
+{
+	int val = ss_readl(SS_REG_CTL);
+	int width_type = SS_RSA_PUB_MODULUS_WIDTH_512;
+
+	switch (size*8) {
+	case 512:
+		width_type = SS_RSA_PUB_MODULUS_WIDTH_512;
+		break;
+	case 1024:
+		width_type = SS_RSA_PUB_MODULUS_WIDTH_1024;
+		break;
+	case 2048:
+		width_type = SS_RSA_PUB_MODULUS_WIDTH_2048;
+		break;
+	case 3072:
+		width_type = SS_RSA_PUB_MODULUS_WIDTH_3072;
+		break;
+	default:
+		break;
+	}
+
+	val &= ~SS_REG_CTL_RSA_PM_WIDTH_MASK;
+	val |= width_type<<SS_REG_CTL_RSA_PM_WIDTH_SHIFT;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_ctrl_start(void)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val |= 1<<27;
+	val |= SS_CTL_START;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_ctrl_stop(void)
+{
+	int val = ss_readl(SS_REG_CTL);
+
+	val &= ~SS_CTL_START;
+	ss_writel(SS_REG_CTL, val);
+}
+
+void ss_wait_idle(void)
+{
+	while ((ss_readl(SS_REG_CTL) & SS_REG_CTL_IDLE_MASK) == SS_CTL_BUSY) {
+		/* SS_DBG("Need wait for the hardware.\n"); */
+		msleep(20);
+	}
+}
+
+void ss_data_src_set(int addr)
+{
+	ss_writel(SS_REG_DATA_SRC_H, 0);
+	ss_writel(SS_REG_DATA_SRC_L, addr);
+}
+
+void ss_data_dst_set(int addr)
+{
+	ss_writel(SS_REG_DATA_DST_H, 0);
+	ss_writel(SS_REG_DATA_DST_L, addr);
+}
+
+void ss_data_len_set(int len)
+{
+	ss_writel(SS_REG_DATA_LEN, len);
+}
+
+int ss_reg_print(char *buf, int len)
+{
+	return snprintf(buf, len,
+		"The SS control register:\n"
+		"[CTL] 0x%02x = 0x%08x\n"
+		"[ICR] 0x%02x = 0x%08x, [ISR] 0x%02x = 0x%08x\n"
+		"[KEY_L] 0x%02x = 0x%08x, [KEY_H] 0x%02x = 0x%08x\n"
+		"[IV_L]  0x%02x = 0x%08x, [IV_H]  0x%02x = 0x%08x\n"
+		"[DATA_SRC_L] 0x%02x = 0x%08x, [DATA_SRC_H] 0x%02x = 0x%08x\n"
+		"[DATA_DST_L] 0x%02x = 0x%08x, [DATA_DST_H] 0x%02x = 0x%08x\n"
+		"[DATA_LEN] 0x%02x = 0x%08x\n"
+		"[CNT0-3] 0x%02x = 0x%08x, 0x%08x, 0x%08x, 0x%08x\n"
+		"[CNT4-7] 0x%02x = 0x%08x, 0x%08x, 0x%08x, 0x%08x\n",
+		SS_REG_CTL, ss_readl(SS_REG_CTL),
+		SS_REG_ICR, ss_readl(SS_REG_ICR),
+		SS_REG_ISR, ss_readl(SS_REG_ISR),
+		SS_REG_KEY_L, ss_readl(SS_REG_KEY_L),
+		SS_REG_KEY_H, ss_readl(SS_REG_KEY_H),
+		SS_REG_IV_L, ss_readl(SS_REG_IV_L),
+		SS_REG_IV_H, ss_readl(SS_REG_IV_H),
+		SS_REG_DATA_SRC_L, ss_readl(SS_REG_DATA_SRC_L),
+		SS_REG_DATA_SRC_H, ss_readl(SS_REG_DATA_SRC_H),
+		SS_REG_DATA_DST_L, ss_readl(SS_REG_DATA_DST_L),
+		SS_REG_DATA_DST_H, ss_readl(SS_REG_DATA_DST_H),
+		SS_REG_DATA_LEN, ss_readl(SS_REG_DATA_LEN),
+		SS_REG_CNT(0),
+		ss_readl(SS_REG_CNT(0)), ss_readl(SS_REG_CNT(1)),
+		ss_readl(SS_REG_CNT(2)), ss_readl(SS_REG_CNT(3)),
+		SS_REG_FLOW1_CNT(0),
+		ss_readl(SS_REG_FLOW1_CNT(0)), ss_readl(SS_REG_FLOW1_CNT(1)),
+		ss_readl(SS_REG_FLOW1_CNT(2)), ss_readl(SS_REG_FLOW1_CNT(3))
+		);
+}
+
diff --git a/drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.h b/drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.h
new file mode 100644
index 000000000..54467e12a
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v2/sunxi_ce_reg.h
@@ -0,0 +1,234 @@
+/*
+ * The register macro of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_SECURITY_SYSTEM_REG_H_
+#define _SUNXI_SECURITY_SYSTEM_REG_H_
+
+#include <mach/platform.h>
+
+#define SS_REG_CTL			0x00
+#define SS_REG_ICR			0x04
+#define SS_REG_ISR			0x08
+#define SS_REG_KEY_L		0x10
+#define SS_REG_KEY_H		0x14
+#define SS_REG_IV_L			0x18
+#define SS_REG_IV_H			0x1C
+#define SS_REG_DATA_SRC_L	0x20
+#define SS_REG_DATA_SRC_H	0x24
+#define SS_REG_DATA_DST_L	0x28
+#define SS_REG_DATA_DST_H	0x2C
+#define SS_REG_DATA_LEN		0x30
+
+#define SS_REG_CNT_BASE			0x34
+#define SS_REG_CNT(n)			(SS_REG_CNT_BASE + 4*n)
+#define SS_REG_CNT_NUM			4
+#define SS_REG_FLOW1_CNT_BASE	0x48
+#define SS_REG_FLOW1_CNT(n)		(SS_REG_FLOW1_CNT_BASE + 4*n)
+
+#define SS_STREM1_SELECT		BIT(31)
+#define SS_STREM0_SELECT		BIT(30)
+
+#define SS_CTL_BUSY				0
+#define SS_CTL_IDLE				1
+#define SS_REG_CTL_IDLE_SHIFT	29
+#define SS_REG_CTL_IDLE_MASK	(0x1 << SS_REG_CTL_IDLE_SHIFT)
+
+#define SS_FLOW_MODE_NON_CONTINUE	0
+#define SS_FLOW_MODE_CONINUE		1
+#define SS_REG_CTL_FLOW_MODE_SHIFT	28
+#define SS_REG_CTL_FLOW_MODE_MASK	(0x1 << SS_REG_CTL_FLOW_MODE_SHIFT)
+
+#define SS_DMA_CONSISENT_SEND_END	0
+#define SS_DMA_CONSISENT_WAIT_END	1
+#define SS_REG_CTL_DMA_CON_SHIFT	27
+#define SS_REG_CTL_DMA_CON_MASK		(0x1 << SS_REG_CTL_DMA_CON_SHIFT)
+
+#define SS_KEY_SELECT_INPUT			0
+#define SS_KEY_SELECT_INTER0		8
+#define SS_REG_CTL_KEY_SELECT_SHIFT	23
+#define SS_REG_CTL_KEY_SELECT_MASK	(0xF << SS_REG_CTL_KEY_SELECT_SHIFT)
+
+#define SS_CFG_GENERATE_POLY_NONE	0
+#define SS_CFG_GENERATE_POLY		1
+#define SS_REG_CTL_POLY_SHIFT		19
+#define SS_REG_CTL_POLY_MASK		(0x1 << SS_REG_CTL_POLY_SHIFT)
+
+#define SS_RNG_MODE_ONESHOT			0
+#define SS_RNG_MODE_CONTINUE		1
+#define SS_REG_CTL_PRNG_MODE_SHIFT	18
+#define SS_REG_CTL_PRNG_MODE_MASK	(1 << SS_REG_CTL_PRNG_MODE_SHIFT)
+
+#define SS_IV_MODE_CONSTANT			0
+#define SS_IV_MODE_ARBITRARY		1
+#define SS_REG_CTL_IV_MODE_SHIFT	17
+#define SS_REG_CTL_IV_MODE_MASK		(1 << SS_REG_CTL_IV_MODE_SHIFT)
+
+#define SS_REG_CTL_AES_CTS_LAST		BIT(16)
+
+#define SS_REG_WRITABLE				0
+#define SS_REG_UNWRITABLE			1
+#define SS_REG_CTL_CFG_VALID_SHIFT	15
+#define SS_REG_CTL_CFG_VALID_MASK	(0x1 << SS_REG_CTL_CFG_VALID_SHIFT)
+
+#define SS_AES_MODE_ECB				0
+#define SS_AES_MODE_CBC				1
+#define SS_AES_MODE_CTR				2
+#define SS_AES_MODE_CTS				3
+#define SS_REG_CTL_OP_MODE_SHIFT	13
+#define SS_REG_CTL_OP_MODE_MASK		(0x3 << SS_REG_CTL_OP_MODE_SHIFT)
+
+#define SS_CTR_SIZE_16				0
+#define SS_CTR_SIZE_32				1
+#define SS_CTR_SIZE_64				2
+#define SS_CTR_SIZE_128				3
+#define SS_REG_CTL_CTR_SIZE_SHIFT	11
+#define SS_REG_CTL_CTR_SIZE_MASK	(0x3 << SS_REG_CTL_CTR_SIZE_SHIFT)
+
+#define CE_RSA_OP_M_EXP		1 /* modular exponentiation */
+#define SS_RSA_PUB_MODULUS_WIDTH_512	0
+#define SS_RSA_PUB_MODULUS_WIDTH_1024	1
+#define SS_RSA_PUB_MODULUS_WIDTH_2048	2
+#define SS_RSA_PUB_MODULUS_WIDTH_3072	3
+#define SS_REG_CTL_RSA_PM_WIDTH_SHIFT	9
+#define SS_REG_CTL_RSA_PM_WIDTH_MASK	(0x3 << SS_REG_CTL_RSA_PM_WIDTH_SHIFT)
+
+#define SS_AES_KEY_SIZE_128         0
+#define SS_AES_KEY_SIZE_192         1
+#define SS_AES_KEY_SIZE_256         2
+#define SS_REG_CTL_KEY_SIZE_SHIFT   7
+#define SS_REG_CTL_KEY_SIZE_MASK    (0x3 << SS_REG_CTL_KEY_SIZE_SHIFT)
+
+#define SS_DIR_ENCRYPT              0
+#define SS_DIR_DECRYPT              1
+#define SS_REG_CTL_OP_DIR_SHIFT     6
+#define SS_REG_CTL_OP_DIR_MASK      (0x1 << SS_REG_CTL_OP_DIR_SHIFT)
+
+#define SS_METHOD_AES			0
+#define SS_METHOD_DES			1
+#define SS_METHOD_3DES			2
+#define SS_METHOD_MD5			3
+#define SS_METHOD_PRNG			4
+#define SS_METHOD_TRNG			5
+#define SS_METHOD_SHA1			6
+#define SS_METHOD_SHA224		7
+#define SS_METHOD_SHA256		8
+#define SS_METHOD_RSA			9
+#define SS_METHOD_CRC32			10
+#define SS_REG_CTL_METHOD_SHIFT	2
+#define SS_REG_CTL_METHOD_MASK	(0xF << SS_REG_CTL_METHOD_SHIFT)
+
+#define SS_METHOD_IS_HASH(type) ((type == SS_METHOD_MD5) \
+				|| (type == SS_METHOD_SHA1) \
+				|| (type == SS_METHOD_SHA224) \
+				|| (type == SS_METHOD_SHA256))
+
+#define SS_CTL_START			1
+#define SS_REG_CTL_START_MASK	0x1
+
+#define SS_FLOW_END_INT_DISABLE		0
+#define SS_FLOW_END_INT_ENABLE		1
+#define SS_REG_ICR_FLOW1_INT_SHIFT	1
+#define SS_REG_ICR_FLOW1_INT_MASK	(1 << SS_REG_ICR_FLOW1_INT_SHIFT)
+#define SS_REG_ICR_FLOW0_INT_SHIFT	0
+#define SS_REG_ICR_FLOW0_INT_MASK	(1 << SS_REG_ICR_FLOW0_INT_SHIFT)
+
+#define SS_FLOW_NO_PENDING				0
+#define SS_FLOW_PENDING					1
+#define SS_REG_ICR_FLOW1_PENDING_SHIFT	1
+#define SS_REG_ICR_FLOW1_PENDING_MASK	(1 << SS_REG_ICR_FLOW1_PENDING_SHIFT)
+#define SS_REG_ICR_FLOW0_PENDING_SHIFT	0
+#define SS_REG_ICR_FLOW0_PENDING_MASK	(1 << SS_REG_ICR_FLOW0_PENDING_SHIFT)
+
+#ifdef CONFIG_ARCH_SUN8IW6
+#define SS_TRNG_OSC_ADDR	((void __iomem *)IO_ADDRESS(0x01f01400 + 0x1f4))
+#endif
+
+#define SS_SEED_SIZE			24
+
+#define CE_KEY_SELECT_INPUT			0
+#define CE_KEY_SELECT_SSK			1
+#define CE_KEY_SELECT_HUK			2
+#define CE_KEY_SELECT_RSSK			3
+#define CE_KEY_SELECT_INTERNAL_0	8
+#define CE_KEY_SELECT_INTERNAL_1	9
+#define CE_KEY_SELECT_INTERNAL_2	10
+#define CE_KEY_SELECT_INTERNAL_3	11
+#define CE_KEY_SELECT_INTERNAL_4	12
+#define CE_KEY_SELECT_INTERNAL_5	13
+#define CE_KEY_SELECT_INTERNAL_6	14
+#define CE_KEY_SELECT_INTERNAL_7	15
+
+/* The identification string to indicate the key source. */
+#define CE_KS_SSK			"KEY_SEL_SSK"
+#define CE_KS_HUK			"KEY_SEL_HUK"
+#define CE_KS_RSSK			"KEY_SEL_RSSK"
+#define CE_KS_INTERNAL_0	"KEY_SEL_INTRA_0"
+#define CE_KS_INTERNAL_1	"KEY_SEL_INTRA_1"
+#define CE_KS_INTERNAL_2	"KEY_SEL_INTRA_2"
+#define CE_KS_INTERNAL_3	"KEY_SEL_INTRA_3"
+#define CE_KS_INTERNAL_4	"KEY_SEL_INTRA_4"
+#define CE_KS_INTERNAL_5	"KEY_SEL_INTRA_5"
+#define CE_KS_INTERNAL_6	"KEY_SEL_INTRA_6"
+#define CE_KS_INTERNAL_7	"KEY_SEL_INTRA_7"
+
+
+/* Function declaration */
+
+u32 ss_reg_rd(u32 offset);
+void ss_reg_wr(u32 offset, u32 val);
+
+void ss_keyselect_set(int select);
+void ss_keysize_set(int size);
+void ss_key_set(char *key, int size);
+
+void ss_fifo_init(void);
+void ss_flow_enable(int flow);
+void ss_flow_mode_set(int mode);
+
+int ss_pending_get(void);
+void ss_pending_clear(int flow);
+void ss_irq_enable(int flow);
+void ss_irq_disable(int flow);
+
+void ss_iv_set(char *iv, int size);
+void ss_cntsize_set(int size);
+void ss_cnt_set(char *cnt, int size);
+void ss_cnt_get(int flow, char *cnt, int size);
+
+void ss_md_get(char *dst, char *src, int size);
+void ss_sha_final(void);
+void ss_check_sha_end(void);
+
+void ss_iv_mode_set(int mode);
+
+void ss_rsa_width_set(int size);
+
+void ss_cts_last(void);
+
+void ss_method_set(int dir, int type);
+void ss_aes_mode_set(int mode);
+void ss_rng_mode_set(int mode);
+void ss_trng_osc_enable(void);
+void ss_trng_osc_disable(void);
+
+void ss_wait_idle(void);
+void ss_ctrl_start(void);
+void ss_ctrl_stop(void);
+
+void ss_data_src_set(int addr);
+void ss_data_dst_set(int addr);
+void ss_data_len_set(int len);
+
+int ss_reg_print(char *buf, int len);
+
+#endif /* end of _SUNXI_SECURITY_SYSTEM_REG_H_ */
+
diff --git a/drivers/crypto/sunxi-ce/v3/sunxi_ce_cdev_comm.c b/drivers/crypto/sunxi-ce/v3/sunxi_ce_cdev_comm.c
new file mode 100644
index 000000000..b3cac9ce2
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v3/sunxi_ce_cdev_comm.c
@@ -0,0 +1,447 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2014 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/internal/aead.h>
+#include <crypto/hash.h>
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+
+#include "../sunxi_ce_cdev.h"
+#include "sunxi_ce_reg.h"
+
+#define NO_DMA_MAP		(0xE7)
+#define SRC_DATA_DIR	(0)
+#define DST_DATA_DIR	(0x1)
+
+extern sunxi_ce_cdev_t	*ce_cdev;
+
+irqreturn_t sunxi_ce_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ce_cdev_t *p_cdev = (sunxi_ce_cdev_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&p_cdev->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING << (2 * i))) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&p_cdev->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&p_cdev->lock, flags);
+	return IRQ_HANDLED;
+}
+
+static int check_aes_ctx_vaild(crypto_aes_req_ctx_t *req)
+{
+	if (!req->src_buffer || !req->dst_buffer || !req->key_buffer) {
+		SS_ERR("Invalid para: src = 0x%px, dst = 0x%px key = 0x%p\n",
+				req->src_buffer, req->dst_buffer, req->key_buffer);
+		return -EINVAL;
+	}
+
+	if (req->iv_length) {
+		if (!req->iv_buf) {
+			SS_ERR("Invalid para: iv_buf = 0x%px\n", req->iv_buf);
+			return -EINVAL;
+		}
+	}
+
+	SS_DBG("key_length = %d\n", req->key_length);
+	if (req->key_length > AES_MAX_KEY_SIZE) {
+		SS_ERR("Invalid para: key_length = %d\n", req->key_length);
+		return -EINVAL;
+	} else if (req->key_length < AES_MIN_KEY_SIZE) {
+		SS_ERR("Invalid para: key_length = %d\n", req->key_length);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void ce_aes_config(crypto_aes_req_ctx_t *req, ce_task_desc_t *task)
+{
+	task->chan_id = req->channel_id;
+	ss_method_set(req->dir, SS_METHOD_AES, task);
+	ss_aes_mode_set(req->aes_mode, task);
+}
+
+static void task_iv_init(crypto_aes_req_ctx_t *req, ce_task_desc_t *task, int flag)
+{
+	if (req->iv_length) {
+		if (flag == DMA_MEM_TO_DEV) {
+			ss_iv_set(req->iv_buf, req->iv_length, task);
+			req->iv_phy = dma_map_single(ce_cdev->pdevice, req->iv_buf,
+									req->iv_length, DMA_MEM_TO_DEV);
+			SS_DBG("iv = %px, iv_phy_addr = 0x%lx\n", req->iv_buf, req->iv_phy);
+		} else if (flag == DMA_DEV_TO_MEM) {
+			dma_unmap_single(ce_cdev->pdevice,
+				req->iv_phy, req->iv_length, DMA_DEV_TO_MEM);
+		} else if (flag == NO_DMA_MAP) {
+			task->iv_addr = (req->iv_phy >> WORD_ALGIN);
+			SS_DBG("iv_phy_addr = 0x%lx\n", req->iv_phy);
+		}
+	}
+	return;
+}
+
+static ce_task_desc_t *ce_alloc_task(void)
+{
+	dma_addr_t task_phy_addr;
+	ce_task_desc_t *task;
+
+	task = dma_pool_zalloc(ce_cdev->task_pool, GFP_KERNEL, &task_phy_addr);
+	if (task == NULL) {
+		SS_ERR("Failed to alloc for task\n");
+		return NULL;
+	} else {
+		task->next_virt = NULL;
+		task->task_phy_addr = task_phy_addr;
+		SS_DBG("task = 0x%px task_phy = 0x%px\n", task, (void *)task_phy_addr);
+	}
+
+	return task;
+}
+
+static void ce_task_destroy(ce_task_desc_t *task)
+{
+	ce_task_desc_t *prev;
+
+	while (task != NULL) {
+		prev = task;
+		task = task->next_virt;
+		SS_DBG("prev = 0x%px, prev_phy = 0x%px\n", prev, (void *)prev->task_phy_addr);
+		dma_pool_free(ce_cdev->task_pool, prev, prev->task_phy_addr);
+	}
+	return;
+}
+
+static int ce_task_data_init(crypto_aes_req_ctx_t *req, phys_addr_t src_phy,
+						phys_addr_t dst_phy, u32 length, ce_task_desc_t *task)
+{
+	u32 block_size = 127 * 1024;
+	u32 block_size_word = (block_size >> 2);
+	u32 block_num, alloc_flag = 0;
+	u32 last_data_len, last_size;
+	u32 data_len_offset = 0;
+	u32 i = 0, n;
+	dma_addr_t ptask_phy;
+	dma_addr_t next_iv_phy;
+	ce_task_desc_t *ptask = task, *prev;
+
+	block_num = length / block_size;
+	last_size = length % block_size;
+	ptask->data_len = 0;
+	SS_DBG("total_len = 0x%x block_num =%d last_size =%d\n", length, block_num, last_size);
+	while (length) {
+
+		if (alloc_flag) {
+			ptask = dma_pool_zalloc(ce_cdev->task_pool, GFP_KERNEL, &ptask_phy);
+			if (ptask == NULL) {
+				SS_ERR("Failed to alloc for ptask\n");
+				return -ENOMEM;
+			}
+			ptask->chan_id  = prev->chan_id;
+			ptask->comm_ctl = prev->comm_ctl;
+			ptask->sym_ctl  = prev->sym_ctl;
+			ptask->asym_ctl = prev->asym_ctl;
+			ptask->key_addr = (prev->key_addr >> WORD_ALGIN);
+			ptask->iv_addr = (prev->iv_addr >> WORD_ALGIN);
+			ptask->data_len = 0;
+			prev->next_task_addr = (ptask_phy >> WORD_ALGIN);
+			prev->next_virt = ptask;
+			ptask->task_phy_addr = ptask_phy;
+
+			SS_DBG("ptask = 0x%px, ptask_phy = 0x%px\n", ptask, (void *)ptask_phy);
+
+			if (SS_AES_MODE_CBC == req->aes_mode) {
+				req->iv_phy = next_iv_phy;
+				task_iv_init(req, ptask, NO_DMA_MAP);
+			}
+			i = 0;
+		}
+
+		if (block_num) {
+			n = (block_num > 8) ? CE_SCATTERS_PER_TASK : block_num;
+			for (i = 0; i < n; i++) {
+				ptask->src[i].addr = ((src_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->src[i].len = block_size_word;
+				ptask->dst[i].addr = ((dst_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->dst[i].len = block_size_word;
+				ptask->data_len += block_size;
+				data_len_offset += block_size;
+			}
+			block_num = block_num - n;
+		}
+
+		SS_DBG("block_num =%d i =%d\n", block_num, i);
+
+		/*the last no engure block size*/
+		if ((block_num == 0) && (last_size == 0)) {	/*block size aglin */
+			ptask->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+			alloc_flag = 0;
+			ptask->next_task_addr = 0;
+			break;
+		} else if ((block_num == 0) && (last_size != 0)) {
+			SS_DBG("last_size =%d data_len_offset= %d\n", last_size, data_len_offset);
+			/* not block size aglin */
+			if ((i < CE_SCATTERS_PER_TASK) && (data_len_offset < length)) {
+				last_data_len = length - data_len_offset;
+
+				ptask->src[i].addr = ((src_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->src[i].len = (last_data_len >> 2);
+				ptask->dst[i].addr = ((dst_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->dst[i].len = (last_data_len >> 2);
+				ptask->data_len += last_data_len;
+				ptask->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+				ptask->next_task_addr = 0;
+				break;
+			}
+		}
+
+		if (req->dir == SS_DIR_ENCRYPT) {
+			next_iv_phy = ptask->dst[7].addr + (ptask->dst[7].len << 2) - 16;
+		} else {
+			next_iv_phy = ptask->src[7].addr + (ptask->src[7].len << 2) - 16;
+		}
+		alloc_flag = 1;
+		prev = ptask;
+
+	}
+	return 0;
+}
+
+static int aes_crypto_start(crypto_aes_req_ctx_t *req, u8 *src_buffer,
+							u32 src_length, u8 *dst_buffer)
+{
+	int ret = 0;
+	int channel_id = req->channel_id;
+	u32 padding_flag = ce_cdev->flows[req->channel_id].buf_pendding;
+	phys_addr_t key_phy = 0;
+	phys_addr_t src_phy = 0;
+	phys_addr_t dst_phy = 0;
+	ce_task_desc_t *task = NULL;
+
+	task = ce_alloc_task();
+	if (!task) {
+		return -1;
+	}
+
+	/*task_mode_set*/
+	ce_aes_config(req, task);
+
+	/*task_key_set*/
+	if (req->key_length) {
+		key_phy = dma_map_single(ce_cdev->pdevice,
+					req->key_buffer, req->key_length, DMA_MEM_TO_DEV);
+		SS_DBG("key = 0x%px, key_phy_addr = 0x%px\n", req->key_buffer, (void *)key_phy);
+		ss_key_set(req->key_buffer, req->key_length, task);
+	}
+
+	SS_DBG("ion_flag = %d padding_flag =%d", req->ion_flag, padding_flag);
+	/*task_iv_set*/
+	if (req->ion_flag && padding_flag) {
+		task_iv_init(req, task, NO_DMA_MAP);
+	} else {
+		task_iv_init(req, task, DMA_MEM_TO_DEV);
+	}
+
+	/*task_data_set*/
+	/*only the last src_buf is malloc*/
+	if (req->ion_flag && (!padding_flag)) {
+		src_phy = req->src_phy;
+	} else {
+		src_phy = dma_map_single(ce_cdev->pdevice, src_buffer, src_length, DMA_MEM_TO_DEV);
+	}
+	SS_DBG("src = 0x%px, src_phy_addr = 0x%px\n", src_buffer, (void *)src_phy);
+
+	/*the dst_buf is from user*/
+	if (req->ion_flag) {
+		dst_phy = req->dst_phy;
+	} else {
+		dst_phy = dma_map_single(ce_cdev->pdevice, dst_buffer, src_length, DMA_MEM_TO_DEV);
+	}
+	SS_DBG("dst = 0x%px, dst_phy_addr = 0x%px\n", dst_buffer, (void *)dst_phy);
+
+	ce_task_data_init(req, src_phy, dst_phy, src_length, task);
+	/*ce_print_task_info(task);*/
+
+	/*start ce*/
+	ss_pending_clear(channel_id);
+	ss_irq_enable(channel_id);
+
+	init_completion(&ce_cdev->flows[channel_id].done);
+	ss_ctrl_start(task);
+	/*ce_reg_print();*/
+
+
+	ret = wait_for_completion_timeout(&ce_cdev->flows[channel_id].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ce_task_destroy(task);
+		ce_reset();
+		ret = -ETIMEDOUT;
+	}
+
+	ss_irq_disable(channel_id);
+	ce_task_destroy(task);
+
+	/*key*/
+	if (req->key_length) {
+		dma_unmap_single(ce_cdev->pdevice,
+			key_phy, req->key_length, DMA_DEV_TO_MEM);
+	}
+
+	/*iv*/
+	if (req->ion_flag && padding_flag) {
+		;
+	} else {
+		task_iv_init(req, task, DMA_DEV_TO_MEM);
+	}
+
+	/*data*/
+	if (req->ion_flag && (!padding_flag)) {
+		;
+	} else {
+		dma_unmap_single(ce_cdev->pdevice,
+				src_phy, src_length, DMA_DEV_TO_MEM);
+	}
+
+	if (req->ion_flag) {
+		;
+	} else {
+		dma_unmap_single(ce_cdev->pdevice,
+				dst_phy, src_length, DMA_DEV_TO_MEM);
+	}
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+
+	if (ss_flow_err(channel_id)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(channel_id));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int do_aes_crypto(crypto_aes_req_ctx_t *req_ctx)
+{
+	u32 last_block_size = 0;
+	u32 block_num = 0;
+	u32 padding_size = 0;
+	u32 first_encypt_size = 0;
+	u8 data_block[AES_BLOCK_SIZE];
+	int channel_id = req_ctx->channel_id;
+	int ret;
+
+	ret = check_aes_ctx_vaild(req_ctx);
+	if (ret) {
+		return -1;
+	}
+
+	memset(data_block, 0x0, AES_BLOCK_SIZE);
+	ce_cdev->flows[channel_id].buf_pendding = 0;
+
+	if (req_ctx->dir == SS_DIR_DECRYPT) {
+		ret = aes_crypto_start(req_ctx, req_ctx->src_buffer,
+								req_ctx->src_length, req_ctx->dst_buffer);
+		if (ret) {
+			SS_ERR("aes decrypt fail\n");
+			return -2;
+		}
+		req_ctx->dst_length = req_ctx->src_length;
+	} else {
+		block_num = req_ctx->src_length / AES_BLOCK_SIZE;
+		last_block_size = req_ctx->src_length % AES_BLOCK_SIZE;
+		padding_size = AES_BLOCK_SIZE - last_block_size;
+
+		if (block_num > 0) {
+			SS_DBG("block_num = %d\n", block_num);
+			first_encypt_size = block_num * AES_BLOCK_SIZE;
+			SS_DBG("src_phy = 0x%lx, dst_phy = 0x%lx\n", req_ctx->src_phy, req_ctx->dst_phy);
+			ret = aes_crypto_start(req_ctx, req_ctx->src_buffer,
+						first_encypt_size,
+						req_ctx->dst_buffer
+						);
+			if (ret) {
+				SS_ERR("aes encrypt fail\n");
+				return -2;
+			}
+			req_ctx->dst_length = block_num * AES_BLOCK_SIZE;
+			/*not align 16byte*/
+			if (last_block_size) {
+				SS_DBG("last_block_size = %d\n", last_block_size);
+				SS_DBG("padding_size = %d\n", padding_size);
+				ce_cdev->flows[channel_id].buf_pendding = padding_size;
+				if (req_ctx->ion_flag) {
+					SS_ERR("ion memery must be 16 byte algin\n");
+				} else {
+					memcpy(data_block, req_ctx->src_buffer + first_encypt_size, last_block_size);
+					memset(data_block + last_block_size, padding_size, padding_size);
+				}
+				if (SS_AES_MODE_CBC == req_ctx->aes_mode) {
+					if (req_ctx->ion_flag) {
+						req_ctx->iv_phy = req_ctx->dst_phy + first_encypt_size - AES_BLOCK_SIZE;
+					} else {
+						req_ctx->iv_buf = req_ctx->dst_buffer + first_encypt_size - AES_BLOCK_SIZE;
+					}
+				}
+
+				ret = aes_crypto_start(req_ctx, data_block, AES_BLOCK_SIZE,
+										req_ctx->dst_buffer + first_encypt_size
+										);
+				if (ret) {
+					SS_ERR("aes encrypt fail\n");
+					return -2;
+				}
+				req_ctx->dst_length = req_ctx->dst_length + AES_BLOCK_SIZE;
+			}
+		} else {
+			SS_DBG("padding_size = %d\n", padding_size);
+			ce_cdev->flows[channel_id].buf_pendding = padding_size;
+			if (req_ctx->ion_flag) {
+				SS_ERR("ion memery must be 16 byte algin\n");
+			} else {
+				memcpy(data_block, req_ctx->src_buffer, req_ctx->src_length);
+				memset(data_block + last_block_size, padding_size, padding_size);
+			}
+			ret = aes_crypto_start(req_ctx, data_block, AES_BLOCK_SIZE,
+									req_ctx->dst_buffer
+									);
+			if (ret) {
+				SS_ERR("aes encrypt fail\n");
+				return -2;
+			}
+
+			req_ctx->dst_length = (block_num + 1) * AES_BLOCK_SIZE;
+		}
+	}
+	SS_ERR("do_aes_crypto sucess\n");
+	return 0;
+}
+
+
diff --git a/drivers/crypto/sunxi-ce/v3/sunxi_ce_proc.c b/drivers/crypto/sunxi-ce/v3/sunxi_ce_proc.c
new file mode 100644
index 000000000..56ee184c1
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v3/sunxi_ce_proc.c
@@ -0,0 +1,894 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <linux/highmem.h>
+#include <linux/dmaengine.h>
+#include <linux/dmapool.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/des.h>
+
+#include "../sunxi_ce.h"
+#include "../sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+void ce_print_task_desc(ce_task_desc_t *task)
+{
+	int i = 0;
+
+#ifndef SUNXI_CE_DEBUG
+		return;
+#endif
+	pr_err("---------------------task_info--------------------\n");
+	pr_err("task->comm_ctl = 0x%x\n", task->comm_ctl);
+	pr_err("task->sym_ctl = 0x%x\n", task->sym_ctl);
+	pr_err("task->asym_ctl = 0x%x\n", task->asym_ctl);
+	pr_err("task->key_addr = 0x%x\n", task->key_addr);
+	pr_err("task->iv_addr = 0x%x\n", task->iv_addr);
+	pr_err("task->ctr_addr = 0x%x\n", task->ctr_addr);
+	pr_err("task->data_len = 0x%x\n", task->data_len);
+	for (i = 0; i < 8; i++) {
+		if (task->src[i].addr) {
+			pr_err("task->src[%d].addr = 0x%x\n", i, task->src[i].addr);
+			pr_err("task->src[%d].len = 0x%x\n", i, task->src[i].len);
+		}
+	}
+
+	for (i = 0; i < 8; i++) {
+		if (task->dst[i].addr) {
+			pr_err("task->dst[%d].addr = 0x%x\n", i, task->dst[i].addr);
+			pr_err("task->dst[%d].len = 0x%x\n", i, task->dst[i].len);
+		}
+	}
+	pr_err("task->task_phy_addr = 0x%px\n", (void *)task->task_phy_addr);
+}
+
+void ss_task_desc_init(ce_task_desc_t *task, u32 flow)
+{
+	memset(task, 0, sizeof(ce_task_desc_t));
+	task->chan_id = flow;
+	task->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+}
+
+static int ss_sg_len(struct scatterlist *sg, int total)
+{
+	int nbyte = 0;
+	struct scatterlist *cur = sg;
+
+	while (cur != NULL) {
+		SS_DBG("cur: %px, len: %d, is_last: %ld\n",
+			cur, cur->length, sg_is_last(cur));
+		nbyte += cur->length;
+
+		cur = sg_next(cur);
+	}
+
+	return nbyte;
+}
+
+static int ss_aes_align_size(int type, int mode)
+{
+	if ((type == SS_METHOD_ECC) || CE_METHOD_IS_HMAC(type)
+		|| (CE_IS_AES_MODE(type, mode, CTS))
+		|| (CE_IS_AES_MODE(type, mode, XTS)))
+		return 4;
+	else if ((type == SS_METHOD_DES) || (type == SS_METHOD_3DES))
+		return DES_BLOCK_SIZE;
+	else
+		return AES_BLOCK_SIZE;
+}
+
+static int ss_copy_from_user(void *to, struct scatterlist *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(from);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg 0x%px (%d).\n", from, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%px, sg_addr = 0x%px, size = %d\n", vaddr, from, size);
+	memcpy(to, vaddr + from->offset, size);
+	kunmap(ppage);
+	return 0;
+}
+
+static int ss_copy_to_user(struct scatterlist *to, void *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(to);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg: 0x%px (%d).\n", to, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%px sg_addr = 0x%px, size = %d\n", vaddr, to, size);
+	memcpy(vaddr+to->offset, from, size);
+	kunmap(ppage);
+	return 0;
+}
+
+static int ss_sg_config(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int type, int mode, int tail)
+{
+	int cnt = 0;
+	int last_sg_len = 0;
+	struct scatterlist *cur = info->sg;
+
+	while (cur != NULL) {
+		if (cnt >= CE_SCATTERS_PER_TASK) {
+			WARN(1, "Too many scatter: %d\n", cnt);
+			return -1;
+		}
+		info->mapping[cnt].virt_addr = (void *)sg_dma_address(cur);
+		scatter[cnt].addr = (sg_dma_address(cur) >> WORD_ALGIN) & 0xffffffff;
+		scatter[cnt].len = sg_dma_len(cur) >> 2;
+		info->last_sg = cur;
+		last_sg_len = sg_dma_len(cur);
+		SS_DBG("%d cur: 0x%px, scatter: addr 0x%x, len %d (%d)\n",
+				cnt, cur, (scatter[cnt].addr << WORD_ALGIN),
+				scatter[cnt].len, sg_dma_len(cur));
+		cnt++;
+		cur = sg_next(cur);
+	}
+
+#ifdef SS_HASH_HW_PADDING
+	if (CE_METHOD_IS_HMAC(type)) {
+		scatter[cnt-1].len += (tail+3) >> 2;
+		info->has_padding = 0;
+		return 0;
+	}
+#endif
+
+	info->nents = cnt;
+	if (tail == 0) {
+		info->has_padding = 0;
+		return 0;
+	}
+
+	if (CE_METHOD_IS_HASH(type)) {
+		scatter[cnt-1].len -= tail >> 2;
+		return 0;
+	}
+
+	/* CTS/CTR/CFB/OFB need algin with word/block, so replace the last sg.*/
+
+	last_sg_len += ss_aes_align_size(0, mode) - tail;
+	info->padding = kzalloc(last_sg_len, GFP_KERNEL);
+	if (info->padding == NULL) {
+		SS_ERR("Failed to kmalloc(%d)!\n", last_sg_len);
+		return -ENOMEM;
+	}
+	SS_DBG("AES(%d)-%d padding: 0x%px, tail = %d/%d, cnt = %d\n",
+		type, mode, info->padding, tail, last_sg_len, cnt);
+	info->mapping[cnt - 1].virt_addr = info->padding;
+	scatter[cnt-1].addr = (virt_to_phys(info->padding) >> WORD_ALGIN) & 0xffffffff;
+	ss_copy_from_user(info->padding,
+		info->last_sg, last_sg_len - ss_aes_align_size(0, mode) + tail);
+	scatter[cnt-1].len = last_sg_len >> 2;
+
+	info->has_padding = 1;
+	return 0;
+}
+
+static void ss_aes_unpadding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int tail)
+{
+	int last_sg_len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	/* Only the dst sg need to be recovered. */
+	if (info->dir == DMA_DEV_TO_MEM) {
+		last_sg_len = scatter[index].len * 4;
+		last_sg_len -= ss_aes_align_size(0, mode) - tail;
+		ss_copy_to_user(info->last_sg, info->padding, last_sg_len);
+	}
+
+	kfree(info->padding);
+	info->padding = NULL;
+	info->has_padding = 0;
+}
+
+static void ss_aes_map_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%px, len: %d, dir: %d\n",
+		info->mapping[index].virt_addr, len, dir);
+	dma_map_single(&ss_dev->pdev->dev, info->mapping[index].virt_addr, len, dir);
+	info->dir = dir;
+}
+
+static void ss_aes_unmap_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%x, len: %d, dir: %d\n",
+		scatter[index].addr, len, dir);
+	dma_unmap_single(&ss_dev->pdev->dev, scatter[index].addr, len, dir);
+}
+
+void ss_change_clk(int type)
+{
+#ifdef SS_RSA_CLK_ENABLE
+	if ((type == SS_METHOD_RSA) || (type == SS_METHOD_ECC))
+		ss_clk_set(ss_dev->rsa_clkrate);
+	else
+		ss_clk_set(ss_dev->gen_clkrate);
+#endif
+}
+
+void ce_task_destroy(ce_task_desc_t *task)
+{
+#ifdef TASK_DMA_POOL
+	dma_pool_free(ss_dev->task_pool, task, task->task_phy_addr);
+#endif
+}
+
+static int ss_aes_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int src_len = len;
+	int align_size = 0;
+	u32 flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+#ifdef TASK_DMA_POOL
+	dma_addr_t task_phy_addr = 0;
+	task = dma_pool_alloc(ss_dev->task_pool, GFP_KERNEL, &task_phy_addr);
+	if (task == NULL) {
+		return -ENOMEM;
+	} else {
+		ss_task_desc_init(task, flow);
+		task->task_phy_addr = task_phy_addr;
+	}
+	SS_DBG("task = 0x%px task_phy = 0x%px\n", task, (void *)task->task_phy_addr);
+#else
+	ss_task_desc_init(task, flow);
+#endif
+
+	ss_change_clk(req_ctx->type);
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+#ifdef SS_XTS_MODE_ENABLE
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_method_set(req_ctx->dir, SS_METHOD_RAES, task);
+	else
+#endif
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	if ((req_ctx->type == SS_METHOD_RSA)
+		|| (req_ctx->type == SS_METHOD_DH)) {
+#ifdef SS_SUPPORT_CE_V3_1
+		if (req_ctx->mode == CE_RSA_OP_M_MUL)
+			ss_rsa_width_set(ctx->iv_size, task);
+		else
+			ss_rsa_width_set(ctx->key_size, task);
+#else
+		ss_rsa_width_set(len, task);
+#endif
+		ss_rsa_op_mode_set(req_ctx->mode, task);
+	} else if (req_ctx->type == SS_METHOD_ECC) {
+#ifdef SS_SUPPORT_CE_V3_1
+		ss_ecc_width_set(ctx->key_size, task);
+#else
+		ss_ecc_width_set(len>>1, task);
+#endif
+		ss_ecc_op_mode_set(req_ctx->mode, task);
+	} else if (CE_METHOD_IS_HMAC(req_ctx->type))
+		ss_hmac_sha1_last(task);
+	else
+		ss_aes_mode_set(req_ctx->mode, task);
+
+#ifdef SS_CFB_MODE_ENABLE
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode == SS_AES_MODE_CFB))
+		ss_cfb_bitwidth_set(req_ctx->bitwidth, task);
+#endif
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = 0x%px\n", ctx->key, (void *)phy_addr);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%px\n", task, (void *)phy_addr);
+
+#ifdef SS_XTS_MODE_ENABLE
+	SS_DBG("The current Key:\n");
+	ss_print_hex(ctx->key, ctx->key_size, ctx->key);
+
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_key_set(ctx->key, ctx->key_size/2, task);
+	else
+#endif
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		phy_addr = virt_to_phys(ctx->iv);
+		SS_DBG("ctx->iv vir = 0x%px phy = 0x%px\n", ctx->iv, (void *)phy_addr);
+		ss_iv_set(ctx->iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->iv, ctx->iv_size, DMA_MEM_TO_DEV);
+
+		phy_addr = virt_to_phys(ctx->next_iv);
+		SS_DBG("ctx->next_iv addr, vir = 0x%px, phy = 0x%px\n",
+			ctx->next_iv, (void *)phy_addr);
+		ss_cnt_set(ctx->next_iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->next_iv, ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, src_len);
+	if ((req_ctx->type == SS_METHOD_ECC)
+		|| CE_METHOD_IS_HMAC(req_ctx->type)
+		|| ((req_ctx->type == SS_METHOD_RSA) &&
+			(req_ctx->mode == CE_RSA_OP_M_MUL)))
+		src_len = ss_sg_len(req_ctx->dma_src.sg, len);
+
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src,	&req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, src_len%align_size);
+	ss_aes_map_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->dst,	&req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, len%align_size);
+	ss_aes_map_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+
+#ifdef SS_SUPPORT_CE_V3_1
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		ss_data_len_set(len, task);
+/*if (len < SZ_4K)  A bad way to determin the last packet of CTS mode. */
+			ss_cts_last(task);
+	} else {
+		SS_DBG("src_data_len = 0x%x align_size = %d data_len = %d\n",
+				src_len, align_size, DIV_ROUND_UP(src_len, align_size)*align_size/4);
+		ss_data_len_set(
+			DIV_ROUND_UP(src_len, align_size)*align_size/4, task);
+	}
+#else
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		/* A bad way to determin the last packet. */
+		/* if (len < SZ_4K) */
+		ss_cts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS)) {
+		ss_xts_first(task);
+		ss_xts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (CE_METHOD_IS_HMAC(req_ctx->type)) {
+		ss_data_len_set(src_len * 8, task);
+		task->ctr_addr = task->key_addr;
+		task->reserved[0] = src_len * 8;
+		task->key_addr = (virt_to_phys(&task->reserved[0]) >> WORD_ALGIN) & 0xffffffff;
+	} else if (req_ctx->type == SS_METHOD_RSA)
+		ss_data_len_set(len*3, task);
+	else
+		ss_data_len_set(DIV_ROUND_UP(src_len, align_size)*align_size,
+			task);
+#endif
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+#ifndef TASK_DMA_POOL
+	dma_map_single(&ss_dev->pdev->dev, task, sizeof(ce_task_desc_t),
+		DMA_MEM_TO_DEV);
+#endif
+	SS_DBG("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x, data_len:%d\n",
+		task->comm_ctl, task->sym_ctl, task->asym_ctl, task->data_len);
+
+	ce_print_task_desc(task);
+	ss_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+
+#ifndef TASK_DMA_POOL
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+#endif
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, len%align_size);
+	ss_aes_unmap_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, src_len%align_size);
+	ss_aes_unmap_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->iv),
+			ctx->iv_size, DMA_MEM_TO_DEV);
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->next_iv),
+			ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+	/* Backup the next IV from ctr_descriptor, except CBC/CTS/XTS mode. */
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode != SS_AES_MODE_CBC)
+		&& (req_ctx->mode != SS_AES_MODE_CTS)
+		&& (req_ctx->mode != SS_AES_MODE_XTS))
+		memcpy(ctx->iv, ctx->next_iv, ctx->iv_size);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+	ce_task_destroy(task);
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* verify the key_len */
+int ss_aes_key_valid(struct crypto_ablkcipher *tfm, int len)
+{
+	if (unlikely(len > SS_RSA_MAX_SIZE)) {
+		SS_ERR("Unsupported key size: %d\n", len);
+		tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return 0;
+}
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+static void ss_rsa_preprocess(ss_aes_ctx_t *ctx,
+	ss_aes_req_ctx_t *req_ctx, int len)
+{
+	struct scatterlist sg = {0};
+	ss_aes_req_ctx_t *tmp_req_ctx = NULL;
+
+	if (!((req_ctx->type == SS_METHOD_RSA) &&
+		(req_ctx->mode != CE_RSA_OP_M_MUL)))
+		return;
+
+	tmp_req_ctx = kmalloc(sizeof(ss_aes_req_ctx_t), GFP_KERNEL);
+	if (tmp_req_ctx == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", sizeof(ss_aes_req_ctx_t));
+		return;
+	}
+
+	memcpy(tmp_req_ctx, req_ctx, sizeof(ss_aes_req_ctx_t));
+	tmp_req_ctx->mode = CE_RSA_OP_M_MUL;
+
+	sg_init_one(&sg, ctx->key, ctx->iv_size*2);
+	tmp_req_ctx->dma_src.sg = &sg;
+
+	ss_aes_start(ctx, tmp_req_ctx, len);
+
+	SS_DBG("The preporcess of RSA complete!\n\n");
+	kfree(tmp_req_ctx);
+}
+#endif
+
+static int ss_rng_start(ss_aes_ctx_t *ctx, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	int rng_len = 0;
+	char *buf = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	if (trng)
+		rng_len = DIV_ROUND_UP(dlen, 32)*32; /* align with 32 Bytes */
+	else
+		rng_len = DIV_ROUND_UP(dlen, 20)*20; /* align with 20 Bytes */
+	if (rng_len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", rng_len);
+		rng_len = SS_RNG_MAX_LEN;
+	}
+
+	buf = kmalloc(rng_len, GFP_KERNEL);
+	if (buf == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", rng_len);
+		return -ENOMEM;
+	}
+
+	ss_change_clk(SS_METHOD_PRNG);
+
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	if (trng)
+		ss_method_set(SS_DIR_ENCRYPT, SS_METHOD_TRNG, task);
+	else
+		ss_method_set(SS_DIR_ENCRYPT, SS_METHOD_PRNG, task);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = 0x%p\n", ctx->key, (void *)phy_addr);
+
+	if (trng == 0) {
+		/* Must set the seed addr in PRNG. */
+		ss_key_set(ctx->key, ctx->key_size, task);
+		ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+	}
+	phy_addr = virt_to_phys(buf);
+	SS_DBG("buf addr, vir = 0x%px, phy = 0x%px\n", buf, (void *)phy_addr);
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = (virt_to_phys(buf) >> WORD_ALGIN) & 0xffffffff;
+	task->dst[0].len  = rng_len >> 2;
+	dma_map_single(&ss_dev->pdev->dev, buf, rng_len, DMA_DEV_TO_MEM);
+	SS_DBG("task->dst_addr = 0x%x\n", task->dst[0].addr);
+#ifdef SS_SUPPORT_CE_V3_1
+	ss_data_len_set(rng_len/4, task);
+#else
+	ss_data_len_set(rng_len, task);
+#endif
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n", flow, dlen, rng_len);
+
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%px\n", task, (void *)phy_addr);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	ce_print_task_desc(task);
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, TSK: 0x%08x ICR: 0x%08x TLR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_TSK), ss_reg_rd(CE_REG_ICR), ss_reg_rd(CE_REG_TLR));
+
+	ss_ctrl_start(task);
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(buf),
+		rng_len, DMA_DEV_TO_MEM);
+	if (trng == 0)
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->key),
+			ctx->key_size, DMA_MEM_TO_DEV);
+	memcpy(rdata, buf, dlen);
+	ss_irq_disable(flow);
+	ret = dlen;
+
+	return ret;
+}
+
+int ss_rng_get_random(struct crypto_rng *tfm, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	ss_aes_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, data = %px, len = %d, trng = %d\n",
+		ctx->comm.flow, data, len, trng);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+	if (trng) {
+		len = DIV_ROUND_UP(dlen, SHA256_DIGEST_SIZE)*SHA256_BLOCK_SIZE;
+		data = kzalloc(len, GFP_KERNEL);
+		if (data == NULL) {
+			SS_ERR("Failed to malloc(%d)\n", len);
+			return -ENOMEM;
+		}
+		SS_DBG("In fact, flow = %d, data = %px, len = %d\n",
+			ctx->comm.flow, data, len);
+	}
+#endif
+
+	ss_dev_lock();
+	ret = ss_rng_start(ctx, data, len, trng);
+	ss_dev_unlock();
+
+	SS_DBG("Get %d byte random.\n", ret);
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+	if (trng) {
+		ss_trng_postprocess(rdata, dlen, data, len);
+		ret = dlen;
+		kfree(data);
+	}
+#endif
+
+	return ret;
+}
+
+u32 ss_hash_start(ss_hash_ctx_t *ctx,
+		ss_aes_req_ctx_t *req_ctx, u32 len, u32 last)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	u32 blk_size = ss_hash_blk_size(req_ctx->type);
+	char *digest = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	/* Total len is too small, so process it in the padding data later. */
+	if ((last == 0) && (len > 0) && (len < blk_size)) {
+		ctx->cnt += len;
+		return 0;
+	}
+	ss_change_clk(req_ctx->type);
+
+	digest = kzalloc(SHA512_DIGEST_SIZE, GFP_KERNEL);
+	if (digest == NULL) {
+		SS_ERR("Failed to kmalloc(%d)\n", SHA512_DIGEST_SIZE);
+		return -ENOMEM;
+	}
+
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d / %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+	SS_DBG("IV address = 0x%px, size = %d\n", ctx->md, ctx->md_size);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%px\n", task, (void *)phy_addr);
+
+	ss_iv_set(ctx->md, ctx->md_size, task);
+	ss_iv_mode_set(CE_HASH_IV_INPUT, task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->md, ctx->md_size, DMA_MEM_TO_DEV);
+
+#ifdef SS_SUPPORT_CE_V3_1
+	ss_data_len_set((len - len%blk_size)/4, task);
+#else
+	if (last == 1) {
+		ss_hmac_sha1_last(task);
+		ss_data_len_set(ctx->tail_len*8, task);
+	} else
+		ss_data_len_set((len - len%blk_size)*8, task);
+#endif
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src,
+		&req_ctx->dma_src, req_ctx->type, 0, len%blk_size);
+
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		task->src[0].len = (ctx->tail_len + 3)/4;
+		SS_DBG("cnt %d, tail_len %d.\n", ctx->cnt, ctx->tail_len);
+		ctx->cnt <<= 3; /* Translate to bits in the last pakcket */
+		dma_map_single(&ss_dev->pdev->dev, &ctx->cnt, 4,
+			DMA_MEM_TO_DEV);
+		task->key_addr = (virt_to_phys(&ctx->cnt) >> WORD_ALGIN) & 0xffffffff;
+	}
+#endif
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = (virt_to_phys(digest) >> WORD_ALGIN) & 0xffffffff;
+	task->dst[0].len  = ctx->md_size  >> 2;
+	dma_map_single(&ss_dev->pdev->dev,
+		digest, SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	phy_addr = virt_to_phys(digest);
+	SS_DBG("digest addr, vir = 0x%px, phy = 0x%px\n", digest, (void *)phy_addr);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_ICR));
+	/*ce_print_task_desc(task);*/
+
+	ss_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(digest),
+		SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->md),
+		ctx->md_size, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(&ctx->cnt), 4,
+			DMA_MEM_TO_DEV);
+		ctx->cnt >>= 3;
+	}
+#endif
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+	ss_print_hex(digest, SHA512_DIGEST_SIZE, digest);
+
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		kfree(digest);
+		return -EINVAL;
+	}
+
+	/* Backup the MD to ctx->md. */
+	memcpy(ctx->md, digest, ctx->md_size);
+
+	if (last == 0)
+		ctx->cnt += len;
+	kfree(digest);
+	return 0;
+}
+
+
+void ss_load_iv(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx,
+	char *buf, int size)
+{
+	if (buf == NULL)
+		return;
+
+	/* Only AES/DES/3DES-ECB don't need IV. */
+	if (CE_METHOD_IS_AES(req_ctx->type) &&
+		(req_ctx->mode == SS_AES_MODE_ECB))
+		return;
+
+	/* CBC/CTS need update the IV eachtime. */
+	if ((ctx->cnt == 0)
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CBC))
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS))) {
+		SS_DBG("IV address = 0x%px, size = %d\n", buf, size);
+		ctx->iv_size = size;
+		memcpy(ctx->iv, buf, ctx->iv_size);
+	}
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, ctx->iv_size, ctx->iv);
+}
+
+int ss_aes_one_req(sunxi_ss_t *sss, struct ablkcipher_request *req)
+{
+	int ret = 0;
+	struct crypto_ablkcipher *tfm = NULL;
+	ss_aes_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = 0x%px, dst = 0x%px\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_ablkcipher_reqtfm(req);
+	req_ctx = ablkcipher_request_ctx(req);
+	ctx = crypto_ablkcipher_ctx(tfm);
+
+	ss_load_iv(ctx, req_ctx, req->info, crypto_ablkcipher_ivsize(tfm));
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+	ss_rsa_preprocess(ctx, req_ctx, req->nbytes);
+#endif
+
+	ret = ss_aes_start(ctx, req_ctx, req->nbytes);
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR) {
+		SS_DBG("CNT: %08x %08x %08x %08x\n",
+			*(int *)&ctx->iv[0], *(int *)&ctx->iv[4],
+			*(int *)&ctx->iv[8], *(int *)&ctx->iv[12]);
+	}
+#endif
+
+	ctx->cnt += req->nbytes;
+	return ret;
+}
+
+irqreturn_t sunxi_ss_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ss_t *sss = (sunxi_ss_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&sss->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING<<i)) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&sss->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&sss->lock, flags);
+	return IRQ_HANDLED;
+}
diff --git a/drivers/crypto/sunxi-ce/v3/sunxi_ce_proc_walk.c b/drivers/crypto/sunxi-ce/v3/sunxi_ce_proc_walk.c
new file mode 100644
index 000000000..509ce686a
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v3/sunxi_ce_proc_walk.c
@@ -0,0 +1,1171 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <linux/highmem.h>
+#include <linux/dmaengine.h>
+#include <linux/dmapool.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/des.h>
+
+#include "../sunxi_ce.h"
+#include "../sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+void ss_task_desc_init(ce_task_desc_t *task, u32 flow)
+{
+	memset(task, 0, sizeof(ce_task_desc_t));
+	task->chan_id = flow;
+	task->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+}
+
+static int ss_sg_len(struct scatterlist *sg, int total)
+{
+	int nbyte = 0;
+	struct scatterlist *cur = sg;
+
+	while (cur != NULL) {
+		SS_DBG("cur: %p, len: %d, is_last: %ld\n",
+			cur, cur->length, sg_is_last(cur));
+		nbyte += cur->length;
+
+		cur = sg_next(cur);
+	}
+
+	return nbyte;
+}
+
+static int ss_aes_align_size(int type, int mode)
+{
+	if ((type == SS_METHOD_ECC) || CE_METHOD_IS_HMAC(type)
+		|| (CE_IS_AES_MODE(type, mode, CTS))
+		|| (CE_IS_AES_MODE(type, mode, XTS)))
+		return 4;
+	else if ((type == SS_METHOD_DES) || (type == SS_METHOD_3DES))
+		return DES_BLOCK_SIZE;
+	else
+		return AES_BLOCK_SIZE;
+}
+
+static int ss_copy_from_user(void *to, struct scatterlist *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(from);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg 0x%p (%d).\n", from, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%p, sg_addr = 0x%p, size = %d\n", vaddr, from, size);
+	memcpy(to, vaddr + from->offset, size);
+	kunmap(ppage);
+	return 0;
+}
+
+static int ss_copy_to_user(struct scatterlist *to, void *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(to);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg: 0x%p (%d).\n", to, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%p, sg_addr = 0x%p, size = %d\n", vaddr, to, size);
+	memcpy(vaddr+to->offset, from, size);
+	kunmap(ppage);
+	return 0;
+}
+
+static int ss_sg_config(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int type, int mode, int tail)
+{
+	int cnt = 0;
+	int last_sg_len = 0;
+	struct scatterlist *cur = info->sg;
+
+	while (cur != NULL) {
+		if (cnt >= CE_SCATTERS_PER_TASK) {
+			WARN(1, "Too many scatter: %d\n", cnt);
+			return -1;
+		}
+		info->mapping[cnt].virt_addr = (void *)sg_dma_address(cur);
+		scatter[cnt].addr = (sg_dma_address(cur) >> WORD_ALGIN) & 0xffffffff;
+		scatter[cnt].len = sg_dma_len(cur) >> 2;
+		info->last_sg = cur;
+		last_sg_len = sg_dma_len(cur);
+		SS_DBG("%d cur: 0x%p, scatter: addr 0x%x, len %d (%d)\n",
+				cnt, cur, (scatter[cnt].addr << WORD_ALGIN),
+				scatter[cnt].len, sg_dma_len(cur));
+		cnt++;
+		cur = sg_next(cur);
+	}
+
+#ifdef SS_HASH_HW_PADDING
+	if (CE_METHOD_IS_HMAC(type)) {
+		scatter[cnt-1].len += (tail+3) >> 2;
+		info->has_padding = 0;
+		return 0;
+	}
+#endif
+
+	info->nents = cnt;
+	if (tail == 0) {
+		info->has_padding = 0;
+		return 0;
+	}
+
+	if (CE_METHOD_IS_HASH(type)) {
+		scatter[cnt-1].len -= tail >> 2;
+		return 0;
+	}
+
+	/* CTS/CTR/CFB/OFB need algin with word/block, so replace the last sg.*/
+
+	last_sg_len += ss_aes_align_size(0, mode) - tail;
+	info->padding = kzalloc(last_sg_len, GFP_KERNEL);
+	if (info->padding == NULL) {
+		SS_ERR("Failed to kmalloc(%d)!\n", last_sg_len);
+		return -ENOMEM;
+	}
+	SS_DBG("AES(%d)-%d padding: 0x%p, tail = %d/%d, cnt = %d\n",
+		type, mode, info->padding, tail, last_sg_len, cnt);
+	info->mapping[cnt - 1].virt_addr = info->padding;
+	scatter[cnt-1].addr = (virt_to_phys(info->padding) >> WORD_ALGIN) & 0xffffffff;
+	ss_copy_from_user(info->padding,
+		info->last_sg, last_sg_len - ss_aes_align_size(0, mode) + tail);
+	scatter[cnt-1].len = last_sg_len >> 2;
+
+	info->has_padding = 1;
+	return 0;
+}
+
+static void ss_aes_unpadding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int tail)
+{
+	int last_sg_len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	/* Only the dst sg need to be recovered. */
+	if (info->dir == DMA_DEV_TO_MEM) {
+		last_sg_len = scatter[index].len * 4;
+		last_sg_len -= ss_aes_align_size(0, mode) - tail;
+		ss_copy_to_user(info->last_sg, info->padding, last_sg_len);
+	}
+
+	kfree(info->padding);
+	info->padding = NULL;
+	info->has_padding = 0;
+}
+
+static void ss_aes_map_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%p, len: %d, dir: %d\n",
+		info->mapping[index].virt_addr, len, dir);
+	dma_map_single(&ss_dev->pdev->dev, info->mapping[index].virt_addr, len, dir);
+	info->dir = dir;
+}
+
+static void ss_aes_unmap_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%x, len: %d, dir: %d\n",
+		scatter[index].addr, len, dir);
+	dma_unmap_single(&ss_dev->pdev->dev, scatter[index].addr, len, dir);
+}
+
+void ss_change_clk(int type)
+{
+#ifdef SS_RSA_CLK_ENABLE
+	if ((type == SS_METHOD_RSA) || (type == SS_METHOD_ECC))
+		ss_clk_set(ss_dev->rsa_clkrate);
+	else
+		ss_clk_set(ss_dev->gen_clkrate);
+#endif
+}
+#ifdef CE_USE_WALK
+void ss_task_chan_init(ce_task_desc_t *task, u32 flow)
+{
+	task->chan_id = flow;
+}
+
+static phys_addr_t ss_task_iv_init(struct ablkcipher_walk *walk, struct ablkcipher_request *req, ce_task_desc_t *task)
+{
+	ce_task_desc_t *p_task;
+	phys_addr_t nextiv_phys = 0;
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	ss_aes_ctx_t *ctx =	crypto_ablkcipher_ctx(tfm);
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+	u32 iv_size = crypto_ablkcipher_ivsize(tfm);
+
+	SS_DBG("iv len =%d\n", crypto_ablkcipher_ivsize(tfm));
+	if (iv_size) {
+		dma_map_single(&ss_dev->pdev->dev, req->info, iv_size, DMA_BIDIRECTIONAL);
+		dma_map_single(&ss_dev->pdev->dev, ctx->next_iv, iv_size, DMA_BIDIRECTIONAL);
+		nextiv_phys = virt_to_phys(req->info);
+		SS_DBG("walk_iv = 0x%p req_info = 0x%p\n", (void *)nextiv_phys, (void *)virt_to_phys(req->info));
+		ss_printf_hex(req->info, iv_size, req->info);
+		ss_iv_set(req->info, iv_size, task);
+	}
+
+	for (p_task = task; p_task != NULL; p_task = p_task->next) {
+		if (p_task != task) {
+			p_task->chan_id  = task->chan_id;
+			p_task->comm_ctl = task->comm_ctl;
+			p_task->sym_ctl  = task->sym_ctl;
+			p_task->asym_ctl = task->asym_ctl;
+			p_task->key_addr = task->key_addr;
+		}
+
+		if (iv_size) {
+			if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTR)) {
+				SS_ERR("ctr\n");
+				if (p_task != task) {
+					ss_iv_set(req->info, iv_size, p_task);
+				}
+				ss_cnt_set(req->info, iv_size, p_task);
+			} else if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CBC)) {
+				ss_iv_set_phys(nextiv_phys, iv_size, p_task);
+				if (req_ctx->dir == SS_DIR_ENCRYPT) {
+					nextiv_phys = (p_task->dst_phys[p_task->dst_cnt - 1].addr) +
+						(p_task->dst[p_task->dst_cnt - 1].len << 2) - iv_size;
+				} else {
+					nextiv_phys = (p_task->src_phys[p_task->src_cnt - 1].addr) +
+					(p_task->src[p_task->src_cnt - 1].len << 2) - iv_size;
+				}
+			}
+		}
+
+		/*The last entry should enable interrupt.*/
+		if (p_task->next == NULL) {
+			if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+				ss_cts_last(p_task);
+			} else if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS)) {
+				ss_xts_first(p_task);
+				ss_xts_last(p_task);
+			}
+			p_task->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+		}
+
+		SS_DBG("Task addr, vir = 0x%p, phy = 0x%lx\n", p_task, p_task->pthis);
+		SS_DBG("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x,"
+				"key_addr: 0x%p, iv_addr: 0x%p\n",
+				p_task->comm_ctl, p_task->sym_ctl, p_task->asym_ctl,
+				(void *)p_task->key_addr, (void *)p_task->iv_addr);
+		SS_DBG("preCE, data_len:%d\n", p_task->data_len);
+	}
+	return nextiv_phys;
+}
+
+static void ss_task_data_len_set(struct ablkcipher_request *req,
+									ce_task_desc_t *task)
+{
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+	u32 word_len = 0;
+
+	if (CE_METHOD_IS_AES(req_ctx->type)) {
+		if (task->data_len & (AES_BLOCK_SIZE - 1)) {
+				WARN(1, "data len not aes block align: %d\n", task->data_len);
+		}
+	} else if (CE_METHOD_IS_DES(req_ctx->type)) {
+		if (task->data_len & (DES_BLOCK_SIZE - 1)) {
+				WARN(1, "data len not block align: %d\n", task->data_len);
+		}
+	}
+
+#ifdef SS_SUPPORT_CE_V3_1
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		;
+	} else {
+		word_len = task->data_len >> 2;
+		ss_data_len_set(word_len, task);
+	}
+#else
+	if (CE_METHOD_IS_HMAC(req_ctx->type)) {
+		ss_data_len_set(task->data_len * 8, task);
+		task->ctr_addr = task->key_addr;
+		task->reserved[0] = task->data_len * 8;
+		task->key_addr = (virt_to_phys(&task->reserved[0]) >> WORD_ALGIN) & 0xffffffff;
+	} else if (req_ctx->type == SS_METHOD_RSA) {
+		ss_data_len_set(task->data_len * 3, task);
+	}
+#endif
+	SS_ERR("task->data_len =%d\n", task->data_len);
+	return;
+}
+
+static void ss_task_destroy(ce_task_desc_t *task)
+{
+	ce_task_desc_t *prev;
+
+	while (task != NULL) {
+		prev = task;
+		task = task->next;
+		dma_pool_free(ss_dev->task_pool, prev, prev->pthis);
+	}
+	return;
+}
+
+static ce_task_desc_t *ss_task_create(struct ablkcipher_walk *walk,
+					struct ablkcipher_request *req)
+{
+	int err, nbytes;
+	ce_task_desc_t *first = NULL, *prev = NULL, *task;
+
+	dma_map_sg(&ss_dev->pdev->dev, req->src,
+			sg_nents(req->src), DMA_TO_DEVICE);
+
+	dma_map_sg(&ss_dev->pdev->dev, req->dst,
+			sg_nents(req->dst), DMA_FROM_DEVICE);
+
+	ablkcipher_walk_init(walk, req->dst, req->src, req->nbytes);
+	err = ablkcipher_walk_phys(req, walk);
+	if (err)
+		return NULL;
+
+	while (walk->nbytes) {
+
+		dma_addr_t ptask = 0;
+
+		task = dma_pool_alloc(ss_dev->task_pool, GFP_KERNEL, &ptask);
+		if (IS_ERR_OR_NULL(task))
+			goto err;
+
+		/*
+		 * Clear the dirty data of new task
+		 */
+		memset(task, 0, sizeof(ce_task_desc_t));
+		task->pthis = ptask;
+		task->next  = NULL;
+		SS_DBG("Task addr, vir = 0x%p, phy = 0x%p\n", task, (void *)task->pthis);
+		if (prev != NULL) {
+			prev->pnext = (u32)task->pthis;
+			prev->next  = task;
+		} else {
+			first = task;
+		}
+
+		while ((nbytes = walk->nbytes) != 0) {
+			phys_addr_t dst_paddr, src_paddr;
+			SS_ERR("#nbytes = %d\n", nbytes);
+			if (nbytes & 0x3) {
+				SS_ERR("#nbytes = %d\n", nbytes);
+				goto err;
+			}
+			src_paddr = (page_to_phys(walk->src.page) +
+					walk->src.offset);
+			dst_paddr = (page_to_phys(walk->dst.page) +
+					walk->dst.offset);
+
+			task->src_phys[task->src_cnt].addr = src_paddr;
+			task->dst_phys[task->dst_cnt].addr = dst_paddr;
+
+			task->src[task->src_cnt].addr = (src_paddr >> WORD_ALGIN) & 0xffffffff;
+			task->src[task->src_cnt].len = nbytes >> 2;
+			SS_DBG("task->src[%d] = 0x%lx, len = %d\n", task->src_cnt,
+					task->src[task->src_cnt].addr,
+					task->src[task->src_cnt].len << 2);
+
+			task->src_cnt++;
+			task->dst[task->dst_cnt].addr = (dst_paddr >> WORD_ALGIN) & 0xffffffff;
+			task->dst[task->dst_cnt].len = nbytes >> 2;
+			SS_DBG("task->dst[%d] = 0x%lx, len = %d\n", task->dst_cnt,
+					task->dst[task->dst_cnt].addr,
+					task->dst[task->dst_cnt].len << 2);
+
+			task->dst_cnt++;
+			task->data_len += nbytes;
+			err = ablkcipher_walk_done(req, walk, 0);
+			if (err) {
+				break;
+			}
+			if ((task->src_cnt >= CE_SCATTERS_PER_TASK)
+				|| (task->dst_cnt >= CE_SCATTERS_PER_TASK))
+				goto out;
+		}
+
+out:
+		ss_task_data_len_set(req, task);
+		prev = task;
+	}
+
+	return first;
+err:
+	if (first != NULL)
+		ss_task_destroy(first);
+
+	return NULL;
+}
+
+static int ss_aes_start_by_walk(struct ablkcipher_request *req)
+{
+	int ret = 0;
+	phys_addr_t nextiv_phys = 0;
+	u32 len = req->nbytes;
+	int align_size = 0;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = NULL;
+	void *nextiv_virt = NULL;
+	struct ablkcipher_walk walk;
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	ss_aes_req_ctx_t *req_ctx = ablkcipher_request_ctx(req);
+	ss_aes_ctx_t *ctx = crypto_ablkcipher_ctx(tfm);
+	u32 flow = ctx->comm.flow;
+	u32 iv_size = crypto_ablkcipher_ivsize(tfm);
+
+	ss_change_clk(req_ctx->type);
+
+	task = ss_task_create(&walk, req);
+	if (task == NULL)
+		return -ENOMEM;
+
+	/*ss_dev->flows[ctx->comm.flow].task = task;*/
+	/*configure the first entry*/
+	ss_task_chan_init(task, flow);
+#ifdef SS_XTS_MODE_ENABLE
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_method_set(req_ctx->dir, SS_METHOD_RAES, task);
+	else
+#endif
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	ss_aes_mode_set(req_ctx->mode, task);
+
+#ifdef SS_CFB_MODE_ENABLE
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode == SS_AES_MODE_CFB))
+		ss_cfb_bitwidth_set(req_ctx->bitwidth, task);
+#endif
+
+	//phy_addr = virt_to_phys(ctx->key);
+	//SS_DBG("ctx->key addr, vir = 0x%p, phy = 0x%lx\n", ctx->key, phy_addr);
+
+#ifdef SS_XTS_MODE_ENABLE
+	SS_DBG("The current Key:\n");
+	ss_print_hex(ctx->key, ctx->key_size, ctx->key);
+
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_key_set(ctx->key, ctx->key_size/2, task);
+	else
+#endif
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_TO_DEVICE);
+
+	nextiv_phys = ss_task_iv_init(&walk, req, task);
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+
+	ss_ctrl_start(task);
+	SS_ERR("After CE, TSK: 0x%08x, ICR: 0x%08x TLR: 0x%08x\n",
+	ss_reg_rd(CE_REG_TSK), ss_reg_rd(CE_REG_ICR), ss_reg_rd(CE_REG_TLR));
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSK: 0x%08x, ICR: 0x%08x ,ISR: 0x%08x TLR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSK), ss_reg_rd(CE_REG_ICR), ss_reg_rd(CE_REG_ISR), ss_reg_rd(CE_REG_TLR));
+
+	ss_irq_disable(flow);
+
+	ablkcipher_walk_complete(&walk);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->key),
+			ctx->key_size, DMA_MEM_TO_DEV);
+
+	dma_unmap_sg(&ss_dev->pdev->dev, req->src,
+			sg_nents(req->src), DMA_TO_DEVICE);
+
+	dma_unmap_sg(&ss_dev->pdev->dev, req->dst,
+			sg_nents(req->dst), DMA_FROM_DEVICE);
+
+	/*Backup the next IV*/
+	if (iv_size) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(req->info),
+							iv_size, DMA_BIDIRECTIONAL);
+		/*ctr already in req->info*/
+		if (CE_METHOD_IS_AES(req_ctx->type)
+			&& (req_ctx->mode != SS_AES_MODE_CTR)) {
+			nextiv_virt = kmap(phys_to_page(nextiv_phys)) + offset_in_page(nextiv_phys);
+			SS_DBG("nextiv_phys = 0x%lx virt = 0x%p\n", nextiv_phys, nextiv_virt);
+			memcpy(req->info, nextiv_virt, iv_size);
+			ss_printf_hex(req->info, iv_size, req->info);
+			kunmap(virt_to_page(nextiv_virt));
+		}
+	}
+
+	ss_task_destroy(task);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+#endif
+
+static int ss_aes_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int src_len = len;
+	int align_size = 0;
+	u32 flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	ss_change_clk(req_ctx->type);
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+#ifdef SS_XTS_MODE_ENABLE
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_method_set(req_ctx->dir, SS_METHOD_RAES, task);
+	else
+#endif
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	if ((req_ctx->type == SS_METHOD_RSA)
+		|| (req_ctx->type == SS_METHOD_DH)) {
+#ifdef SS_SUPPORT_CE_V3_1
+		if (req_ctx->mode == CE_RSA_OP_M_MUL)
+			ss_rsa_width_set(ctx->iv_size, task);
+		else
+			ss_rsa_width_set(ctx->key_size, task);
+#else
+		ss_rsa_width_set(len, task);
+#endif
+		ss_rsa_op_mode_set(req_ctx->mode, task);
+	} else if (req_ctx->type == SS_METHOD_ECC) {
+#ifdef SS_SUPPORT_CE_V3_1
+		ss_ecc_width_set(ctx->key_size, task);
+#else
+		ss_ecc_width_set(len>>1, task);
+#endif
+		ss_ecc_op_mode_set(req_ctx->mode, task);
+	} else if (CE_METHOD_IS_HMAC(req_ctx->type))
+		ss_hmac_sha1_last(task);
+	else
+		ss_aes_mode_set(req_ctx->mode, task);
+
+#ifdef SS_CFB_MODE_ENABLE
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode == SS_AES_MODE_CFB))
+		ss_cfb_bitwidth_set(req_ctx->bitwidth, task);
+#endif
+
+	SS_ERR("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%p, phy = 0x%p\n", ctx->key, (void *)phy_addr);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%p, phy = 0x%p\n", task, (void *)phy_addr);
+
+#ifdef SS_XTS_MODE_ENABLE
+	SS_DBG("The current Key:\n");
+	ss_print_hex(ctx->key, ctx->key_size, ctx->key);
+
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_key_set(ctx->key, ctx->key_size/2, task);
+	else
+#endif
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		phy_addr = virt_to_phys(ctx->iv);
+		SS_DBG("ctx->iv vir = 0x%p phy = 0x%p\n", ctx->iv, (void *)phy_addr);
+		ss_iv_set(ctx->iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->iv, ctx->iv_size, DMA_MEM_TO_DEV);
+
+		phy_addr = virt_to_phys(ctx->next_iv);
+		SS_DBG("ctx->next_iv addr, vir = 0x%p, phy = 0x%p\n",
+			ctx->next_iv, (void *)phy_addr);
+		ss_cnt_set(ctx->next_iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->next_iv, ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, src_len);
+	if ((req_ctx->type == SS_METHOD_ECC)
+		|| CE_METHOD_IS_HMAC(req_ctx->type)
+		|| ((req_ctx->type == SS_METHOD_RSA) &&
+			(req_ctx->mode == CE_RSA_OP_M_MUL)))
+		src_len = ss_sg_len(req_ctx->dma_src.sg, len);
+
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src,	&req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, src_len%align_size);
+	ss_aes_map_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->dst,	&req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, len%align_size);
+	ss_aes_map_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+
+#ifdef SS_SUPPORT_CE_V3_1
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		ss_data_len_set(len, task);
+/*if (len < SZ_4K)  A bad way to determin the last packet of CTS mode. */
+			ss_cts_last(task);
+	} else
+		ss_data_len_set(
+			DIV_ROUND_UP(src_len, align_size)*align_size/4, task);
+#else
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		/* A bad way to determin the last packet. */
+		/* if (len < SZ_4K) */
+		ss_cts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS)) {
+		ss_xts_first(task);
+		ss_xts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (CE_METHOD_IS_HMAC(req_ctx->type)) {
+		ss_data_len_set(src_len * 8, task);
+		task->ctr_addr = task->key_addr;
+		task->reserved[0] = src_len * 8;
+		task->key_addr = (virt_to_phys(&task->reserved[0]) >> WORD_ALGIN) & 0xffffffff;
+	} else if (req_ctx->type == SS_METHOD_RSA)
+		ss_data_len_set(len*3, task);
+	else
+		ss_data_len_set(DIV_ROUND_UP(src_len, align_size)*align_size,
+			task);
+#endif
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task, sizeof(ce_task_desc_t),
+		DMA_MEM_TO_DEV);
+
+	SS_ERR("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x, data_len:%d\n",
+		task->comm_ctl, task->sym_ctl, task->asym_ctl, task->data_len);
+	/*ss_print_task_info(task);*/
+	ss_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	/*ss_print_task_info(task);*/
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, len%align_size);
+	ss_aes_unmap_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, src_len%align_size);
+	ss_aes_unmap_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->iv),
+			ctx->iv_size, DMA_MEM_TO_DEV);
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->next_iv),
+			ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+	/* Backup the next IV from ctr_descriptor, except CBC/CTS/XTS mode. */
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode != SS_AES_MODE_CBC)
+		&& (req_ctx->mode != SS_AES_MODE_CTS)
+		&& (req_ctx->mode != SS_AES_MODE_XTS))
+		memcpy(ctx->iv, ctx->next_iv, ctx->iv_size);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* verify the key_len */
+int ss_aes_key_valid(struct crypto_ablkcipher *tfm, int len)
+{
+	if (unlikely(len > SS_RSA_MAX_SIZE)) {
+		SS_ERR("Unsupported key size: %d\n", len);
+		tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return 0;
+}
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+static void ss_rsa_preprocess(ss_aes_ctx_t *ctx,
+	ss_aes_req_ctx_t *req_ctx, int len)
+{
+	struct scatterlist sg = {0};
+	ss_aes_req_ctx_t *tmp_req_ctx = NULL;
+
+	if (!((req_ctx->type == SS_METHOD_RSA) &&
+		(req_ctx->mode != CE_RSA_OP_M_MUL)))
+		return;
+
+	tmp_req_ctx = kmalloc(sizeof(ss_aes_req_ctx_t), GFP_KERNEL);
+	if (tmp_req_ctx == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", sizeof(ss_aes_req_ctx_t));
+		return;
+	}
+
+	memcpy(tmp_req_ctx, req_ctx, sizeof(ss_aes_req_ctx_t));
+	tmp_req_ctx->mode = CE_RSA_OP_M_MUL;
+
+	sg_init_one(&sg, ctx->key, ctx->iv_size*2);
+	tmp_req_ctx->dma_src.sg = &sg;
+
+	ss_aes_start(ctx, tmp_req_ctx, len);
+
+	SS_DBG("The preporcess of RSA complete!\n\n");
+	kfree(tmp_req_ctx);
+}
+#endif
+
+static int ss_rng_start(ss_aes_ctx_t *ctx, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	int rng_len = 0;
+	char *buf = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	if (trng)
+		rng_len = DIV_ROUND_UP(dlen, 32)*32; /* align with 32 Bytes */
+	else
+		rng_len = DIV_ROUND_UP(dlen, 20)*20; /* align with 20 Bytes */
+	if (rng_len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", rng_len);
+		rng_len = SS_RNG_MAX_LEN;
+	}
+
+	buf = kmalloc(rng_len, GFP_KERNEL);
+	if (buf == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", rng_len);
+		return -ENOMEM;
+	}
+
+	ss_change_clk(SS_METHOD_PRNG);
+
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	if (trng)
+		ss_method_set(SS_DIR_ENCRYPT, SS_METHOD_TRNG, task);
+	else
+		ss_method_set(SS_DIR_ENCRYPT, SS_METHOD_PRNG, task);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%p, phy = 0x%p\n", ctx->key, (void *)phy_addr);
+
+	if (trng == 0) {
+		/* Must set the seed addr in PRNG. */
+		ss_key_set(ctx->key, ctx->key_size, task);
+		ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+	}
+	phy_addr = virt_to_phys(buf);
+	SS_DBG("buf addr, vir = 0x%p, phy = 0x%p\n", buf, (void *)phy_addr);
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = (virt_to_phys(buf) >> WORD_ALGIN) & 0xffffffff;
+	task->dst[0].len  = rng_len >> 2;
+	dma_map_single(&ss_dev->pdev->dev, buf, rng_len, DMA_DEV_TO_MEM);
+	SS_DBG("task->dst_addr = 0x%x\n", task->dst[0].addr);
+#ifdef SS_SUPPORT_CE_V3_1
+	ss_data_len_set(rng_len/4, task);
+#else
+	ss_data_len_set(rng_len, task);
+#endif
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n", flow, dlen, rng_len);
+
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%p, phy = 0x%p\n", task, (void *)phy_addr);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	ss_ctrl_start(task);
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, TSK: 0x%08x ICR: 0x%08x TLR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_TSK), ss_reg_rd(CE_REG_ICR), ss_reg_rd(CE_REG_TLR));
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(buf),
+		rng_len, DMA_DEV_TO_MEM);
+	if (trng == 0)
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->key),
+			ctx->key_size, DMA_MEM_TO_DEV);
+	memcpy(rdata, buf, dlen);
+	ss_irq_disable(flow);
+	ret = dlen;
+
+	return ret;
+}
+
+int ss_rng_get_random(struct crypto_rng *tfm, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	ss_aes_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, data = %p, len = %d, trng = %d\n",
+		ctx->comm.flow, data, len, trng);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+	if (trng) {
+		len = DIV_ROUND_UP(dlen, SHA256_DIGEST_SIZE)*SHA256_BLOCK_SIZE;
+		data = kzalloc(len, GFP_KERNEL);
+		if (data == NULL) {
+			SS_ERR("Failed to malloc(%d)\n", len);
+			return -ENOMEM;
+		}
+		SS_DBG("In fact, flow = %d, data = %p, len = %d\n",
+			ctx->comm.flow, data, len);
+	}
+#endif
+
+	ss_dev_lock();
+	ret = ss_rng_start(ctx, data, len, trng);
+	ss_dev_unlock();
+
+	SS_DBG("Get %d byte random.\n", ret);
+
+#ifdef SS_TRNG_POSTPROCESS_ENABLE
+	if (trng) {
+		ss_trng_postprocess(rdata, dlen, data, len);
+		ret = dlen;
+		kfree(data);
+	}
+#endif
+
+	return ret;
+}
+
+u32 ss_hash_start(ss_hash_ctx_t *ctx,
+		ss_aes_req_ctx_t *req_ctx, u32 len, u32 last)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	u32 blk_size = ss_hash_blk_size(req_ctx->type);
+	char *digest = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	/* Total len is too small, so process it in the padding data later. */
+	if ((last == 0) && (len > 0) && (len < blk_size)) {
+		ctx->cnt += len;
+		return 0;
+	}
+	ss_change_clk(req_ctx->type);
+
+	digest = kzalloc(SHA512_DIGEST_SIZE, GFP_KERNEL);
+	if (digest == NULL) {
+		SS_ERR("Failed to kmalloc(%d)\n", SHA512_DIGEST_SIZE);
+		return -ENOMEM;
+	}
+
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d / %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+	SS_DBG("IV address = 0x%p, size = %d\n", ctx->md, ctx->md_size);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%p, phy = 0x%p\n", task, (void *)phy_addr);
+
+	ss_iv_set(ctx->md, ctx->md_size, task);
+	ss_iv_mode_set(CE_HASH_IV_INPUT, task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->md, ctx->md_size, DMA_MEM_TO_DEV);
+
+#ifdef SS_SUPPORT_CE_V3_1
+	ss_data_len_set((len - len%blk_size)/4, task);
+#else
+	if (last == 1) {
+		ss_hmac_sha1_last(task);
+		ss_data_len_set(ctx->tail_len*8, task);
+	} else
+		ss_data_len_set((len - len%blk_size)*8, task);
+#endif
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src,
+		&req_ctx->dma_src, req_ctx->type, 0, len%blk_size);
+
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		task->src[0].len = (ctx->tail_len + 3)/4;
+		SS_DBG("cnt %d, tail_len %d.\n", ctx->cnt, ctx->tail_len);
+		ctx->cnt <<= 3; /* Translate to bits in the last pakcket */
+		dma_map_single(&ss_dev->pdev->dev, &ctx->cnt, 4,
+			DMA_MEM_TO_DEV);
+		task->key_addr = (virt_to_phys(&ctx->cnt) >> WORD_ALGIN) & 0xffffffff;
+	}
+#endif
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = (virt_to_phys(digest) >> WORD_ALGIN) & 0xffffffff;
+	task->dst[0].len  = ctx->md_size  >> 2;
+	dma_map_single(&ss_dev->pdev->dev,
+		digest, SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	phy_addr = virt_to_phys(digest);
+	SS_DBG("digest addr, vir = 0x%p, phy = 0x%p\n", digest, (void *)phy_addr);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(digest),
+		SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->md),
+		ctx->md_size, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(&ctx->cnt), 4,
+			DMA_MEM_TO_DEV);
+		ctx->cnt >>= 3;
+	}
+#endif
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+	ss_print_hex(digest, SHA512_DIGEST_SIZE, digest);
+
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		kfree(digest);
+		return -EINVAL;
+	}
+
+	/* Backup the MD to ctx->md. */
+	memcpy(ctx->md, digest, ctx->md_size);
+
+	if (last == 0)
+		ctx->cnt += len;
+	kfree(digest);
+	return 0;
+}
+
+
+void ss_load_iv(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx,
+	char *buf, int size)
+{
+	if (buf == NULL)
+		return;
+
+	/* Only AES/DES/3DES-ECB don't need IV. */
+	if (CE_METHOD_IS_AES(req_ctx->type) &&
+		(req_ctx->mode == SS_AES_MODE_ECB))
+		return;
+
+	/* CBC/CTS need update the IV eachtime. */
+	if ((ctx->cnt == 0)
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CBC))
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS))) {
+		SS_DBG("IV address = 0x%p, size = %d\n", buf, size);
+		ctx->iv_size = size;
+		memcpy(ctx->iv, buf, ctx->iv_size);
+	}
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, ctx->iv_size, ctx->iv);
+}
+
+int ss_aes_one_req(sunxi_ss_t *sss, struct ablkcipher_request *req)
+{
+	int ret = 0;
+	struct crypto_ablkcipher *tfm = NULL;
+	ss_aes_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = 0x%p, dst = 0x%p\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_ablkcipher_reqtfm(req);
+	req_ctx = ablkcipher_request_ctx(req);
+	ctx = crypto_ablkcipher_ctx(tfm);
+
+	ss_load_iv(ctx, req_ctx, req->info, crypto_ablkcipher_ivsize(tfm));
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+	ss_rsa_preprocess(ctx, req_ctx, req->nbytes);
+#endif
+#ifdef CE_USE_WALK
+	if (CE_METHOD_IS_AES(req_ctx->type) ||
+		CE_METHOD_IS_DES(req_ctx->type)) {
+		ret = ss_aes_start_by_walk(req);
+	} else {
+		ret = ss_aes_start(ctx, req_ctx, req->nbytes);
+	}
+#else
+	ret = ss_aes_start(ctx, req_ctx, req->nbytes);
+#endif
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR) {
+		SS_DBG("CNT: %08x %08x %08x %08x\n",
+			*(int *)&ctx->iv[0], *(int *)&ctx->iv[4],
+			*(int *)&ctx->iv[8], *(int *)&ctx->iv[12]);
+	}
+#endif
+
+	ctx->cnt += req->nbytes;
+	return ret;
+}
+
+irqreturn_t sunxi_ss_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ss_t *sss = (sunxi_ss_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&sss->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING<<i)) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&sss->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&sss->lock, flags);
+	return IRQ_HANDLED;
+}
diff --git a/drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.c b/drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.c
new file mode 100644
index 000000000..b04f4cbcc
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.c
@@ -0,0 +1,420 @@
+/*
+ * The interface function of controlling the SS register.
+ *
+ * Copyright (C) 2013 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+
+#include "../sunxi_ce.h"
+#include "sunxi_ce_reg.h"
+
+inline u32 ss_readl(u32 offset)
+{
+	return readl(ss_membase() + offset);
+}
+
+inline void ss_writel(u32 offset, u32 val)
+{
+	writel(val, ss_membase() + offset);
+}
+
+u32 ss_reg_rd(u32 offset)
+{
+	return ss_readl(offset);
+}
+
+void ss_reg_wr(u32 offset, u32 val)
+{
+	ss_writel(offset, val);
+}
+
+void ss_keyselect_set(int select, ce_task_desc_t *task)
+{
+	task->sym_ctl |= select << CE_SYM_CTL_KEY_SELECT_SHIFT;
+}
+
+void ss_keysize_set(int size, ce_task_desc_t *task)
+{
+	int type = CE_AES_KEY_SIZE_128;
+
+	switch (size) {
+	case AES_KEYSIZE_192:
+		type = CE_AES_KEY_SIZE_192;
+		break;
+	case AES_KEYSIZE_256:
+		type = CE_AES_KEY_SIZE_256;
+		break;
+	default:
+		break;
+	}
+
+	task->sym_ctl |= (type << CE_SYM_CTL_KEY_SIZE_SHIFT);
+}
+
+/* key: phsical address. */
+void ss_key_set(char *key, int size, ce_task_desc_t *task)
+{
+	int i = 0;
+	int key_sel = CE_KEY_SELECT_INPUT;
+	struct {
+		int type;
+		char desc[AES_MIN_KEY_SIZE];
+	} keys[] = {
+		{CE_KEY_SELECT_SSK,		   CE_KS_SSK},
+		{CE_KEY_SELECT_HUK,		   CE_KS_HUK},
+		{CE_KEY_SELECT_RSSK,	   CE_KS_RSSK},
+#ifdef SS_SUPPORT_CE_V3_2
+		{CE_KEY_SELECT_SCK0,	   CE_KS_SCK0},
+		{CE_KEY_SELECT_SCK1,	   CE_KS_SCK1},
+#endif
+		{CE_KEY_SELECT_INTERNAL_0, CE_KS_INTERNAL_0},
+		{CE_KEY_SELECT_INTERNAL_1, CE_KS_INTERNAL_1},
+		{CE_KEY_SELECT_INTERNAL_2, CE_KS_INTERNAL_2},
+		{CE_KEY_SELECT_INTERNAL_3, CE_KS_INTERNAL_3},
+		{CE_KEY_SELECT_INTERNAL_4, CE_KS_INTERNAL_4},
+		{CE_KEY_SELECT_INTERNAL_5, CE_KS_INTERNAL_5},
+		{CE_KEY_SELECT_INTERNAL_6, CE_KS_INTERNAL_6},
+		{CE_KEY_SELECT_INTERNAL_7, CE_KS_INTERNAL_7},
+		{CE_KEY_SELECT_INPUT, ""} };
+
+	while (keys[i].type != CE_KEY_SELECT_INPUT) {
+		if (strncasecmp(key, keys[i].desc, AES_MIN_KEY_SIZE) == 0) {
+			key_sel = keys[i].type;
+			memset(key, 0, size);
+			break;
+		}
+		i++;
+	}
+	SS_DBG("The key select: %d\n", key_sel);
+
+	ss_keyselect_set(key_sel, task);
+	ss_keysize_set(size, task);
+	task->key_addr = (virt_to_phys(key) >> WORD_ALGIN) & 0xffffffff;
+	SS_DBG("task->key_addr: 0x%x\n", task->key_addr);
+}
+
+void ss_pending_clear(int flow)
+{
+	int val = CE_CHAN_PENDING << flow;
+
+	ss_writel(CE_REG_ISR, val);
+}
+
+int ss_pending_get(void)
+{
+	return ss_readl(CE_REG_ISR);
+}
+
+void ss_irq_enable(int flow)
+{
+	int val = ss_readl(CE_REG_ICR);
+
+	val |= CE_CHAN_INT_ENABLE << flow;
+	ss_writel(CE_REG_ICR, val);
+}
+
+void ss_irq_disable(int flow)
+{
+	int val = ss_readl(CE_REG_ICR);
+
+	val &= ~(CE_CHAN_INT_ENABLE << flow);
+	ss_writel(CE_REG_ICR, val);
+}
+
+void ss_md_get(char *dst, char *src, int size)
+{
+	memcpy(dst, src, size);
+}
+
+/* iv: phsical address. */
+void ss_iv_set(char *iv, int size, ce_task_desc_t *task)
+{
+	task->iv_addr = (virt_to_phys(iv) >> WORD_ALGIN) & 0xffffffff;
+}
+
+void ss_iv_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->comm_ctl |= mode << CE_COMM_CTL_IV_MODE_SHIFT;
+}
+
+void ss_cntsize_set(int size, ce_task_desc_t *task)
+{
+	task->sym_ctl |= size << CE_SYM_CTL_CTR_SIZE_SHIFT;
+}
+
+void ss_cnt_set(char *cnt, int size, ce_task_desc_t *task)
+{
+	task->ctr_addr = (virt_to_phys(cnt) >> WORD_ALGIN) & 0xffffffff;
+
+	ss_cntsize_set(CE_CTR_SIZE_128, task);
+}
+
+void ss_cts_last(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_CTS_LAST;
+}
+
+#ifndef SS_SUPPORT_CE_V3_1
+
+void ss_xts_first(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_XTS_FIRST;
+}
+
+void ss_xts_last(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_XTS_LAST;
+}
+
+#endif
+
+void ss_hmac_sha1_last(ce_task_desc_t *task)
+{
+	task->comm_ctl |= CE_HMAC_SHA1_LAST;
+}
+
+void ss_method_set(int dir, int type, ce_task_desc_t *task)
+{
+	task->comm_ctl |= dir << CE_COMM_CTL_OP_DIR_SHIFT;
+	task->comm_ctl |= type << CE_COMM_CTL_METHOD_SHIFT;
+}
+
+void ss_aes_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->sym_ctl |= mode << CE_SYM_CTL_OP_MODE_SHIFT;
+}
+
+void ss_cfb_bitwidth_set(int bitwidth, ce_task_desc_t *task)
+{
+	int val = 0;
+
+	switch (bitwidth) {
+	case 1:
+		val = CE_CFB_WIDTH_1;
+		break;
+	case 8:
+		val = CE_CFB_WIDTH_8;
+		break;
+	case 64:
+		val = CE_CFB_WIDTH_64;
+		break;
+	case 128:
+		val = CE_CFB_WIDTH_128;
+		break;
+	default:
+		break;
+	}
+	task->sym_ctl |= val << CE_SYM_CTL_CFB_WIDTH_SHIFT;
+}
+
+void ss_sha_final(void)
+{
+	/* unsupported. */
+}
+
+void ss_check_sha_end(void)
+{
+	/* unsupported. */
+}
+
+void ss_rsa_width_set(int size, ce_task_desc_t *task)
+{
+#ifdef SS_SUPPORT_CE_V3_1
+	int width_type = CE_RSA_PUB_MODULUS_WIDTH_512;
+
+	switch (size*8) {
+	case 512:
+		width_type = CE_RSA_PUB_MODULUS_WIDTH_512;
+		break;
+	case 1024:
+		width_type = CE_RSA_PUB_MODULUS_WIDTH_1024;
+		break;
+	case 2048:
+		width_type = CE_RSA_PUB_MODULUS_WIDTH_2048;
+		break;
+	case 3072:
+		width_type = CE_RSA_PUB_MODULUS_WIDTH_3072;
+		break;
+	case 4096:
+		width_type = CE_RSA_PUB_MODULUS_WIDTH_4096;
+		break;
+	default:
+		break;
+	}
+
+	task->asym_ctl |= width_type<<CE_ASYM_CTL_RSA_PM_WIDTH_SHIFT;
+#else
+	task->asym_ctl |= DIV_ROUND_UP(size, 4);
+#endif
+}
+
+void ss_rsa_op_mode_set(int mode, ce_task_desc_t *task)
+{
+	/* Consider !2 as M_EXP, for compatible with the previous SOC. */
+	if (mode == CE_RSA_OP_M_MUL)
+		task->asym_ctl |= CE_RSA_OP_M_MUL<<CE_ASYM_CTL_RSA_OP_SHIFT;
+	else
+		task->asym_ctl |= CE_RSA_OP_M_EXP<<CE_ASYM_CTL_RSA_OP_SHIFT;
+}
+
+void ss_ecc_width_set(int size, ce_task_desc_t *task)
+{
+#ifdef SS_SUPPORT_CE_V3_1
+	int width_type = CE_ECC_PARA_WIDTH_160;
+
+	switch (size*8) {
+	case 224:
+		width_type = CE_ECC_PARA_WIDTH_224;
+		break;
+	case 256:
+		width_type = CE_ECC_PARA_WIDTH_256;
+		break;
+	case 544: /* align with word */
+		width_type = CE_ECC_PARA_WIDTH_521;
+		break;
+	default:
+		break;
+	}
+
+	task->asym_ctl |= width_type<<CE_ASYM_CTL_ECC_PARA_WIDTH_SHIFT;
+#else
+	task->asym_ctl |= DIV_ROUND_UP(size, 4);
+#endif
+}
+
+void ss_ecc_op_mode_set(int mode, ce_task_desc_t *task)
+{
+#ifdef SS_SUPPORT_CE_V3_1
+	task->asym_ctl |= mode<<CE_ASYM_CTL_ECC_OP_SHIFT;
+#else
+	task->asym_ctl |= mode<<CE_ASYM_CTL_RSA_OP_SHIFT;
+#endif
+}
+
+u32 ss_get_phys_addr(ce_task_desc_t *task)
+{
+	phys_addr_t phy_addr = 0;
+	u32 addr = 0;
+
+	if (task->task_phy_addr) {
+		phy_addr = task->task_phy_addr;
+	} else {
+		phy_addr = virt_to_phys(task);
+	}
+	addr = (phy_addr >> WORD_ALGIN) & 0xffffffff;
+	return addr;
+}
+
+void ss_ctrl_start(ce_task_desc_t *task)
+{
+	u32 addr = 0;
+#ifndef SS_SUPPORT_CE_V3_1
+	u32 method = task->comm_ctl&CE_COMM_CTL_METHOD_MASK;
+#endif
+	addr = ss_get_phys_addr(task);
+	ss_writel(CE_REG_TSK, addr);
+#ifdef SS_SUPPORT_CE_V3_1
+	ss_writel(CE_REG_TLR, 0x1);
+#else
+	ss_writel(CE_REG_TLR, 0x1|(method<<CE_REG_TLR_METHOD_TYPE_SHIFT));
+#endif
+}
+
+void ss_ctrl_stop(void)
+{
+	/* unsupported */
+}
+
+int ss_flow_err(int flow)
+{
+	return ss_readl(CE_REG_ERR) & CE_REG_ESR_CHAN_MASK(flow);
+}
+
+void ss_wait_idle(void)
+{
+#ifdef SS_SUPPORT_CE_V3_1
+	while ((ss_readl(CE_REG_TSR) & CE_REG_TSR_BUSY_MASK) ==
+		CE_REG_TSR_BUSY) {
+		SS_DBG("Need wait for the hardware.\n");
+		msleep(20);
+	}
+#else
+	while ((ss_readl(CE_REG_TSR) & 0xff) != 0x0) {
+		SS_DBG("Need wait for the hardware.\n");
+		msleep(20);
+	}
+#endif
+}
+
+void ss_data_len_set(int len, ce_task_desc_t *task)
+{
+	task->data_len = len;
+}
+
+int ss_reg_print(char *buf, int len)
+{
+	return snprintf(buf, len,
+		"The SS control register:\n"
+		"[TSK] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[CTL] 0x%02x = 0x%08x\n"
+#endif
+		"[ICR] 0x%02x = 0x%08x, [ISR] 0x%02x = 0x%08x\n"
+		"[TLR] 0x%02x = 0x%08x\n"
+		"[TSR] 0x%02x = 0x%08x\n"
+		"[ERR] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[CSS] 0x%02x = 0x%08x, [CDS] 0x%02x = 0x%08x\n"
+#endif
+		"[CSA] 0x%02x = 0x%08x, [CDA] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[TPR] 0x%02x = 0x%08x\n"
+#else
+		"[HCSA] 0x%02x = 0x%08x\n"
+		"[HCDA] 0x%02x = 0x%08x\n"
+		"[ACSA] 0x%02x = 0x%08x\n"
+		"[ACDA] 0x%02x = 0x%08x\n"
+		"[XCSA] 0x%02x = 0x%08x\n"
+		"[XCDA] 0x%02x = 0x%08x\n"
+		"[VER] 0x%02x = 0x%08x\n"
+#endif
+		,
+		CE_REG_TSK, ss_readl(CE_REG_TSK),
+#ifdef SS_SUPPORT_CE_V3_1
+		CE_REG_CTL, ss_readl(CE_REG_CTL),
+#endif
+		CE_REG_ICR, ss_readl(CE_REG_ICR),
+		CE_REG_ISR, ss_readl(CE_REG_ISR),
+		CE_REG_TLR, ss_readl(CE_REG_TLR),
+		CE_REG_TSR, ss_readl(CE_REG_TSR),
+		CE_REG_ERR, ss_readl(CE_REG_ERR),
+#ifdef SS_SUPPORT_CE_V3_1
+		CE_REG_CSS, ss_readl(CE_REG_CSS),
+		CE_REG_CDS, ss_readl(CE_REG_CDS),
+#endif
+		CE_REG_CSA, ss_readl(CE_REG_CSA),
+		CE_REG_CDA, ss_readl(CE_REG_CDA)
+#ifdef SS_SUPPORT_CE_V3_1
+		,
+		CE_REG_TPR, ss_readl(CE_REG_TPR)
+#else
+		,
+		CE_REG_HCSA, ss_readl(CE_REG_HCSA),
+		CE_REG_HCDA, ss_readl(CE_REG_HCDA),
+		CE_REG_ACSA, ss_readl(CE_REG_ACSA),
+		CE_REG_ACDA, ss_readl(CE_REG_ACDA),
+		CE_REG_XCSA, ss_readl(CE_REG_XCSA),
+		CE_REG_XCDA, ss_readl(CE_REG_XCDA),
+		CE_REG_VER, ss_readl(CE_REG_VER)
+#endif
+		);
+}
diff --git a/drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.h b/drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.h
new file mode 100644
index 000000000..58235047a
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v3/sunxi_ce_reg.h
@@ -0,0 +1,324 @@
+/*
+ * The register macro of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2014 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_SECURITY_SYSTEM_REG_H_
+#define _SUNXI_SECURITY_SYSTEM_REG_H_
+
+/* CE: Crypto Engine, start using CE from sun8iw7/sun8iw9 */
+#define CE_REG_TSK			0x00
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_CTL			0x04
+#endif
+#define CE_REG_ICR			0x08
+#define CE_REG_ISR			0x0C
+#define CE_REG_TLR			0x10
+#define CE_REG_TSR			0x14
+#define CE_REG_ERR			0x18
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_CSS			0x1C
+#define CE_REG_CDS			0x20
+#endif
+#define CE_REG_CSA			0x24
+#define CE_REG_CDA			0x28
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_TPR			0x2C
+#endif
+#ifdef SS_SUPPORT_CE_V3_2
+#define CE_REG_HCSA			0x34
+#define CE_REG_HCDA			0x38
+#define CE_REG_ACSA			0x44
+#define CE_REG_ACDA			0x48
+#define CE_REG_XCSA			0x54
+#define CE_REG_XCDA			0x58
+#define CE_REG_VER			0x90
+#endif
+
+#define CE_CHAN_INT_ENABLE		1
+
+#define CE_CHAN_PENDING			1
+
+#ifndef SS_SUPPORT_CE_V3_1
+#define CE_REG_TLR_METHOD_TYPE_SHIFT		8
+#endif
+
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_TSR_BUSY			1
+#define CE_REG_TSR_IDLE			0
+#define CE_REG_TSR_BUSY_SHIFT	0
+#define CE_REG_TSR_BUSY_MASK	(0x1 << CE_REG_TSR_BUSY_SHIFT)
+#endif
+
+#define CE_REG_ESR_ERR_UNSUPPORT	0
+#define CE_REG_ESR_ERR_LEN			1
+#define CE_REG_ESR_ERR_KEYSRAM		2
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_ESR_ERR_ADDR			3
+#else
+#define CE_REG_ESR_ERR_ADDR			5
+#endif
+
+#ifdef SS_SUPPORT_CE_V3_2
+#define CE_REG_ESR_ERR_KEYLADDER		6
+#endif
+
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_ESR_CHAN_SHIFT		4
+#define CE_REG_ESR_CHAN_MASK(flow)	(0xF << (CE_REG_ESR_CHAN_SHIFT*flow))
+#else
+#define CE_REG_ESR_CHAN_SHIFT		8
+#define CE_REG_ESR_CHAN_MASK(flow)	(0xFF << (CE_REG_ESR_CHAN_SHIFT*flow))
+#endif
+
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_REG_CSS_OFFSET_SHIFT		16
+#define CE_REG_CDS_OFFSET_SHIFT		16
+#endif
+
+/* About the common control word */
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_TASK_INT_ENABLE			1
+#endif
+#define CE_COMM_CTL_TASK_INT_SHIFT	31
+#define CE_COMM_CTL_TASK_INT_MASK	(0x1 << CE_COMM_CTL_TASK_INT_SHIFT)
+
+#define CE_CBC_MAC_LEN_SHIFT		17
+
+#define CE_HASH_IV_DEFAULT			0
+#define CE_HASH_IV_INPUT			1
+#define CE_COMM_CTL_IV_MODE_SHIFT	16
+
+#define CE_HMAC_SHA1_LAST			BIT(15)
+
+#define SS_DIR_ENCRYPT				0
+#define SS_DIR_DECRYPT				1
+#define CE_COMM_CTL_OP_DIR_SHIFT	8
+
+#define SS_METHOD_AES				0
+#define SS_METHOD_DES				1
+#define SS_METHOD_3DES				2
+#define SS_METHOD_MD5				16
+#define SS_METHOD_SHA1				17
+#define SS_METHOD_SHA224			18
+#define SS_METHOD_SHA256			19
+#define SS_METHOD_SHA384			20
+#define SS_METHOD_SHA512			21
+#define SS_METHOD_HMAC_SHA1			22
+#define SS_METHOD_HMAC_SHA256		23
+#define SS_METHOD_RSA				32
+#define SS_METHOD_DH				SS_METHOD_RSA
+#ifdef SS_SUPPORT_CE_V3_1
+#define SS_METHOD_TRNG				48
+#define SS_METHOD_PRNG				49
+#define SS_METHOD_ECC				64
+#else
+#define SS_METHOD_TRNG				28
+#define SS_METHOD_PRNG				29
+#define SS_METHOD_ECC				33
+#define SS_METHOD_RAES				48
+#endif
+#define CE_COMM_CTL_METHOD_SHIFT		0
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_COMM_CTL_METHOD_MASK		0x3F
+#else
+#define CE_COMM_CTL_METHOD_MASK		0x7F
+#endif
+
+#define CE_METHOD_IS_HASH(type) ((type == SS_METHOD_MD5) \
+				|| (type == SS_METHOD_SHA1) \
+				|| (type == SS_METHOD_SHA224) \
+				|| (type == SS_METHOD_SHA256) \
+				|| (type == SS_METHOD_SHA384) \
+				|| (type == SS_METHOD_SHA512))
+
+#define CE_METHOD_IS_AES(type) ((type == SS_METHOD_AES) \
+				|| (type == SS_METHOD_DES) \
+				|| (type == SS_METHOD_3DES))
+
+#define CE_METHOD_IS_HMAC(type) ((type == SS_METHOD_HMAC_SHA1) \
+			|| (type == SS_METHOD_HMAC_SHA256))
+
+/* About the symmetric control word */
+
+#define CE_KEY_SELECT_INPUT			0
+#define CE_KEY_SELECT_SSK			1
+#define CE_KEY_SELECT_HUK			2
+#define CE_KEY_SELECT_RSSK			3
+#ifdef SS_SUPPORT_CE_V3_2
+#define CE_KEY_SELECT_SCK0			4
+#define CE_KEY_SELECT_SCK1			5
+#endif
+#define CE_KEY_SELECT_INTERNAL_0	8
+#define CE_KEY_SELECT_INTERNAL_1	9
+#define CE_KEY_SELECT_INTERNAL_2	10
+#define CE_KEY_SELECT_INTERNAL_3	11
+#define CE_KEY_SELECT_INTERNAL_4	12
+#define CE_KEY_SELECT_INTERNAL_5	13
+#define CE_KEY_SELECT_INTERNAL_6	14
+#define CE_KEY_SELECT_INTERNAL_7	15
+#define CE_SYM_CTL_KEY_SELECT_SHIFT	20
+
+/* The identification string to indicate the key source. */
+#define CE_KS_SSK			"KEY_SEL_SSK"
+#define CE_KS_HUK			"KEY_SEL_HUK"
+#define CE_KS_RSSK			"KEY_SEL_RSSK"
+#ifdef SS_SUPPORT_CE_V3_2
+#define CE_KS_SCK0			"KEY_SEL_SCK0"
+#define CE_KS_SCK1			"KEY_SEL_SCK1"
+#endif
+#define CE_KS_INTERNAL_0	"KEY_SEL_INTRA_0"
+#define CE_KS_INTERNAL_1	"KEY_SEL_INTRA_1"
+#define CE_KS_INTERNAL_2	"KEY_SEL_INTRA_2"
+#define CE_KS_INTERNAL_3	"KEY_SEL_INTRA_3"
+#define CE_KS_INTERNAL_4	"KEY_SEL_INTRA_4"
+#define CE_KS_INTERNAL_5	"KEY_SEL_INTRA_5"
+#define CE_KS_INTERNAL_6	"KEY_SEL_INTRA_6"
+#define CE_KS_INTERNAL_7	"KEY_SEL_INTRA_7"
+
+#define CE_CFB_WIDTH_1				0
+#define CE_CFB_WIDTH_8				1
+#define CE_CFB_WIDTH_64				2
+#define CE_CFB_WIDTH_128			3
+#define CE_SYM_CTL_CFB_WIDTH_SHIFT	18
+
+#define CE_SYM_CTL_AES_CTS_LAST		BIT(16)
+
+#ifndef SS_SUPPORT_CE_V3_1
+#define CE_SYM_CTL_AES_XTS_LAST		BIT(13)
+#define CE_SYM_CTL_AES_XTS_FIRST	BIT(12)
+#endif
+
+#define SS_AES_MODE_ECB				0
+#define SS_AES_MODE_CBC				1
+#define SS_AES_MODE_CTR				2
+#define SS_AES_MODE_CTS				3
+#define SS_AES_MODE_OFB				4
+#define SS_AES_MODE_CFB				5
+#define SS_AES_MODE_CBC_MAC			6
+#define SS_AES_MODE_XTS				9
+#define CE_SYM_CTL_OP_MODE_SHIFT	8
+
+#define CE_CTR_SIZE_16				0
+#define CE_CTR_SIZE_32				1
+#define CE_CTR_SIZE_64				2
+#define CE_CTR_SIZE_128				3
+#define CE_SYM_CTL_CTR_SIZE_SHIFT	2
+
+#define CE_AES_KEY_SIZE_128			0
+#define CE_AES_KEY_SIZE_192			1
+#define CE_AES_KEY_SIZE_256			2
+#define CE_SYM_CTL_KEY_SIZE_SHIFT	0
+
+#define CE_IS_AES_MODE(type, mode, M) (CE_METHOD_IS_AES(type) \
+					&& (mode == SS_AES_MODE_##M))
+
+/* About the asymmetric control word */
+
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_RSA_PUB_MODULUS_WIDTH_512	0
+#define CE_RSA_PUB_MODULUS_WIDTH_1024	1
+#define CE_RSA_PUB_MODULUS_WIDTH_2048	2
+#define CE_RSA_PUB_MODULUS_WIDTH_3072	3
+#define CE_RSA_PUB_MODULUS_WIDTH_4096	4
+#define CE_ASYM_CTL_RSA_PM_WIDTH_SHIFT	28
+#endif
+
+#define CE_RSA_OP_M_EXP		0 /* modular exponentiation */
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_RSA_OP_M_MUL		2 /* modular multiplication */
+#else
+#define CE_RSA_OP_M_ADD                 1 /* modular add */
+#define CE_RSA_OP_M_MINUS               2 /* modular minus */
+#define CE_RSA_OP_M_MUL                 3 /* modular multiplication */
+#endif
+#define CE_ASYM_CTL_RSA_OP_SHIFT		16
+
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_ECC_PARA_WIDTH_160			0
+#define CE_ECC_PARA_WIDTH_224			2
+#define CE_ECC_PARA_WIDTH_256			3
+#define CE_ECC_PARA_WIDTH_521			5
+#define CE_ASYM_CTL_ECC_PARA_WIDTH_SHIFT	12
+#endif
+
+#ifdef SS_SUPPORT_CE_V3_1
+#define CE_ECC_OP_POINT_MUL				0
+#define CE_ECC_OP_POINT_ADD				1
+#define CE_ECC_OP_POINT_DBL				2
+#define CE_ECC_OP_POINT_VER				3
+#define CE_ECC_OP_ENC					4
+#define CE_ECC_OP_DEC					5
+#define CE_ECC_OP_SIGN					6
+#define CE_ASYM_CTL_ECC_OP_SHIFT		4
+#else
+#define CE_ECC_OP_POINT_ADD             0 /* point add */
+#define CE_ECC_OP_POINT_DBL             1 /* point double */
+#define CE_ECC_OP_POINT_MUL             2 /* point multiplication */
+#define CE_ECC_OP_POINT_VER             3 /* point verification */
+#define CE_ECC_OP_ENC                   4 /* encryption */
+#define CE_ECC_OP_DEC                   5 /* decryption */
+#define CE_ECC_OP_SIGN                  6 /* sign */
+#define CE_ECC_OP_VERIFY                7 /* verification */
+#endif
+
+#define SS_SEED_SIZE			24
+
+/* Function declaration */
+
+u32 ss_reg_rd(u32 offset);
+void ss_reg_wr(u32 offset, u32 val);
+
+void ss_key_set(char *key, int size, ce_task_desc_t *task);
+
+int ss_pending_get(void);
+void ss_pending_clear(int flow);
+void ss_irq_enable(int flow);
+void ss_irq_disable(int flow);
+
+void ss_iv_set(char *iv, int size, ce_task_desc_t *task);
+void ss_iv_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_cnt_set(char *cnt, int size, ce_task_desc_t *task);
+void ss_cnt_get(int flow, char *cnt, int size);
+
+void ss_md_get(char *dst, char *src, int size);
+void ss_sha_final(void);
+void ss_check_sha_end(void);
+
+void ss_rsa_width_set(int size, ce_task_desc_t *task);
+void ss_rsa_op_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_ecc_width_set(int size, ce_task_desc_t *task);
+void ss_ecc_op_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_cts_last(ce_task_desc_t *task);
+void ss_hmac_sha1_last(ce_task_desc_t *task);
+
+void ss_xts_first(ce_task_desc_t *task);
+void ss_xts_last(ce_task_desc_t *task);
+
+void ss_method_set(int dir, int type, ce_task_desc_t *task);
+
+void ss_aes_mode_set(int mode, ce_task_desc_t *task);
+void ss_cfb_bitwidth_set(int bitwidth, ce_task_desc_t *task);
+
+void ss_wait_idle(void);
+void ss_ctrl_start(ce_task_desc_t *task);
+void ss_ctrl_stop(void);
+int ss_flow_err(int flow);
+
+void ss_data_len_set(int len, ce_task_desc_t *task);
+
+int ss_reg_print(char *buf, int len);
+void ss_keyselect_set(int select, ce_task_desc_t *task);
+void ss_keysize_set(int size, ce_task_desc_t *task);
+
+#endif /* end of _SUNXI_SECURITY_SYSTEM_REG_H_ */
diff --git a/drivers/crypto/sunxi-ce/v4/sunxi_ce_cdev_comm.c b/drivers/crypto/sunxi-ce/v4/sunxi_ce_cdev_comm.c
new file mode 100644
index 000000000..edc9f8d86
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v4/sunxi_ce_cdev_comm.c
@@ -0,0 +1,447 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2014 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/internal/aead.h>
+#include <crypto/hash.h>
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+
+#include "../sunxi_ce_cdev.h"
+#include "sunxi_ce_reg.h"
+
+#define NO_DMA_MAP		(0xE7)
+#define SRC_DATA_DIR	(0)
+#define DST_DATA_DIR	(0x1)
+
+extern sunxi_ce_cdev_t	*ce_cdev;
+
+irqreturn_t sunxi_ce_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ce_cdev_t *p_cdev = (sunxi_ce_cdev_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&p_cdev->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING << (2 * i))) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&p_cdev->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&p_cdev->lock, flags);
+	return IRQ_HANDLED;
+}
+
+static int check_aes_ctx_vaild(crypto_aes_req_ctx_t *req)
+{
+	if (!req->src_buffer || !req->dst_buffer || !req->key_buffer) {
+		SS_ERR("Invalid para: src = 0x%px, dst = 0x%px key = 0x%p\n",
+				req->src_buffer, req->dst_buffer, req->key_buffer);
+		return -EINVAL;
+	}
+
+	if (req->iv_length) {
+		if (!req->iv_buf) {
+			SS_ERR("Invalid para: iv_buf = 0x%px\n", req->iv_buf);
+			return -EINVAL;
+		}
+	}
+
+	SS_DBG("key_length = %d\n", req->key_length);
+	if (req->key_length > AES_MAX_KEY_SIZE) {
+		SS_ERR("Invalid para: key_length = %d\n", req->key_length);
+		return -EINVAL;
+	} else if (req->key_length < AES_MIN_KEY_SIZE) {
+		SS_ERR("Invalid para: key_length = %d\n", req->key_length);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void ce_aes_config(crypto_aes_req_ctx_t *req, ce_task_desc_t *task)
+{
+	task->chan_id = req->channel_id;
+	ss_method_set(req->dir, SS_METHOD_AES, task);
+	ss_aes_mode_set(req->aes_mode, task);
+}
+
+static void task_iv_init(crypto_aes_req_ctx_t *req, ce_task_desc_t *task, int flag)
+{
+	if (req->iv_length) {
+		if (flag == DMA_MEM_TO_DEV) {
+			ss_iv_set(req->iv_buf, req->iv_length, task);
+			req->iv_phy = dma_map_single(ce_cdev->pdevice, req->iv_buf,
+									req->iv_length, DMA_MEM_TO_DEV);
+			SS_DBG("iv = %px, iv_phy_addr = 0x%lx\n", req->iv_buf, req->iv_phy);
+		} else if (flag == DMA_DEV_TO_MEM) {
+			dma_unmap_single(ce_cdev->pdevice,
+				req->iv_phy, req->iv_length, DMA_DEV_TO_MEM);
+		} else if (flag == NO_DMA_MAP) {
+			task->iv_addr = (req->iv_phy >> WORD_ALGIN);
+			SS_DBG("iv_phy_addr = 0x%lx\n", req->iv_phy);
+		}
+	}
+	return;
+}
+
+static ce_task_desc_t *ce_alloc_task(void)
+{
+	dma_addr_t task_phy_addr;
+	ce_task_desc_t *task;
+
+	task = dma_pool_zalloc(ce_cdev->task_pool, GFP_KERNEL, &task_phy_addr);
+	if (task == NULL) {
+		SS_ERR("Failed to alloc for task\n");
+		return NULL;
+	} else {
+		task->next_virt = NULL;
+		task->task_phy_addr = task_phy_addr;
+		SS_DBG("task = 0x%px task_phy = 0x%px\n", task, (void *)task_phy_addr);
+	}
+
+	return task;
+}
+
+static void ce_task_destroy(ce_task_desc_t *task)
+{
+	ce_task_desc_t *prev;
+
+	while (task != NULL) {
+		prev = task;
+		task = task->next_virt;
+		SS_DBG("prev = 0x%px, prev_phy = 0x%px\n", prev, (void *)prev->task_phy_addr);
+		dma_pool_free(ce_cdev->task_pool, prev, prev->task_phy_addr);
+	}
+	return;
+}
+
+static int ce_task_data_init(crypto_aes_req_ctx_t *req, phys_addr_t src_phy,
+						phys_addr_t dst_phy, u32 length, ce_task_desc_t *task)
+{
+	u32 block_size = 127 * 1024;
+	u32 block_size_word = (block_size >> 2);
+	u32 block_num, alloc_flag = 0;
+	u32 last_data_len, last_size;
+	u32 data_len_offset = 0;
+	u32 i = 0, n;
+	dma_addr_t ptask_phy;
+	dma_addr_t next_iv_phy;
+	ce_task_desc_t *ptask = task, *prev;
+
+	block_num = length / block_size;
+	last_size = length % block_size;
+	ptask->data_len = 0;
+	SS_DBG("total_len = 0x%x block_num =%d last_size =%d\n", length, block_num, last_size);
+	while (length) {
+
+		if (alloc_flag) {
+			ptask = dma_pool_zalloc(ce_cdev->task_pool, GFP_KERNEL, &ptask_phy);
+			if (ptask == NULL) {
+				SS_ERR("Failed to alloc for ptask\n");
+				return -ENOMEM;
+			}
+			ptask->chan_id  = prev->chan_id;
+			ptask->comm_ctl = prev->comm_ctl;
+			ptask->sym_ctl  = prev->sym_ctl;
+			ptask->asym_ctl = prev->asym_ctl;
+			ptask->key_addr = (prev->key_addr >> WORD_ALGIN);
+			ptask->iv_addr = (prev->iv_addr >> WORD_ALGIN);
+			ptask->data_len = 0;
+			prev->next_task_addr = (ptask_phy >> WORD_ALGIN);
+			prev->next_virt = ptask;
+			ptask->task_phy_addr = ptask_phy;
+
+			SS_DBG("ptask = 0x%px, ptask_phy = 0x%px\n", ptask, (void *)ptask_phy);
+
+			if (SS_AES_MODE_CBC == req->aes_mode) {
+				req->iv_phy = next_iv_phy;
+				task_iv_init(req, ptask, NO_DMA_MAP);
+			}
+			i = 0;
+		}
+
+		if (block_num) {
+			n = (block_num > 8) ? CE_SCATTERS_PER_TASK : block_num;
+			for (i = 0; i < n; i++) {
+				ptask->src[i].addr = ((src_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->src[i].len = block_size_word;
+				ptask->dst[i].addr = ((dst_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->dst[i].len = block_size_word;
+				ptask->data_len += block_size;
+				data_len_offset += block_size;
+			}
+			block_num = block_num - n;
+		}
+
+		SS_DBG("block_num =%d i =%d\n", block_num, i);
+
+		/*the last no engure block size*/
+		if ((block_num == 0) && (last_size == 0)) {	/*block size aglin */
+			ptask->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+			alloc_flag = 0;
+			ptask->next_task_addr = 0;
+			break;
+		} else if ((block_num == 0) && (last_size != 0)) {
+			SS_DBG("last_size =%d data_len_offset= %d\n", last_size, data_len_offset);
+			/* not block size aglin */
+			if ((i < CE_SCATTERS_PER_TASK) && (data_len_offset < length)) {
+				last_data_len = length - data_len_offset;
+
+				ptask->src[i].addr = ((src_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->src[i].len = (last_data_len >> 2);
+				ptask->dst[i].addr = ((dst_phy + data_len_offset) >> WORD_ALGIN);
+				ptask->dst[i].len = (last_data_len >> 2);
+				ptask->data_len += last_data_len;
+				ptask->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+				ptask->next_task_addr = 0;
+				break;
+			}
+		}
+
+		if (req->dir == SS_DIR_ENCRYPT) {
+			next_iv_phy = ptask->dst[7].addr + (ptask->dst[7].len << 2) - 16;
+		} else {
+			next_iv_phy = ptask->src[7].addr + (ptask->src[7].len << 2) - 16;
+		}
+		alloc_flag = 1;
+		prev = ptask;
+
+	}
+	return 0;
+}
+
+static int aes_crypto_start(crypto_aes_req_ctx_t *req, u8 *src_buffer,
+							u32 src_length, u8 *dst_buffer)
+{
+	int ret = 0;
+	int channel_id = req->channel_id;
+	u32 padding_flag = ce_cdev->flows[req->channel_id].buf_pendding;
+	phys_addr_t key_phy = 0;
+	phys_addr_t src_phy = 0;
+	phys_addr_t dst_phy = 0;
+	ce_task_desc_t *task = NULL;
+
+	task = ce_alloc_task();
+	if (!task) {
+		return -1;
+	}
+
+	/*task_mode_set*/
+	ce_aes_config(req, task);
+
+	/*task_key_set*/
+	if (req->key_length) {
+		key_phy = dma_map_single(ce_cdev->pdevice,
+					req->key_buffer, req->key_length, DMA_MEM_TO_DEV);
+		SS_DBG("key = 0x%px, key_phy_addr = 0x%px\n", req->key_buffer, (void *)key_phy);
+		ss_key_set(req->key_buffer, req->key_length, task);
+	}
+
+	SS_DBG("ion_flag = %d padding_flag =%d", req->ion_flag, padding_flag);
+	/*task_iv_set*/
+	if (req->ion_flag && padding_flag) {
+		task_iv_init(req, task, NO_DMA_MAP);
+	} else {
+		task_iv_init(req, task, DMA_MEM_TO_DEV);
+	}
+
+	/*task_data_set*/
+	/*only the last src_buf is malloc*/
+	if (req->ion_flag && (!padding_flag)) {
+		src_phy = req->src_phy;
+	} else {
+		src_phy = dma_map_single(ce_cdev->pdevice, src_buffer, src_length, DMA_MEM_TO_DEV);
+	}
+	SS_DBG("src = 0x%px, src_phy_addr = 0x%px\n", src_buffer, (void *)src_phy);
+
+	/*the dst_buf is from user*/
+	if (req->ion_flag) {
+		dst_phy = req->dst_phy;
+	} else {
+		dst_phy = dma_map_single(ce_cdev->pdevice, dst_buffer, src_length, DMA_MEM_TO_DEV);
+	}
+	SS_DBG("dst = 0x%px, dst_phy_addr = 0x%px\n", dst_buffer, (void *)dst_phy);
+
+	ce_task_data_init(req, src_phy, dst_phy, src_length, task);
+	/*ce_print_task_info(task);*/
+
+	/*start ce*/
+	ss_pending_clear(channel_id);
+	ss_irq_enable(channel_id);
+
+	init_completion(&ce_cdev->flows[channel_id].done);
+	ce_ctrl_start(task, task->task_phy_addr);
+	/*ce_reg_print();*/
+
+
+	ret = wait_for_completion_timeout(&ce_cdev->flows[channel_id].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ce_task_destroy(task);
+		ce_reset();
+		ret = -ETIMEDOUT;
+	}
+
+	ss_irq_disable(channel_id);
+	ce_task_destroy(task);
+
+	/*key*/
+	if (req->key_length) {
+		dma_unmap_single(ce_cdev->pdevice,
+			key_phy, req->key_length, DMA_DEV_TO_MEM);
+	}
+
+	/*iv*/
+	if (req->ion_flag && padding_flag) {
+		;
+	} else {
+		task_iv_init(req, task, DMA_DEV_TO_MEM);
+	}
+
+	/*data*/
+	if (req->ion_flag && (!padding_flag)) {
+		;
+	} else {
+		dma_unmap_single(ce_cdev->pdevice,
+				src_phy, src_length, DMA_DEV_TO_MEM);
+	}
+
+	if (req->ion_flag) {
+		;
+	} else {
+		dma_unmap_single(ce_cdev->pdevice,
+				dst_phy, src_length, DMA_DEV_TO_MEM);
+	}
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+
+	if (ss_flow_err(channel_id)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(channel_id));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int do_aes_crypto(crypto_aes_req_ctx_t *req_ctx)
+{
+	u32 last_block_size = 0;
+	u32 block_num = 0;
+	u32 padding_size = 0;
+	u32 first_encypt_size = 0;
+	u8 data_block[AES_BLOCK_SIZE];
+	int channel_id = req_ctx->channel_id;
+	int ret;
+
+	ret = check_aes_ctx_vaild(req_ctx);
+	if (ret) {
+		return -1;
+	}
+
+	memset(data_block, 0x0, AES_BLOCK_SIZE);
+	ce_cdev->flows[channel_id].buf_pendding = 0;
+
+	if (req_ctx->dir == SS_DIR_DECRYPT) {
+		ret = aes_crypto_start(req_ctx, req_ctx->src_buffer,
+								req_ctx->src_length, req_ctx->dst_buffer);
+		if (ret) {
+			SS_ERR("aes decrypt fail\n");
+			return -2;
+		}
+		req_ctx->dst_length = req_ctx->src_length;
+	} else {
+		block_num = req_ctx->src_length / AES_BLOCK_SIZE;
+		last_block_size = req_ctx->src_length % AES_BLOCK_SIZE;
+		padding_size = AES_BLOCK_SIZE - last_block_size;
+
+		if (block_num > 0) {
+			SS_DBG("block_num = %d\n", block_num);
+			first_encypt_size = block_num * AES_BLOCK_SIZE;
+			SS_DBG("src_phy = 0x%lx, dst_phy = 0x%lx\n", req_ctx->src_phy, req_ctx->dst_phy);
+			ret = aes_crypto_start(req_ctx, req_ctx->src_buffer,
+						first_encypt_size,
+						req_ctx->dst_buffer
+						);
+			if (ret) {
+				SS_ERR("aes encrypt fail\n");
+				return -2;
+			}
+			req_ctx->dst_length = block_num * AES_BLOCK_SIZE;
+			/*not align 16byte*/
+			if (last_block_size) {
+				SS_DBG("last_block_size = %d\n", last_block_size);
+				SS_DBG("padding_size = %d\n", padding_size);
+				ce_cdev->flows[channel_id].buf_pendding = padding_size;
+				if (req_ctx->ion_flag) {
+					SS_ERR("ion memery must be 16 byte algin\n");
+				} else {
+					memcpy(data_block, req_ctx->src_buffer + first_encypt_size, last_block_size);
+					memset(data_block + last_block_size, padding_size, padding_size);
+				}
+				if (SS_AES_MODE_CBC == req_ctx->aes_mode) {
+					if (req_ctx->ion_flag) {
+						req_ctx->iv_phy = req_ctx->dst_phy + first_encypt_size - AES_BLOCK_SIZE;
+					} else {
+						req_ctx->iv_buf = req_ctx->dst_buffer + first_encypt_size - AES_BLOCK_SIZE;
+					}
+				}
+
+				ret = aes_crypto_start(req_ctx, data_block, AES_BLOCK_SIZE,
+										req_ctx->dst_buffer + first_encypt_size
+										);
+				if (ret) {
+					SS_ERR("aes encrypt fail\n");
+					return -2;
+				}
+				req_ctx->dst_length = req_ctx->dst_length + AES_BLOCK_SIZE;
+			}
+		} else {
+			SS_DBG("padding_size = %d\n", padding_size);
+			ce_cdev->flows[channel_id].buf_pendding = padding_size;
+			if (req_ctx->ion_flag) {
+				SS_ERR("ion memery must be 16 byte algin\n");
+			} else {
+				memcpy(data_block, req_ctx->src_buffer, req_ctx->src_length);
+				memset(data_block + last_block_size, padding_size, padding_size);
+			}
+			ret = aes_crypto_start(req_ctx, data_block, AES_BLOCK_SIZE,
+									req_ctx->dst_buffer
+									);
+			if (ret) {
+				SS_ERR("aes encrypt fail\n");
+				return -2;
+			}
+
+			req_ctx->dst_length = (block_num + 1) * AES_BLOCK_SIZE;
+		}
+	}
+	SS_ERR("do_aes_crypto sucess\n");
+	return 0;
+}
+
+
diff --git a/drivers/crypto/sunxi-ce/v4/sunxi_ce_proc.c b/drivers/crypto/sunxi-ce/v4/sunxi_ce_proc.c
new file mode 100644
index 000000000..5b60e4c7c
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v4/sunxi_ce_proc.c
@@ -0,0 +1,1490 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2018 Allwinner.
+ *
+ * <xupengliu@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <linux/highmem.h>
+#include <linux/dmaengine.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/des.h>
+
+#include <crypto/aead.h>
+#include <crypto/internal/aead.h>
+
+#include "../sunxi_ce.h"
+#include "../sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+void ce_print_new_task_desc(ce_new_task_desc_t *task)
+{
+	int i;
+
+#ifndef SUNXI_CE_DEBUG
+	return;
+#endif
+	printk("---------------------task_info--------------------\n");
+	printk("task->comm_ctl = 0x%x\n", task->common_ctl);
+	printk("task->main_cmd = 0x%x\n", task->main_cmd);
+	printk("task->data_len = 0x%x\n", task->data_len);
+	printk("task->key_addr = 0x%x\n", task->key_addr);
+	printk("task->iv_addr = 0x%x\n", task->iv_addr);
+
+	for (i = 0; i < 8; i++) {
+		if (task->src[i].addr) {
+			printk("task->src[%d].addr = 0x%x\n", i, task->src[i].addr);
+			printk("task->src[%d].len = 0x%x\n", i, task->src[i].len);
+		}
+	}
+
+	for (i = 0; i < 8; i++) {
+		if (task->dst[i].addr) {
+			printk("task->dst[%d].addr = 0x%x\n", i, task->dst[i].addr);
+			printk("task->dst[%d].len = 0x%x\n", i, task->dst[i].len);
+		}
+	}
+	printk("task->task_phy_addr = %pa\n", &task->task_phy_addr);
+}
+
+void ce_print_task_desc(ce_task_desc_t *task)
+{
+	int i = 0;
+
+#ifndef SUNXI_CE_DEBUG
+		return;
+#endif
+	printk("---------------------task_info--------------------\n");
+	printk("task->comm_ctl = 0x%x\n", task->comm_ctl);
+	printk("task->sym_ctl = 0x%x\n", task->sym_ctl);
+	printk("task->asym_ctl = 0x%x\n", task->asym_ctl);
+	printk("task->key_addr = 0x%x\n", task->key_addr);
+	printk("task->iv_addr = 0x%x\n", task->iv_addr);
+	printk("task->ctr_addr = 0x%x\n", task->ctr_addr);
+	printk("task->data_len = 0x%x\n", task->data_len);
+
+	for (i = 0; i < 8; i++) {
+		if (task->src[i].addr) {
+			printk("task->src[%d].addr = 0x%x\n", i, task->src[i].addr);
+			printk("task->src[%d].len = 0x%x\n", i, task->src[i].len);
+		}
+	}
+
+	for (i = 0; i < 8; i++) {
+		if (task->dst[i].addr) {
+			printk("task->dst[%d].addr = 0x%x\n", i, task->dst[i].addr);
+			printk("task->dst[%d].len = 0x%x\n", i, task->dst[i].len);
+		}
+	}
+	printk("task->task_phy_addr = %pa\n", &task->task_phy_addr);
+}
+
+/* ss_new_task_desc_init used for hash/rng alg */
+void ss_new_task_desc_init(ce_new_task_desc_t *task, u32 flow)
+{
+	memset(task, 0, sizeof(ce_new_task_desc_t));
+
+	task->common_ctl |= (flow << CE_CTL_CHAN_MASK);
+	task->common_ctl |= CE_CTL_IE_MASK;
+}
+
+void ss_task_desc_init(ce_task_desc_t *task, u32 flow)
+{
+	memset(task, 0, sizeof(ce_task_desc_t));
+	task->chan_id = flow;
+	task->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+}
+
+static int ss_sg_len(struct scatterlist *sg, int total)
+{
+	int nbyte = 0;
+	struct scatterlist *cur = sg;
+
+	while (cur != NULL) {
+		SS_DBG("cur: 0x%px, len: %d, is_last: %ld\n",
+			cur, cur->length, sg_is_last(cur));
+		nbyte += cur->length;
+
+		cur = sg_next(cur);
+	}
+
+	return nbyte;
+}
+
+static int ss_aes_align_size(int type, int mode)
+{
+	if ((type == SS_METHOD_ECC) || CE_METHOD_IS_HMAC(type)
+		|| (CE_IS_AES_MODE(type, mode, CTS))
+		|| (CE_IS_AES_MODE(type, mode, XTS)))
+		return 4;
+	else if ((type == SS_METHOD_DES) || (type == SS_METHOD_3DES))
+		return DES_BLOCK_SIZE;
+	else
+		return AES_BLOCK_SIZE;
+}
+
+static int ss_copy_from_user(void *to, struct scatterlist *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(from);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg 0x%px (%d).\n", from, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%px, sg_addr = 0x%px, size = %d\n", vaddr, from, size);
+	memcpy(to, vaddr + from->offset, size);
+	kunmap(ppage);
+	return 0;
+}
+
+static int ss_copy_to_user(struct scatterlist *to, void *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(to);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg: 0x%px (%d).\n", to, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%px, sg_addr = 0x%px, size = %d\n", vaddr, to, size);
+	memcpy(vaddr+to->offset, from, size);
+	kunmap(ppage);
+	return 0;
+}
+static int ss_aead_sg_config(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int type, int mode, int tail)
+{
+	int cnt = 0;
+	int last_sg_len = 0;
+	struct scatterlist *cur = info->sg;
+
+	while (cur != NULL) {
+		if (cnt >= CE_SCATTERS_PER_TASK-1) {
+			WARN(1, "Too many scatter: %d\n", cnt);
+			return -1;
+		}
+
+		scatter[cnt+1].addr = sg_dma_address(cur) >> 2;	/* address in words*/
+		scatter[cnt+1].len = sg_dma_len(cur)/4;
+		info->last_sg = cur;
+		last_sg_len = sg_dma_len(cur);
+		SS_DBG("%d cur: 0x%px, scatter: addr 0x%x, len %d (%d)\n",
+				cnt, cur, scatter[cnt+1].addr,
+				scatter[cnt+1].len, sg_dma_len(cur));
+		cnt++;
+		cur = sg_next(cur);
+	}
+
+	info->nents = cnt;
+	if (tail == 0) {
+		info->has_padding = 0;
+		return 0;
+	}
+
+	/* CTS/CTR/CFB/OFB need algin with word/block, so replace the last sg.*/
+
+	last_sg_len += ss_aes_align_size(0, mode) - tail;
+	info->padding = kzalloc(last_sg_len, GFP_KERNEL);
+	if (info->padding == NULL) {
+		SS_ERR("Failed to kmalloc(%d)!\n", last_sg_len);
+		return -ENOMEM;
+	}
+	SS_DBG("AES(%d)-%d padding: 0x%px, tail = %d/%d, cnt = %d\n",
+		type, mode, info->padding, tail, last_sg_len, cnt);
+
+	scatter[cnt].addr = virt_to_phys(info->padding) >> 2;	/* address in words*/
+	ss_copy_from_user(info->padding,
+		info->last_sg, last_sg_len - ss_aes_align_size(0, mode) + tail);
+	scatter[cnt].len = last_sg_len/4;
+
+	info->has_padding = 1;
+	return 0;
+}
+
+static int ss_sg_config(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int type, int mode, int tail, int hash_type)
+{
+	int cnt = 0;
+	int last_sg_len = 0;
+	struct scatterlist *cur = info->sg;
+
+	while (cur != NULL) {
+		if (cnt >= CE_SCATTERS_PER_TASK) {
+			WARN(1, "Too many scatter: %d\n", cnt);
+			return -1;
+		}
+
+		scatter[cnt].addr = sg_dma_address(cur) >> 2;/* address in words*/
+		scatter[cnt].len = sg_dma_len(cur)/4;
+		info->last_sg = cur;
+		last_sg_len = sg_dma_len(cur);
+		SS_DBG("%d cur: 0x%px, scatter: addr 0x%x, len %d (%d)\n",
+				cnt, cur, scatter[cnt].addr,
+				scatter[cnt].len, sg_dma_len(cur));
+		cnt++;
+		cur = sg_next(cur);
+	}
+
+#ifdef SS_HASH_HW_PADDING
+		if (CE_METHOD_IS_HMAC(type)) {
+			scatter[cnt-1].len += (tail+3)/4;
+			info->has_padding = 0;
+			return 0;
+		}
+#endif
+
+	info->nents = cnt;
+	if (tail == 0) {
+		info->has_padding = 0;
+		return 0;
+	}
+
+	if (hash_type) {
+		scatter[cnt-1].len -= tail/4;
+		return 0;
+	}
+	/* CTS/CTR/CFB/OFB need algin with word/block, so replace the last sg.*/
+
+	last_sg_len += ss_aes_align_size(0, mode) - tail;
+	info->padding = kzalloc(last_sg_len, GFP_KERNEL);
+	if (info->padding == NULL) {
+		SS_ERR("Failed to kmalloc(%d)!\n", last_sg_len);
+		return -ENOMEM;
+	}
+	SS_DBG("AES(%d)-%d padding: 0x%px, tail = %d/%d, cnt = %d\n",
+		type, mode, info->padding, tail, last_sg_len, cnt);
+
+	info->mapping[cnt - 1].virt_addr = info->padding;
+	scatter[cnt-1].addr = virt_to_phys(info->padding) >> 2;/* address in words*/
+
+	ss_copy_from_user(info->padding,
+		info->last_sg, last_sg_len - ss_aes_align_size(0, mode) + tail);
+	scatter[cnt-1].len = last_sg_len/4;
+
+	info->has_padding = 1;
+	return 0;
+}
+
+static void ss_aes_unpadding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int tail)
+{
+	int last_sg_len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	/* Only the dst sg need to be recovered. */
+	if (info->dir == DMA_DEV_TO_MEM) {
+		last_sg_len = scatter[index].len * 4;
+		last_sg_len -= ss_aes_align_size(0, mode) - tail;
+		ss_copy_to_user(info->last_sg, info->padding, last_sg_len);
+	}
+
+	kfree(info->padding);
+	info->padding = NULL;
+	info->has_padding = 0;
+}
+
+static void ss_aes_map_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%x, len: %d, dir: %d\n",
+		scatter[index].addr, len, dir);
+
+	/* task address in words, so phys-to-virt need to change*/
+	dma_map_single(&ss_dev->pdev->dev,
+			info->mapping[index].virt_addr, len, dir);
+	info->dir = dir;
+}
+
+static void ss_aead_map_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%x, len: %d, dir: %d\n",
+		scatter[index].addr, len, dir);
+
+	/* task address in words, so phys-to-virt need to change*/
+	dma_map_single(&ss_dev->pdev->dev,
+		info->mapping[index].virt_addr, len, dir);
+	info->dir = dir;
+}
+
+static void ss_aes_unmap_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%x, len: %d, dir: %d\n",
+		scatter[index].addr, len, dir);
+	dma_unmap_single(&ss_dev->pdev->dev, scatter[index].addr, len, dir);
+}
+static void ss_aead_unmap_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir)
+{
+	int len = 0;
+	int index = info->nents;
+
+	if (info->has_padding == 0)
+		return;
+
+	len = scatter[index].len * 4;
+	SS_DBG("AES padding: 0x%x, len: %d, dir: %d\n",
+		scatter[index].addr, len, dir);
+	dma_unmap_single(&ss_dev->pdev->dev, scatter[index].addr, len, dir);
+}
+
+void ss_change_clk(int type)
+{
+#ifdef SS_RSA_CLK_ENABLE
+	if ((type == SS_METHOD_RSA) || (type == SS_METHOD_ECC))
+		ss_clk_set(ss_dev->rsa_clkrate);
+	else
+		ss_clk_set(ss_dev->gen_clkrate);
+#endif
+}
+
+void ss_hash_rng_change_clk(void)
+{
+#ifdef SS_RSA_CLK_ENABLE
+		ss_clk_set(ss_dev->gen_clkrate);
+#endif
+}
+
+static int ss_hmac_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int i = 0;
+	int src_len = len;
+	int align_size;
+	int flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	ss_hash_rng_change_clk();
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	if (CE_METHOD_IS_HMAC(req_ctx->type) && (req_ctx->type == SS_METHOD_HMAC_SHA1))
+		ss_hmac_method_set(SS_METHOD_SHA1, task);
+	else if (CE_METHOD_IS_HMAC(req_ctx->type) && (req_ctx->type == SS_METHOD_HMAC_SHA256))
+		ss_hmac_method_set(SS_METHOD_SHA256, task);
+	else
+		ss_hash_method_set(req_ctx->type, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d / %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = %pa\n", ctx->key, &phy_addr);
+	phy_addr = virt_to_phys(ctx->iv);
+	SS_DBG("ctx->iv addr, vir = 0x%px, phy = %pa\n", ctx->iv, &phy_addr);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = %pa\n", task, &phy_addr);
+
+	ss_rng_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, src_len);
+	src_len = ss_sg_len(req_ctx->dma_src.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src, &req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, src_len%align_size, 1);
+	ss_aes_map_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->dst, &req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, len%align_size, 1);
+	ss_aes_map_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+
+	/* data_len set and last_flag set */
+	ss_hmac_sha1_last(task);
+	ss_hash_data_len_set((src_len - SHA256_BLOCK_SIZE) * 8, task);
+
+	/*  for openssl add SHA256_BLOCK_SIZE after data*/
+	task->src[0].len = (task->src[0].len << 2) - SHA256_BLOCK_SIZE;
+
+	/* addr should set in word, src_len and dst_len set in bytes */
+	for (i = 0; i < 8; i++) {
+		task->dst[i].len = (task->dst[i].len) << 2;
+	}
+
+	ce_print_new_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->common_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+			sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, len%align_size);
+	ss_aes_unmap_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, src_len%align_size);
+	ss_aes_unmap_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	return 0;
+}
+static int ss_aead_start(ss_aead_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx)
+{
+	int ret = 0;
+	int gcm_iv_mode;
+	int i = 0;
+	int in_len = ctx->cryptlen;
+	int data_len;
+	int align_size = 0;
+	u32 flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+	int more;
+
+	ss_change_clk(req_ctx->type);
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	ss_aead_mode_set(req_ctx->mode, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, in_len);
+
+	/* hardware gcm may be only support continus data, so maybe need software to fix it. */
+	/* set iv_addr for task descriptor */
+	memset(ctx->task_iv, 0, sizeof(ctx->task_iv));
+	if (req_ctx->dir == SS_DIR_DECRYPT) {
+		if (!ctx->assoclen) {
+			strncpy(ctx->tag, ((char *)sg_dma_address(req_ctx->dma_src.sg) +
+					(sg_dma_len(req_ctx->dma_src.sg) - ctx->tag_len)), ctx->tag_len);
+		} else {
+			strncpy(ctx->tag, ((char *)sg_dma_address(sg_next(req_ctx->dma_src.sg)) +
+					(sg_dma_len(sg_next(req_ctx->dma_src.sg)) - ctx->tag_len)), ctx->tag_len);
+		}
+		for (i = TAG_START; i < ctx->tag_len + TAG_START; i++) {
+			ctx->task_iv[i] = ctx->tag[i-TAG_START];  /* only decrypt need */
+		}
+	}
+
+	ctx->task_iv[IV_SIZE_START] = (ctx->iv_size * 8) & 0xff;
+	ctx->task_iv[IV_SIZE_START+1] = ((ctx->iv_size * 8)>>8) & 0xff;
+	ctx->task_iv[IV_SIZE_START+2] = ((ctx->iv_size * 8)>>16) & 0xff;
+	ctx->task_iv[IV_SIZE_START+3] = ((ctx->iv_size * 8)>>24) & 0xff;
+
+	ctx->task_iv[AAD_SIZE_START] = (ctx->assoclen * 8) & 0xff;
+	ctx->task_iv[AAD_SIZE_START+1] = ((ctx->assoclen * 8)>>8) & 0xff;
+	ctx->task_iv[AAD_SIZE_START+2] = ((ctx->assoclen * 8)>>16) & 0xff;
+	ctx->task_iv[AAD_SIZE_START+3] = ((ctx->assoclen * 8)>>24) & 0xff;
+
+	ctx->task_iv[PT_SIZE_START] = (ctx->cryptlen * 8) & 0xff;
+	ctx->task_iv[PT_SIZE_START+1] = ((ctx->cryptlen * 8)>>8) & 0xff;
+	ctx->task_iv[PT_SIZE_START+2] = ((ctx->cryptlen * 8)>>16) & 0xff;
+	ctx->task_iv[PT_SIZE_START+3] = ((ctx->cryptlen * 8)>>24) & 0xff;
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = %pa\n", ctx->key, &phy_addr);
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	phy_addr = virt_to_phys(ctx->task_iv);
+	SS_DBG("ctx->task_iv vir = 0x%px, phy = 0x%pa\n", ctx->task_iv, &phy_addr);
+	ss_iv_set(ctx->task_iv, sizeof(ctx->task_iv), task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->task_iv, sizeof(ctx->task_iv), DMA_MEM_TO_DEV);
+
+	phy_addr = virt_to_phys(ctx->task_ctr);
+	SS_DBG("ctx->task_ctr vir = 0x%px, phy = 0x%pa\n", ctx->task_ctr, &phy_addr);
+	ss_gcm_cnt_set(ctx->task_ctr, sizeof(ctx->task_ctr), task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->task_ctr, sizeof(ctx->task_ctr), DMA_DEV_TO_MEM);
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, in_len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	phy_addr = virt_to_phys(ctx->iv);
+	ss_gcm_src_config(&(task->src[0]), phy_addr, DIV_ROUND_UP(ctx->iv_size, align_size)*align_size);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->iv, sizeof(ctx->iv_size), DMA_DEV_TO_MEM);
+
+	ss_aead_sg_config(task->src, &req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, in_len%align_size);
+	ss_aead_map_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+	if (req_ctx->dir == SS_DIR_DECRYPT)
+		task->src[req_ctx->dma_src.nents].len = ((task->src[req_ctx->dma_src.nents].len << 2) - ctx->tag_len) >> 2;
+
+	ce_print_task_desc(task);
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, in_len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->dst,	&req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, in_len%align_size, 0);
+	ss_aes_map_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+
+	ss_tag_len_set((ctx->tag_len) * 8, task);
+	if (ctx->iv_size == 12)
+		gcm_iv_mode = 1;
+	else
+		gcm_iv_mode = 3;
+	ss_gcm_iv_mode(task, gcm_iv_mode);
+	ss_cts_last(task);
+	more = (req_ctx->dir) ? 0 : DIV_ROUND_UP(ctx->tag_len, align_size) * align_size;
+	ss_gcm_reserve_set(task, ctx->iv_size, ctx->assoclen, in_len);
+	data_len = (DIV_ROUND_UP(in_len, align_size)*align_size +
+			DIV_ROUND_UP(ctx->iv_size, align_size)*align_size +
+			DIV_ROUND_UP(ctx->assoclen, align_size)*align_size + more);
+	ss_data_len_set(data_len, task);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task, sizeof(ce_task_desc_t),
+		DMA_MEM_TO_DEV);
+
+	SS_DBG("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x, data_len:%d\n",
+		task->comm_ctl, task->sym_ctl, task->asym_ctl, task->data_len);
+
+	ss_ctrl_start(task, req_ctx->type, req_ctx->mode);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, in_len%align_size);
+	ss_aes_unmap_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, in_len%align_size);
+	ss_aead_unmap_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->task_iv), sizeof(ctx->task_iv), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->iv),
+			ctx->iv_size, DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->task_ctr), sizeof(ctx->task_ctr), DMA_DEV_TO_MEM);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int ss_aes_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int src_len = len;
+	int align_size = 0;
+	u32 flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	ss_change_clk(req_ctx->type);
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+#ifdef SS_XTS_MODE_ENABLE
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_method_set(req_ctx->dir, SS_METHOD_RAES, task);
+	else
+#endif
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	if ((req_ctx->type == SS_METHOD_RSA)
+		|| (req_ctx->type == SS_METHOD_DH)) {
+		ss_rsa_width_set(len, task);
+		ss_rsa_op_mode_set(req_ctx->mode, task);
+	} else if (req_ctx->type == SS_METHOD_ECC) {
+		ss_ecc_width_set(len>>1, task);
+		ss_ecc_op_mode_set(req_ctx->mode, task);
+	} else
+		ss_aes_mode_set(req_ctx->mode, task);
+
+#ifdef SS_CFB_MODE_ENABLE
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode == SS_AES_MODE_CFB))
+		ss_cfb_bitwidth_set(req_ctx->bitwidth, task);
+#endif
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = %pa\n", ctx->key, &phy_addr);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+#ifdef SS_XTS_MODE_ENABLE
+	SS_DBG("The current Key:\n");
+	ss_print_hex(ctx->key, ctx->key_size, ctx->key);
+
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_key_set(ctx->key, ctx->key_size/2, task);
+	else
+#endif
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		phy_addr = virt_to_phys(ctx->iv);
+		SS_DBG("ctx->iv vir = 0x%px, phy = 0x%pa\n", ctx->iv, &phy_addr);
+		ss_iv_set(ctx->iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->iv, ctx->iv_size, DMA_MEM_TO_DEV);
+
+		phy_addr = virt_to_phys(ctx->next_iv);
+		SS_DBG("ctx->next_iv addr, vir = 0x%px, phy = 0x%pa\n",
+			ctx->next_iv, &phy_addr);
+		ss_cnt_set(ctx->next_iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->next_iv, ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, src_len);
+	if ((req_ctx->type == SS_METHOD_ECC)
+		|| ((req_ctx->type == SS_METHOD_RSA) &&
+			(req_ctx->mode == CE_RSA_OP_M_MUL)))
+		src_len = ss_sg_len(req_ctx->dma_src.sg, len);
+
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src,	&req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, src_len%align_size, 0);
+	ss_aes_map_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->dst,	&req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, len%align_size, 0);
+	ss_aes_map_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+
+#ifdef SS_SUPPORT_CE_V3_1
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		ss_data_len_set(len, task);
+/*if (len < SZ_4K)  A bad way to determin the last packet of CTS mode. */
+			ss_cts_last(task);
+	} else
+		ss_data_len_set(
+			DIV_ROUND_UP(src_len, align_size)*align_size/4, task);
+#else
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		/* A bad way to determin the last packet. */
+		/* if (len < SZ_4K) */
+		ss_cts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS)) {
+		ss_xts_first(task);
+		ss_xts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (req_ctx->type == SS_METHOD_RSA)
+		ss_data_len_set(len*3, task);
+	else
+		ss_data_len_set(DIV_ROUND_UP(src_len, align_size)*align_size,
+			task);
+#endif
+
+	ce_print_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task, sizeof(ce_task_desc_t),
+		DMA_MEM_TO_DEV);
+
+	SS_DBG("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x, data_len:%d\n",
+		task->comm_ctl, task->sym_ctl, task->asym_ctl, task->data_len);
+
+	ss_ctrl_start(task, req_ctx->type, req_ctx->mode);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, len%align_size);
+	ss_aes_unmap_padding(task->dst,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, src_len%align_size);
+	ss_aes_unmap_padding(task->src,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->iv),
+			ctx->iv_size, DMA_MEM_TO_DEV);
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->next_iv),
+			ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+	/* Backup the next IV from ctr_descriptor, except CBC/CTS/XTS mode. */
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode != SS_AES_MODE_CBC)
+		&& (req_ctx->mode != SS_AES_MODE_CTS)
+		&& (req_ctx->mode != SS_AES_MODE_XTS))
+		memcpy(ctx->iv, ctx->next_iv, ctx->iv_size);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* verify the key_len */
+int ss_aes_key_valid(struct crypto_ablkcipher *tfm, int len)
+{
+	if (unlikely(len > SS_RSA_MAX_SIZE)) {
+		SS_ERR("Unsupported key size: %d\n", len);
+		tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return 0;
+}
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+static void ss_rsa_preprocess(ss_aes_ctx_t *ctx,
+	ss_aes_req_ctx_t *req_ctx, int len)
+{
+	struct scatterlist sg = {0};
+	ss_aes_req_ctx_t *tmp_req_ctx = NULL;
+
+	if (!((req_ctx->type == SS_METHOD_RSA) &&
+		(req_ctx->mode != CE_RSA_OP_M_MUL)))
+		return;
+
+	tmp_req_ctx = kmalloc(sizeof(ss_aes_req_ctx_t), GFP_KERNEL);
+	if (tmp_req_ctx == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", sizeof(ss_aes_req_ctx_t));
+		return;
+	}
+
+	memcpy(tmp_req_ctx, req_ctx, sizeof(ss_aes_req_ctx_t));
+	tmp_req_ctx->mode = CE_RSA_OP_M_MUL;
+
+	sg_init_one(&sg, ctx->key, ctx->iv_size*2);
+	tmp_req_ctx->dma_src.sg = &sg;
+
+	ss_aes_start(ctx, tmp_req_ctx, len);
+
+	SS_DBG("The preporcess of RSA complete!\n\n");
+	kfree(tmp_req_ctx);
+}
+#endif
+
+static int ss_rng_start(ss_aes_ctx_t *ctx, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	int i = 0;
+	int flow = ctx->comm.flow;
+	int rng_len = 0;
+	char *buf = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	if (trng)
+		rng_len = DIV_ROUND_UP(dlen, 32)*32; /* align with 32 Bytes */
+	else
+		rng_len = DIV_ROUND_UP(dlen, 20)*20; /* align with 20 Bytes */
+
+	if (rng_len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", rng_len);
+		rng_len = SS_RNG_MAX_LEN;
+	}
+
+	buf = kmalloc(rng_len, GFP_KERNEL);
+	if (buf == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", rng_len);
+		return -ENOMEM;
+	}
+
+	ss_hash_rng_change_clk();
+
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	if (trng)
+		ss_rng_method_set(SS_METHOD_SHA256, SS_METHOD_TRNG, task);
+	else
+		ss_rng_method_set(SS_METHOD_SHA1, SS_METHOD_PRNG, task);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = 0x%pa\n", ctx->key, &phy_addr);
+
+	if (trng == 0) {
+		/* Must set the seed addr in PRNG, key_len 5 words stable*/
+		ctx->key_size = 5 * sizeof(int);
+		ss_rng_key_set(ctx->key, ctx->key_size, task);
+		task->data_len = 0;
+		ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+	}
+	phy_addr = virt_to_phys(buf);
+	SS_DBG("buf addr, vir = 0x%px, phy = 0x%pa\n", buf, &phy_addr);
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = virt_to_phys(buf) >> 2;	/*address in words*/
+	task->dst[0].len  = (rng_len + 3)/4;
+	dma_map_single(&ss_dev->pdev->dev, buf, rng_len, DMA_DEV_TO_MEM);
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n", flow, dlen, rng_len);
+
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+	/* addr should set in word, src_len and dst_len set in bytes */
+	for (i = 0; i < 8; i++) {
+		task->src[i].len = (task->src[i].len) << 2;
+		task->dst[i].len = (task->dst[i].len) << 2;
+	}
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->common_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(buf),
+		rng_len, DMA_DEV_TO_MEM);
+	if (trng == 0)
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->key),
+			ctx->key_size, DMA_MEM_TO_DEV);
+	memcpy(rdata, buf, dlen);
+	kfree(buf);
+	ss_irq_disable(flow);
+	ret = dlen;
+
+	return ret;
+}
+
+int ss_rng_get_random(struct crypto_rng *tfm, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	ss_aes_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, data = 0x%px, len = %d, trng = %d\n",
+		ctx->comm.flow, data, len, trng);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	ss_dev_lock();
+	ret = ss_rng_start(ctx, data, len, trng);
+	ss_dev_unlock();
+
+	SS_DBG("Get %d byte random.\n", ret);
+
+	return ret;
+}
+
+static int ss_drbg_start(ss_drbg_ctx_t *ctx, u8 *src, u32 slen, u8 *rdata, u32 dlen, u32 mode)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	int rng_len = dlen;
+	char *buf = NULL;
+	char *entropy = NULL;
+	char *person = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	if (rng_len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", rng_len);
+		rng_len = SS_RNG_MAX_LEN;
+	}
+
+	if (ctx->entropt_size < 80 / 8) {
+		SS_ERR("The DRBG length is too small: %d, less than 80 bit\n", ctx->entropt_size);
+		return -EINVAL;
+	}
+
+	buf = kmalloc(rng_len, GFP_KERNEL);
+	if (buf == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", rng_len);
+		return -ENOMEM;
+	}
+
+	entropy = kmalloc(ctx->entropt_size, GFP_KERNEL);
+	if (entropy == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", ctx->entropt_size);
+		return -ENOMEM;
+	}
+	memcpy(entropy, ctx->entropt, ctx->entropt_size);
+
+	if (ctx->person_size) {
+		person = kmalloc(ctx->person_size, GFP_KERNEL);
+		if (buf == NULL) {
+			SS_ERR("Failed to malloc(%d)\n", ctx->person_size);
+			return -ENOMEM;
+		}
+	}
+	ss_hash_rng_change_clk();
+
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_rng_method_set(mode, SS_METHOD_DRBG, task);
+
+	phy_addr = virt_to_phys(ctx->entropt);
+	SS_DBG("ctx->entropt, vir = 0x%px, phy = 0x%pa\n", ctx->entropt, &phy_addr);
+
+	phy_addr = virt_to_phys(ctx->person);
+	SS_DBG("ctx->person, vir = 0x%px, phy = 0x%pa\n", ctx->person, &phy_addr);
+
+	phy_addr = virt_to_phys(buf);
+	SS_DBG("buf addr, vir = 0x%px, phy = %pa\n", buf, &phy_addr);
+
+	/* Prepare the dst scatterlist */
+	task->src[0].addr = virt_to_phys(entropy) >> 2;	/*address in words*/
+	task->src[0].len = ctx->entropt_size;
+	task->src[1].addr = (virt_to_phys(ctx->nonce)) >> 2;	/* in software, not used*/
+	task->src[1].len = 0;
+	task->src[2].addr = virt_to_phys(person) >> 2;	/*address in words*/
+	task->src[2].len = ctx->person_size;
+	task->src[3].addr = virt_to_phys(src) >> 2;	/*address in words*/
+	task->src[3].len = slen;
+
+	dma_map_single(&ss_dev->pdev->dev, entropy, ctx->entropt_size, DMA_MEM_TO_DEV);
+	dma_map_single(&ss_dev->pdev->dev, ctx->nonce, ctx->nonce_size, DMA_MEM_TO_DEV);
+	dma_map_single(&ss_dev->pdev->dev, person, ctx->person_size, DMA_MEM_TO_DEV);
+	dma_map_single(&ss_dev->pdev->dev, src, slen, DMA_MEM_TO_DEV);
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = virt_to_phys(buf) >> 2;	/*address in words*/
+	task->dst[0].len  = rng_len;
+
+	dma_map_single(&ss_dev->pdev->dev, buf, rng_len, DMA_DEV_TO_MEM);
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n", flow, dlen, rng_len);
+
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+	ce_print_new_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->common_ctl, ss_reg_rd(CE_REG_ICR));
+
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(entropy),
+					ctx->entropt_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->nonce),
+					ctx->nonce_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(person),
+					ctx->person_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(src),
+					slen, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(buf),
+					rng_len, DMA_DEV_TO_MEM);
+
+	memcpy(rdata, buf, dlen);
+	kfree(buf);
+	if (ctx->person_size)
+		kfree(person);
+	kfree(entropy);
+	ss_irq_disable(flow);
+	ret = dlen;
+
+	return ret;
+}
+
+int ss_drbg_get_random(struct crypto_rng *tfm, const u8 *src, u32 slen, u8 *rdata, u32 dlen, u32 mode)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	u8 *src_t;
+	ss_drbg_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, src = 0x%px, slen = %d, data = 0x%px, len = %d, hash_mode = %d\n",
+		ctx->comm.flow, src, slen, data, len, mode);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	src_t = kzalloc(slen, GFP_KERNEL);
+	if (src_t == NULL) {
+		SS_ERR("Failed to kmalloc(%d)\n", slen);
+		return -ENOMEM;
+	}
+	memcpy(src_t, src, slen);
+
+	ss_dev_lock();
+	ret = ss_drbg_start(ctx, src_t, slen, data, len, mode);
+	ss_dev_unlock();
+
+	kfree(src_t);
+	SS_DBG("Get %d byte random.\n", ret);
+
+	return ret;
+}
+
+
+u32 ss_hash_start(ss_hash_ctx_t *ctx,
+		ss_aes_req_ctx_t *req_ctx, u32 len, u32 last)
+{
+	int ret = 0;
+	int i = 0;
+	int flow = ctx->comm.flow;
+	u32 blk_size = ss_hash_blk_size(req_ctx->type);
+	char *digest = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	/* Total len is too small, so process it in the padding data later. */
+	if ((last == 0) && (len > 0) && (len < blk_size)) {
+		ctx->cnt += len;
+		return 0;
+	}
+	ss_hash_rng_change_clk();
+
+	digest = kzalloc(SHA512_DIGEST_SIZE, GFP_KERNEL);
+	if (digest == NULL) {
+		SS_ERR("Failed to kmalloc(%d)\n", SHA512_DIGEST_SIZE);
+		return -ENOMEM;
+	}
+
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_hash_method_set(req_ctx->type, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d / %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+	SS_DBG("IV address = 0x%px, size = %d\n", ctx->md, ctx->md_size);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+	ss_hash_iv_set(ctx->md, ctx->md_size, task);
+	ss_hash_iv_mode_set(1, task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->md, ctx->md_size, DMA_MEM_TO_DEV);
+
+	if (last == 1) {
+		ss_hmac_sha1_last(task);
+		ss_hash_data_len_set(ctx->tail_len*8, task);
+	} else
+		ss_hash_data_len_set((len - len%blk_size)*8, task);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->src,
+		&req_ctx->dma_src, req_ctx->type, 0, len%blk_size, 1);
+
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		task->src[0].len = (ctx->tail_len + 3)/4;	/* src/dst data_len special*/
+		SS_DBG("cnt %d, tail_len %d.\n", ctx->cnt, ctx->tail_len);
+		ctx->cnt <<= 3; /* Translate to bits in the last pakcket */
+		task->data_len = ctx->cnt;
+	}
+#endif
+
+	/* Prepare the dst scatterlist */
+	task->dst[0].addr = virt_to_phys(digest) >> 2; /*address in word*/
+	task->dst[0].len  = ctx->md_size/4 ;		/* src/dst data_len special*/
+
+	if (last == 1) {
+		if (req_ctx->type == SS_METHOD_SHA224)
+			task->dst[0].len  = SHA224_DIGEST_SIZE/4 ;
+		if (req_ctx->type == SS_METHOD_SHA384)
+			task->dst[0].len  = SHA384_DIGEST_SIZE/4 ;
+	}
+
+	dma_map_single(&ss_dev->pdev->dev,
+		digest, SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	phy_addr = virt_to_phys(digest);
+	SS_DBG("digest addr, vir = 0x%px, phy = 0x%pa\n", digest, &phy_addr);
+
+	/* addr should set in word, src_len and dst_len set in bytes */
+		for (i = 0; i < 8; i++) {
+			task->src[i].len = (task->src[i].len) << 2;
+			task->dst[i].len = (task->dst[i].len) << 2;
+		}
+
+	ce_print_new_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->common_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(digest),
+		SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		ctx->cnt >>= 3;
+	}
+#endif
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+	ss_print_hex(digest, SHA512_DIGEST_SIZE, digest);
+
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		kfree(digest);
+		return -EINVAL;
+	}
+
+	/* Backup the MD to ctx->md. */
+	memcpy(ctx->md, digest, ctx->md_size);
+
+	if (last == 0)
+		ctx->cnt += len;
+	kfree(digest);
+	return 0;
+}
+
+void ss_load_iv(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx,
+	char *buf, int size)
+{
+	if (buf == NULL)
+		return;
+
+	/* Only AES/DES/3DES-ECB don't need IV. */
+	if (CE_METHOD_IS_AES(req_ctx->type) &&
+		(req_ctx->mode == SS_AES_MODE_ECB))
+		return;
+
+	/* CBC/CTS/GCM need update the IV eachtime. */
+	if ((ctx->cnt == 0)
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CBC))
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS))
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, GCM))) {
+		SS_DBG("IV address = 0x%px, size = %d\n", buf, size);
+		ctx->iv_size = size;
+		memcpy(ctx->iv, buf, ctx->iv_size);
+	}
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, ctx->iv_size, ctx->iv);
+}
+
+void ss_aead_load_iv(ss_aead_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx,
+	char *buf, int size)
+{
+	if (buf == NULL)
+		return;
+
+	/* CBC/CTS/GCM need update the IV eachtime. */
+	if ((ctx->cnt == 0)
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, GCM))) {
+		SS_DBG("IV address = 0x%px, size = %d\n", buf, size);
+		ctx->iv_size = size;
+		memcpy(ctx->iv, buf, ctx->iv_size);
+	}
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, ctx->iv_size, ctx->iv);
+}
+
+int ss_aead_one_req(sunxi_ss_t *sss, struct aead_request *req)
+{
+	int ret = 0;
+	struct crypto_aead *tfm = NULL;
+	ss_aead_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = 0x%px, dst = 0x%px\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_aead_reqtfm(req);
+	req_ctx = aead_request_ctx(req);
+	ctx = crypto_aead_ctx(tfm);
+
+	ss_aead_load_iv(ctx, req_ctx, req->iv, crypto_aead_ivsize(tfm));
+
+	memset(ctx->task_ctr, 0, sizeof(ctx->task_ctr));
+	ctx->tag_len = tfm->authsize;
+	ctx->assoclen = req->assoclen;
+	ctx->cryptlen = req->cryptlen;
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+	ret = ss_aead_start(ctx, req_ctx);
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+	return ret;
+}
+
+int ss_aes_one_req(sunxi_ss_t *sss, struct ablkcipher_request *req)
+{
+	int ret = 0;
+	struct crypto_ablkcipher *tfm = NULL;
+	ss_aes_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = 0x%px, dst = 0x%px\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_ablkcipher_reqtfm(req);
+	req_ctx = ablkcipher_request_ctx(req);
+	ctx = crypto_ablkcipher_ctx(tfm);
+
+	ss_load_iv(ctx, req_ctx, req->info, crypto_ablkcipher_ivsize(tfm));
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+	ss_rsa_preprocess(ctx, req_ctx, req->nbytes);
+#endif
+
+	if (CE_METHOD_IS_HMAC(req_ctx->type)) {
+		ret = ss_hmac_start(ctx, req_ctx, req->nbytes);
+	} else {
+		ret = ss_aes_start(ctx, req_ctx, req->nbytes);
+	}
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR) {
+		SS_DBG("CNT: %08x %08x %08x %08x\n",
+			*(int *)&ctx->iv[0], *(int *)&ctx->iv[4],
+			*(int *)&ctx->iv[8], *(int *)&ctx->iv[12]);
+	}
+#endif
+
+	ctx->cnt += req->nbytes;
+	return ret;
+}
+
+irqreturn_t sunxi_ss_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ss_t *sss = (sunxi_ss_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&sss->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING << (2 * i))) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&sss->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&sss->lock, flags);
+	return IRQ_HANDLED;
+}
diff --git a/drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.c b/drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.c
new file mode 100644
index 000000000..930a5f5d4
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.c
@@ -0,0 +1,450 @@
+/*
+ * The interface function of controlling the SS register.
+ *
+ * Copyright (C) 2018 Allwinner.
+ *
+ *<xupengliu@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+
+#include "../sunxi_ce.h"
+#include "sunxi_ce_reg.h"
+
+inline u32 ss_readl(u32 offset)
+{
+	return readl(ss_membase() + offset);
+}
+
+inline void ss_writel(u32 offset, u32 val)
+{
+	writel(val, ss_membase() + offset);
+}
+
+u32 ss_reg_rd(u32 offset)
+{
+	return ss_readl(offset);
+}
+
+void ss_reg_wr(u32 offset, u32 val)
+{
+	ss_writel(offset, val);
+}
+
+void ss_keyselect_set(int select, ce_task_desc_t *task)
+{
+	task->sym_ctl |= select << CE_SYM_CTL_KEY_SELECT_SHIFT;
+}
+
+void ss_keysize_set(int size, ce_task_desc_t *task)
+{
+	int type = CE_AES_KEY_SIZE_128;
+
+	switch (size) {
+	case AES_KEYSIZE_192:
+		type = CE_AES_KEY_SIZE_192;
+		break;
+	case AES_KEYSIZE_256:
+		type = CE_AES_KEY_SIZE_256;
+		break;
+	default:
+		break;
+	}
+
+	task->sym_ctl |= (type << CE_SYM_CTL_KEY_SIZE_SHIFT);
+}
+
+/* key: phsical address. */
+void ss_key_set(char *key, int size, ce_task_desc_t *task)
+{
+	int i = 0;
+	int key_sel = CE_KEY_SELECT_INPUT;
+	struct {
+		int type;
+		char desc[AES_MIN_KEY_SIZE];
+	} keys[] = {
+		{CE_KEY_SELECT_SSK,		   CE_KS_SSK},
+		{CE_KEY_SELECT_HUK,		   CE_KS_HUK},
+		{CE_KEY_SELECT_RSSK,	   CE_KS_RSSK},
+
+		{CE_KEY_SELECT_INTERNAL_0, CE_KS_INTERNAL_0},
+		{CE_KEY_SELECT_INTERNAL_1, CE_KS_INTERNAL_1},
+		{CE_KEY_SELECT_INTERNAL_2, CE_KS_INTERNAL_2},
+		{CE_KEY_SELECT_INTERNAL_3, CE_KS_INTERNAL_3},
+		{CE_KEY_SELECT_INTERNAL_4, CE_KS_INTERNAL_4},
+		{CE_KEY_SELECT_INTERNAL_5, CE_KS_INTERNAL_5},
+		{CE_KEY_SELECT_INTERNAL_6, CE_KS_INTERNAL_6},
+		{CE_KEY_SELECT_INTERNAL_7, CE_KS_INTERNAL_7},
+		{CE_KEY_SELECT_INPUT, ""} };
+
+	while (keys[i].type != CE_KEY_SELECT_INPUT) {
+		if (strncasecmp(key, keys[i].desc, AES_MIN_KEY_SIZE) == 0) {
+			key_sel = keys[i].type;
+			memset(key, 0, size);
+			break;
+		}
+		i++;
+	}
+	SS_DBG("The key select: %d\n", key_sel);
+
+	ss_keyselect_set(key_sel, task);
+	ss_keysize_set(size, task);
+	task->key_addr = (virt_to_phys(key)) >> 2;	//address in word
+}
+
+void ss_pending_clear(int flow)
+{
+	int val = 0;
+
+	switch (flow) {
+	case (0):
+		val = CE_CHAN_PENDING << 0;
+		break;
+	case (1):
+		val = CE_CHAN_PENDING << 2;
+		break;
+	case (2):
+		val = CE_CHAN_PENDING << 4;
+		break;
+	case (3):
+		val = CE_CHAN_PENDING << 6;
+		break;
+
+	default:
+		break;
+	}
+	ss_writel(CE_REG_ISR, val);
+}
+
+int ss_pending_get(void)
+{
+	return ss_readl(CE_REG_ISR);
+}
+
+void ss_irq_enable(int flow)
+{
+	int val = ss_readl(CE_REG_ICR);
+
+	val |= CE_CHAN_INT_ENABLE << flow;
+	ss_writel(CE_REG_ICR, val);
+}
+
+void ss_irq_disable(int flow)
+{
+	int val = ss_readl(CE_REG_ICR);
+
+	val &= ~(CE_CHAN_INT_ENABLE << flow);
+	ss_writel(CE_REG_ICR, val);
+}
+
+void ss_md_get(char *dst, char *src, int size)
+{
+	memcpy(dst, src, size);
+}
+
+/* iv: phsical address. */
+void ss_iv_set(char *iv, int size, ce_task_desc_t *task)
+{
+	task->iv_addr = (virt_to_phys(iv)) >> 2;	/*address in word*/
+}
+
+void ss_iv_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->comm_ctl |= mode << CE_COMM_CTL_IV_MODE_SHIFT;
+}
+
+void ss_cntsize_set(int size, ce_task_desc_t *task)
+{
+	task->sym_ctl |= size << CE_SYM_CTL_CTR_SIZE_SHIFT;
+}
+
+void ss_cnt_set(char *cnt, int size, ce_task_desc_t *task)
+{
+	task->ctr_addr = (virt_to_phys(cnt)) >> 2;		/*address in word*/
+
+	ss_cntsize_set(CE_CTR_SIZE_128, task);
+}
+
+void ss_gcm_cnt_set(char *cnt, int size, ce_task_desc_t *task)
+{
+	task->ctr_addr = (virt_to_phys(cnt)) >> 2;		/*address in word*/
+
+	ss_cntsize_set(CE_CTR_SIZE_32, task);
+}
+
+void ss_cts_last(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_CTS_LAST;
+}
+
+void ss_tag_len_set(u32 len, ce_task_desc_t *task)
+{
+	task->comm_ctl |= len << CE_COM_CTL_GCM_TAGLEN_SHIFT;
+}
+
+void ss_gcm_iv_mode(ce_task_desc_t *task, int iv_mode)
+{
+	task->sym_ctl |= iv_mode << CE_SYM_CTL_GCM_IV_MODE_SHIFT;
+}
+
+void ss_gcm_reserve_set(ce_task_desc_t *task, int iv_len, int aad_len, int pt_len)
+{
+	task->reserved[0] = iv_len * 8;
+	task->reserved[1] = aad_len * 8;
+	task->reserved[2] = pt_len * 8;
+}
+
+void ss_gcm_src_config(ce_scatter_t *src, u32 addr, u32 len)
+{
+	src->addr = addr >> 2;	/* address in words*/
+	src->len = len >> 2;	/* in word*/
+}
+
+void ss_xts_first(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_XTS_FIRST;
+}
+
+void ss_xts_last(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_XTS_LAST;
+}
+
+void ss_method_set(int dir, int type, ce_task_desc_t *task)
+{
+	task->comm_ctl |= dir << CE_COMM_CTL_OP_DIR_SHIFT;
+	task->comm_ctl |= type << CE_COMM_CTL_METHOD_SHIFT;
+}
+
+void ss_aes_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->sym_ctl |= mode << CE_SYM_CTL_OP_MODE_SHIFT;
+}
+
+void ss_aead_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->sym_ctl |= mode << CE_SYM_CTL_OP_MODE_SHIFT;
+	task->sym_ctl |= 1 << 2;	/*set gcm mode counter width*/
+}
+
+void ss_cfb_bitwidth_set(int bitwidth, ce_task_desc_t *task)
+{
+	int val = 0;
+
+	switch (bitwidth) {
+	case 1:
+		val = CE_CFB_WIDTH_1;
+		break;
+	case 8:
+		val = CE_CFB_WIDTH_8;
+		break;
+	case 64:
+		val = CE_CFB_WIDTH_64;
+		break;
+	case 128:
+		val = CE_CFB_WIDTH_128;
+		break;
+	default:
+		break;
+	}
+	task->sym_ctl |= val << CE_SYM_CTL_CFB_WIDTH_SHIFT;
+}
+
+void ss_sha_final(void)
+{
+	/* unsupported. */
+}
+
+void ss_check_sha_end(void)
+{
+	/* unsupported. */
+}
+
+void ss_rsa_width_set(int size, ce_task_desc_t *task)
+{
+	task->asym_ctl |= DIV_ROUND_UP(size, 4);
+}
+
+void ss_rsa_op_mode_set(int mode, ce_task_desc_t *task)
+{
+	/* Consider !2 as M_EXP, for compatible with the previous SOC. */
+	if (mode == CE_RSA_OP_M_MUL)
+		task->asym_ctl |= CE_RSA_OP_M_MUL<<CE_ASYM_CTL_RSA_OP_SHIFT;
+	else
+		task->asym_ctl |= CE_RSA_OP_M_EXP<<CE_ASYM_CTL_RSA_OP_SHIFT;
+}
+
+void ss_ecc_width_set(int size, ce_task_desc_t *task)
+{
+	task->asym_ctl |= DIV_ROUND_UP(size, 4);
+}
+
+void ss_ecc_op_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->asym_ctl |= mode<<CE_ASYM_CTL_RSA_OP_SHIFT;
+}
+
+void ss_ctrl_start(ce_task_desc_t *task, int type, int mode)
+{
+	ss_writel(CE_REG_TSK, (virt_to_phys(task)) >> 2);	/* task addr in word */
+
+	if (CE_METHOD_IS_AES(type) && (mode == SS_AES_MODE_XTS))
+		ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_RAES_TYPE_SHIFT);
+	else if (CE_METHOD_IS_AES(type) && (mode != SS_AES_MODE_XTS))
+		ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_SYMM_TYPE_SHIFT);
+	else
+		ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_ASYM_TYPE_SHIFT);
+}
+
+void ss_ctrl_stop(void)
+{
+	/* unsupported */
+}
+
+int ss_flow_err(int flow)
+{
+	return ss_readl(CE_REG_ERR) & CE_REG_ESR_CHAN_MASK(flow);
+}
+
+void ss_wait_idle(void)
+{
+#ifdef SS_SUPPORT_CE_V3_1
+	while ((ss_readl(CE_REG_TSR) & CE_REG_TSR_BUSY_MASK) ==
+		CE_REG_TSR_BUSY) {
+		SS_DBG("Need wait for the hardware.\n");
+		msleep(20);
+	}
+#else
+	while ((ss_readl(CE_REG_TSR) & 0xff) != 0x0) {
+		SS_DBG("Need wait for the hardware.\n");
+		msleep(20);
+	}
+#endif
+}
+
+void ss_data_len_set(int len, ce_task_desc_t *task)
+{
+	task->data_len = len;
+}
+
+int ss_reg_print(char *buf, int len)
+{
+	return snprintf(buf, len,
+		"The SS control register:\n"
+		"[TSK] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[CTL] 0x%02x = 0x%08x\n"
+#endif
+		"[ICR] 0x%02x = 0x%08x, [ISR] 0x%02x = 0x%08x\n"
+		"[TLR] 0x%02x = 0x%08x\n"
+		"[TSR] 0x%02x = 0x%08x\n"
+		"[ERR] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[CSS] 0x%02x = 0x%08x, [CDS] 0x%02x = 0x%08x\n"
+#endif
+		"[CSA] 0x%02x = 0x%08x, [CDA] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[TPR] 0x%02x = 0x%08x\n"
+#else
+		"[HCSA] 0x%02x = 0x%08x\n"
+		"[HCDA] 0x%02x = 0x%08x\n"
+		"[ACSA] 0x%02x = 0x%08x\n"
+		"[ACDA] 0x%02x = 0x%08x\n"
+		"[XCSA] 0x%02x = 0x%08x\n"
+		"[XCDA] 0x%02x = 0x%08x\n"
+		"[VER] 0x%02x = 0x%08x\n"
+#endif
+		,
+		CE_REG_TSK, ss_readl(CE_REG_TSK),
+#ifdef SS_SUPPORT_CE_V3_1
+		CE_REG_CTL, ss_readl(CE_REG_CTL),
+#endif
+		CE_REG_ICR, ss_readl(CE_REG_ICR),
+		CE_REG_ISR, ss_readl(CE_REG_ISR),
+		CE_REG_TLR, ss_readl(CE_REG_TLR),
+		CE_REG_TSR, ss_readl(CE_REG_TSR),
+		CE_REG_ERR, ss_readl(CE_REG_ERR),
+#ifdef SS_SUPPORT_CE_V3_1
+		CE_REG_CSS, ss_readl(CE_REG_CSS),
+		CE_REG_CDS, ss_readl(CE_REG_CDS),
+#endif
+		CE_REG_CSA, ss_readl(CE_REG_CSA),
+		CE_REG_CDA, ss_readl(CE_REG_CDA)
+#ifdef SS_SUPPORT_CE_V3_1
+		,
+		CE_REG_TPR, ss_readl(CE_REG_TPR)
+#else
+		,
+		CE_REG_HCSA, ss_readl(CE_REG_HCSA),
+		CE_REG_HCDA, ss_readl(CE_REG_HCDA),
+		CE_REG_ACSA, ss_readl(CE_REG_ACSA),
+		CE_REG_ACDA, ss_readl(CE_REG_ACDA),
+		CE_REG_XCSA, ss_readl(CE_REG_XCSA),
+		CE_REG_XCDA, ss_readl(CE_REG_XCDA),
+		CE_REG_VER, ss_readl(CE_REG_VER)
+#endif
+		);
+}
+
+/* key: phsical address. */
+void ss_rng_key_set(char *key, int size, ce_new_task_desc_t *task)
+{
+	task->key_addr = (virt_to_phys(key)) >> 2;
+}
+
+/* iv: phsical address. */
+void ss_hash_iv_set(char *iv, int size, ce_new_task_desc_t *task)
+{
+	task->iv_addr = (virt_to_phys(iv)) >> 2;
+}
+
+void ss_hash_iv_mode_set(int mode, ce_new_task_desc_t *task)
+{
+	task->common_ctl |= mode << CE_CTL_IV_MODE_SHIFT;
+}
+
+void ss_hmac_sha1_last(ce_new_task_desc_t *task)
+{
+	task->common_ctl |= CE_CTL_HMAC_SHA1_LAST;
+}
+
+void ss_hmac_method_set(int type, ce_new_task_desc_t *task)
+{
+	task->main_cmd |= type << CE_CMD_HASH_METHOD_SHIFT;
+	task->main_cmd |= 0x1 << CE_CMD_HMAC_METHOD_SHIFT;
+}
+
+void ss_hash_method_set(int type, ce_new_task_desc_t *task)
+{
+	task->main_cmd |= type << CE_CMD_HASH_METHOD_SHIFT;
+}
+
+void ss_rng_method_set(int hash_type, int type, ce_new_task_desc_t *task)
+{
+	task->main_cmd |= (type << CE_CMD_RNG_METHOD_SHIFT);
+	task->main_cmd |= (hash_type << CE_CMD_HASH_METHOD_SHIFT);
+	if (type == SS_METHOD_DRBG) {
+		task->common_ctl |= CE_CTL_HMAC_SHA1_LAST;
+		/* for drbg, need set sub_cmd bit[28:16] */
+		task->main_cmd |= (2 << 16 | 0 << 20 | BIT(28));
+	}
+}
+
+void ss_hash_rng_ctrl_start(ce_new_task_desc_t *task)
+{
+	ss_writel(CE_REG_TSK, (virt_to_phys(task)) >> 2);
+	ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_HASH_RBG_TYPE_SHIFT);
+}
+
+void ss_hash_data_len_set(int len, ce_new_task_desc_t *task)
+{
+	task->common_ctl |= 0 << 13;
+	task->data_len = len;
+}
+
diff --git a/drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.h b/drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.h
new file mode 100644
index 000000000..002e5694e
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v4/sunxi_ce_reg.h
@@ -0,0 +1,296 @@
+/*
+ * The register macro of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2018 Allwinner.
+ *
+ * <xupengliu@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_SECURITY_SYSTEM_REG_H_
+#define _SUNXI_SECURITY_SYSTEM_REG_H_
+
+/* CE: Crypto Engine, start using CE from sun8iw7/sun8iw9 */
+#define CE_REG_TSK			0x00
+#define CE_REG_ICR			0x08
+#define CE_REG_ISR			0x0C
+#define CE_REG_TLR			0x10
+#define CE_REG_TSR			0x14
+#define CE_REG_ERR			0x18
+
+#define CE_REG_DRL			0x1c
+
+#define CE_REG_CSA			0x24
+#define CE_REG_CDA			0x28
+#define CE_REG_HCSA			0x34
+#define CE_REG_HCDA			0x38
+#define CE_REG_ACSA			0x44
+#define CE_REG_ACDA			0x48
+#define CE_REG_XCSA			0x54
+#define CE_REG_XCDA			0x58
+#define CE_REG_VER			0x90
+
+#define CE_CHAN_INT_ENABLE		1
+
+
+#define CE_REG_TLR_METHOD_TYPE_SHIFT		8
+
+#define CE_REG_ESR_ERR_UNSUPPORT	0
+#define CE_REG_ESR_ERR_LEN			1
+#define CE_REG_ESR_ERR_KEYSRAM		2
+
+#define CE_REG_ESR_ERR_ADDR			5
+
+
+#define CE_REG_ESR_CHAN_SHIFT		8
+#define CE_REG_ESR_CHAN_MASK(flow)	(0xFF << (CE_REG_ESR_CHAN_SHIFT*flow))
+
+/* About the hash/RBG control word */
+#define CE_CTL_CHAN_MASK	0
+
+#define CE_CHAN_PENDING	0x3
+
+#define CE_REG_TLR_SYMM_TYPE_SHIFT		0
+#define CE_REG_TLR_HASH_RBG_TYPE_SHIFT	1
+#define CE_REG_TLR_ASYM_TYPE_SHIFT		2
+#define CE_REG_TLR_RAES_TYPE_SHIFT		3
+
+#define CE_CMD_HASH_METHOD_SHIFT	0
+#define CE_CMD_RNG_METHOD_SHIFT	8
+#define CE_CMD_HMAC_METHOD_SHIFT	4
+
+#define CE_CTL_IV_MODE_SHIFT	8
+#define CE_CTL_HMAC_SHA1_LAST	BIT(12)
+
+#define CE_CTL_IE_SHIFT		16
+#define CE_CTL_IE_MASK 	(0x1 << CE_CTL_IE_SHIFT)
+
+#define SS_METHOD_MD5				0
+#define SS_METHOD_SHA1			1
+#define SS_METHOD_SHA224			2
+#define SS_METHOD_SHA256			3
+#define SS_METHOD_SHA384			4
+#define SS_METHOD_SHA512			5
+#define SS_METHOD_SM3				6
+
+#define SS_METHOD_PRNG			1
+#define SS_METHOD_TRNG			2
+#define SS_METHOD_DRBG			3
+
+/* About the common control word */
+#define CE_COMM_CTL_TASK_INT_SHIFT	31
+#define CE_COMM_CTL_TASK_INT_MASK	(0x1 << CE_COMM_CTL_TASK_INT_SHIFT)
+
+#define CE_CBC_MAC_LEN_SHIFT		17
+
+#define CE_HASH_IV_DEFAULT			0
+#define CE_HASH_IV_INPUT			1
+#define CE_COMM_CTL_IV_MODE_SHIFT	16
+
+#define CE_HMAC_SHA1_LAST			BIT(15)
+
+#define SS_DIR_ENCRYPT				0
+#define SS_DIR_DECRYPT				1
+#define CE_COMM_CTL_OP_DIR_SHIFT	8
+
+#define SS_METHOD_AES				0x0
+#define SS_METHOD_DES				0x1
+#define SS_METHOD_3DES			0x2
+#define SS_METHOD_SM4				0x3
+
+#define SS_METHOD_HMAC_SHA1		0x10	/*to distinguish hmac,
+										but this is not in hardware in fact*/
+#define SS_METHOD_HMAC_SHA256	0x11
+
+#define SS_METHOD_RSA				0x20
+#define SS_METHOD_DH				SS_METHOD_RSA
+#define SS_METHOD_ECC				0x21
+#define SS_METHOD_SM2				0x22
+#define SS_METHOD_RAES				0x30	/* XTS mode */
+
+#define CE_COMM_CTL_METHOD_SHIFT		0
+#define CE_COMM_CTL_METHOD_MASK		0x7F
+
+#define CE_METHOD_IS_HASH(type) ((type == SS_METHOD_MD5) \
+				|| (type == SS_METHOD_SHA1) \
+				|| (type == SS_METHOD_SHA224) \
+				|| (type == SS_METHOD_SHA256) \
+				|| (type == SS_METHOD_SHA384) \
+				|| (type == SS_METHOD_SHA512) \
+				|| (type == SS_METHOD_SM3))
+
+#define CE_METHOD_IS_AES(type) ((type == SS_METHOD_AES) \
+				|| (type == SS_METHOD_DES) \
+				|| (type == SS_METHOD_3DES) \
+				|| (type == SS_METHOD_SM4))
+
+#define CE_METHOD_IS_HMAC(type) ((type == SS_METHOD_HMAC_SHA1) \
+				|| (type == SS_METHOD_HMAC_SHA256))
+
+/* About the symmetric control word */
+
+#define CE_KEY_SELECT_INPUT			0
+#define CE_KEY_SELECT_SSK			1
+#define CE_KEY_SELECT_HUK			2
+#define CE_KEY_SELECT_RSSK			3
+#define CE_KEY_SELECT_INTERNAL_0	8
+#define CE_KEY_SELECT_INTERNAL_1	9
+#define CE_KEY_SELECT_INTERNAL_2	10
+#define CE_KEY_SELECT_INTERNAL_3	11
+#define CE_KEY_SELECT_INTERNAL_4	12
+#define CE_KEY_SELECT_INTERNAL_5	13
+#define CE_KEY_SELECT_INTERNAL_6	14
+#define CE_KEY_SELECT_INTERNAL_7	15
+#define CE_SYM_CTL_KEY_SELECT_SHIFT	20
+
+/* The identification string to indicate the key source. */
+#define CE_KS_SSK			"KEY_SEL_SSK"
+#define CE_KS_HUK			"KEY_SEL_HUK"
+#define CE_KS_RSSK			"KEY_SEL_RSSK"
+#define CE_KS_INTERNAL_0	"KEY_SEL_INTRA_0"
+#define CE_KS_INTERNAL_1	"KEY_SEL_INTRA_1"
+#define CE_KS_INTERNAL_2	"KEY_SEL_INTRA_2"
+#define CE_KS_INTERNAL_3	"KEY_SEL_INTRA_3"
+#define CE_KS_INTERNAL_4	"KEY_SEL_INTRA_4"
+#define CE_KS_INTERNAL_5	"KEY_SEL_INTRA_5"
+#define CE_KS_INTERNAL_6	"KEY_SEL_INTRA_6"
+#define CE_KS_INTERNAL_7	"KEY_SEL_INTRA_7"
+
+#define CE_CFB_WIDTH_1				0
+#define CE_CFB_WIDTH_8				1
+#define CE_CFB_WIDTH_64				2
+#define CE_CFB_WIDTH_128			3
+
+#define CE_SYM_CTL_CFB_WIDTH_SHIFT	18
+#define CE_SYM_CTL_GCM_IV_MODE_SHIFT	4
+
+#define CE_SYM_CTL_AES_CTS_LAST		BIT(16)
+
+#define CE_SYM_CTL_AES_XTS_LAST		BIT(13)
+#define CE_SYM_CTL_AES_XTS_FIRST	BIT(12)
+#define CE_COM_CTL_GCM_TAGLEN_SHIFT	17
+
+#define TAG_START       48
+#define IV_SIZE_START   64
+#define AAD_SIZE_START  72
+#define PT_SIZE_START   80
+
+#define SS_AES_MODE_ECB				0
+#define SS_AES_MODE_CBC				1
+#define SS_AES_MODE_CTR				2
+#define SS_AES_MODE_CTS				3
+#define SS_AES_MODE_OFB				4
+#define SS_AES_MODE_CFB				5
+#define SS_AES_MODE_CBC_MAC			6
+#define SS_AES_MODE_OCB				7
+#define SS_AES_MODE_GCM				8
+#define SS_AES_MODE_XTS				9
+
+#define CE_SYM_CTL_OP_MODE_SHIFT	8
+
+#define CE_CTR_SIZE_16				0
+#define CE_CTR_SIZE_32				1
+#define CE_CTR_SIZE_64				2
+#define CE_CTR_SIZE_128				3
+#define CE_SYM_CTL_CTR_SIZE_SHIFT	2
+
+#define CE_AES_KEY_SIZE_128			0
+#define CE_AES_KEY_SIZE_192			1
+#define CE_AES_KEY_SIZE_256			2
+#define CE_SYM_CTL_KEY_SIZE_SHIFT	0
+
+#define CE_IS_AES_MODE(type, mode, M) (CE_METHOD_IS_AES(type) \
+					&& (mode == SS_AES_MODE_##M))
+
+/* About the asymmetric control word */
+#define CE_RSA_OP_M_EXP		0 /* modular exponentiation */
+
+#define CE_RSA_OP_M_ADD                 1 /* modular add */
+#define CE_RSA_OP_M_MINUS               2 /* modular minus */
+#define CE_RSA_OP_M_MUL                 3 /* modular multiplication */
+
+#define CE_ASYM_CTL_RSA_OP_SHIFT		16
+
+#define CE_ECC_OP_POINT_ADD             0 /* point add */
+#define CE_ECC_OP_POINT_DBL             1 /* point double */
+#define CE_ECC_OP_POINT_MUL             2 /* point multiplication */
+#define CE_ECC_OP_POINT_VER             3 /* point verification */
+#define CE_ECC_OP_ENC                   4 /* encryption */
+#define CE_ECC_OP_DEC                   5 /* decryption */
+#define CE_ECC_OP_SIGN                  6 /* sign */
+#define CE_ECC_OP_VERIFY                7 /* verification */
+
+#define SS_SEED_SIZE			24
+
+
+/* Function declaration */
+
+u32 ss_reg_rd(u32 offset);
+void ss_reg_wr(u32 offset, u32 val);
+
+void ss_key_set(char *key, int size, ce_task_desc_t *task);
+
+int ss_pending_get(void);
+void ss_pending_clear(int flow);
+void ss_irq_enable(int flow);
+void ss_irq_disable(int flow);
+
+void ss_iv_set(char *iv, int size, ce_task_desc_t *task);
+void ss_iv_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_cnt_set(char *cnt, int size, ce_task_desc_t *task);
+void ss_cnt_get(int flow, char *cnt, int size);
+
+void ss_md_get(char *dst, char *src, int size);
+void ss_sha_final(void);
+void ss_check_sha_end(void);
+
+void ss_rsa_width_set(int size, ce_task_desc_t *task);
+void ss_rsa_op_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_ecc_width_set(int size, ce_task_desc_t *task);
+void ss_ecc_op_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_cts_last(ce_task_desc_t *task);
+
+void ss_xts_first(ce_task_desc_t *task);
+void ss_xts_last(ce_task_desc_t *task);
+
+void ss_method_set(int dir, int type, ce_task_desc_t *task);
+
+void ss_aes_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_aead_mode_set(int mode, ce_task_desc_t *task);
+void ss_tag_len_set(u32 len, ce_task_desc_t *task);
+void ss_gcm_src_config(ce_scatter_t *src, u32 addr, u32 len);
+void ss_gcm_reserve_set(ce_task_desc_t *task, int iv_len, int aad_len, int pt_len);
+void ss_gcm_cnt_set(char *cnt, int size, ce_task_desc_t *task);
+void ss_gcm_iv_mode(ce_task_desc_t *task, int iv_mode);
+
+void ss_cfb_bitwidth_set(int bitwidth, ce_task_desc_t *task);
+
+void ss_wait_idle(void);
+void ss_ctrl_start(ce_task_desc_t *task, int type, int mode);
+void ss_ctrl_stop(void);
+int ss_flow_err(int flow);
+
+void ss_data_len_set(int len, ce_task_desc_t *task);
+
+int ss_reg_print(char *buf, int len);
+void ss_keyselect_set(int select, ce_task_desc_t *task);
+void ss_keysize_set(int size, ce_task_desc_t *task);
+
+void ss_rng_key_set(char *key, int size, ce_new_task_desc_t *task);
+void ss_hash_iv_set(char *iv, int size, ce_new_task_desc_t *task);
+void ss_hash_iv_mode_set(int mode, ce_new_task_desc_t *task);
+void ss_hmac_sha1_last(ce_new_task_desc_t *task);
+void ss_hmac_method_set(int type, ce_new_task_desc_t *task);
+void ss_hash_method_set(int type, ce_new_task_desc_t *task);
+void ss_rng_method_set(int hash_type, int type, ce_new_task_desc_t *task);
+void ss_hash_rng_ctrl_start(ce_new_task_desc_t *task);
+void ss_hash_data_len_set(int len, ce_new_task_desc_t *task);
+
+#endif /* end of _SUNXI_SECURITY_SYSTEM_REG_H_ */
diff --git a/drivers/crypto/sunxi-ce/v5/sunxi_ce_cdev_comm.c b/drivers/crypto/sunxi-ce/v5/sunxi_ce_cdev_comm.c
new file mode 100644
index 000000000..5b3ddf68f
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v5/sunxi_ce_cdev_comm.c
@@ -0,0 +1,455 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2014 Allwinner.
+ *
+ * Mintow <duanmintao@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/internal/aead.h>
+#include <crypto/hash.h>
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+
+#include "../sunxi_ce_cdev.h"
+#include "sunxi_ce_reg.h"
+
+#define NO_DMA_MAP		(0xE7)
+#define SRC_DATA_DIR	(0)
+#define DST_DATA_DIR	(0x1)
+
+extern sunxi_ce_cdev_t	*ce_cdev;
+
+irqreturn_t sunxi_ce_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ce_cdev_t *p_cdev = (sunxi_ce_cdev_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&p_cdev->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING << (2 * i))) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&p_cdev->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&p_cdev->lock, flags);
+	return IRQ_HANDLED;
+}
+
+static int check_aes_ctx_vaild(crypto_aes_req_ctx_t *req)
+{
+	if (!req->src_buffer || !req->dst_buffer || !req->key_buffer) {
+		SS_ERR("Invalid para: src = 0x%px, dst = 0x%px key = 0x%p\n",
+				req->src_buffer, req->dst_buffer, req->key_buffer);
+		return -EINVAL;
+	}
+
+	if (req->iv_length) {
+		if (!req->iv_buf) {
+			SS_ERR("Invalid para: iv_buf = 0x%px\n", req->iv_buf);
+			return -EINVAL;
+		}
+	}
+
+	SS_DBG("key_length = %d\n", req->key_length);
+	if (req->key_length > AES_MAX_KEY_SIZE) {
+		SS_ERR("Invalid para: key_length = %d\n", req->key_length);
+		return -EINVAL;
+	} else if (req->key_length < AES_MIN_KEY_SIZE) {
+		SS_ERR("Invalid para: key_length = %d\n", req->key_length);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void ce_aes_config(crypto_aes_req_ctx_t *req, ce_task_desc_t *task)
+{
+	task->chan_id = req->channel_id;
+	ss_method_set(req->dir, SS_METHOD_AES, task);
+	ss_aes_mode_set(req->aes_mode, task);
+}
+
+static void task_iv_init(crypto_aes_req_ctx_t *req, ce_task_desc_t *task, int flag)
+{
+	if (req->iv_length) {
+		if (flag == DMA_MEM_TO_DEV) {
+			ss_iv_set(req->iv_buf, req->iv_length, task);
+			req->iv_phy = dma_map_single(ce_cdev->pdevice, req->iv_buf,
+									req->iv_length, DMA_MEM_TO_DEV);
+			SS_DBG("iv = %px, iv_phy_addr = 0x%lx\n", req->iv_buf, req->iv_phy);
+		} else if (flag == DMA_DEV_TO_MEM) {
+			dma_unmap_single(ce_cdev->pdevice,
+				req->iv_phy, req->iv_length, DMA_DEV_TO_MEM);
+		} else if (flag == NO_DMA_MAP) {
+			ce_iv_phyaddr_set(req->iv_phy, task);
+			//task->iv_addr = (req->iv_phy >> WORD_ALGIN);
+			SS_DBG("iv_phy_addr = 0x%lx\n", req->iv_phy);
+		}
+	}
+	return;
+}
+
+static ce_task_desc_t *ce_alloc_task(void)
+{
+	dma_addr_t task_phy_addr;
+	ce_task_desc_t *task;
+
+	task = dma_pool_zalloc(ce_cdev->task_pool, GFP_KERNEL, &task_phy_addr);
+	if (task == NULL) {
+		SS_ERR("Failed to alloc for task\n");
+		return NULL;
+	} else {
+		task->next_virt = NULL;
+		task->task_phy_addr = task_phy_addr;
+		SS_DBG("task = 0x%px task_phy = 0x%px\n", task, (void *)task_phy_addr);
+	}
+
+	return task;
+}
+
+static void ce_task_destroy(ce_task_desc_t *task)
+{
+	ce_task_desc_t *prev;
+
+	while (task != NULL) {
+		prev = task;
+		task = task->next_virt;
+		SS_DBG("prev = 0x%px, prev_phy = 0x%px\n", prev, (void *)prev->task_phy_addr);
+		dma_pool_free(ce_cdev->task_pool, prev, prev->task_phy_addr);
+	}
+	return;
+}
+
+static int ce_task_data_init(crypto_aes_req_ctx_t *req, phys_addr_t src_phy,
+						phys_addr_t dst_phy, u32 length, ce_task_desc_t *task)
+{
+	u32 block_size = 127 * 1024;
+	u32 block_size_word = (block_size >> 2);
+	u32 block_num, alloc_flag = 0;
+	u32 last_data_len, last_size;
+	u32 data_len_offset = 0;
+	u32 i = 0, n;
+	dma_addr_t ptask_phy;
+	dma_addr_t tmp_addr;
+	dma_addr_t next_iv_phy;
+	ce_task_desc_t *ptask = task, *prev;
+
+	block_num = length / block_size;
+	last_size = length % block_size;
+	ptask->data_len = 0;
+	SS_DBG("total_len = 0x%x block_num =%d last_size =%d\n", length, block_num, last_size);
+	while (length) {
+
+		if (alloc_flag) {
+			ptask = dma_pool_zalloc(ce_cdev->task_pool, GFP_KERNEL, &ptask_phy);
+			if (ptask == NULL) {
+				SS_ERR("Failed to alloc for ptask\n");
+				return -ENOMEM;
+			}
+			ptask->chan_id  = prev->chan_id;
+			ptask->comm_ctl = prev->comm_ctl;
+			ptask->sym_ctl  = prev->sym_ctl;
+			ptask->asym_ctl = prev->asym_ctl;
+			ce_task_addr_set(NULL, ce_task_addr_get(prev->key_addr), ptask->key_addr);
+			ce_task_addr_set(NULL, ce_task_addr_get(prev->iv_addr), ptask->iv_addr);
+			ptask->data_len = 0;
+			ce_task_addr_set(NULL, ce_task_addr_get(prev->next_task_addr), ptask->iv_addr);
+			//prev->next = (ce_task_desc_t *)(ptask_phy >> WORD_ALGIN);
+			prev->next_virt = ptask;
+			ptask->task_phy_addr = ptask_phy;
+
+			SS_DBG("ptask = 0x%px, ptask_phy = 0x%px\n", ptask, (void *)ptask_phy);
+
+			if (SS_AES_MODE_CBC == req->aes_mode) {
+				req->iv_phy = next_iv_phy;
+				task_iv_init(req, ptask, NO_DMA_MAP);
+			}
+			i = 0;
+		}
+
+		if (block_num) {
+			n = (block_num > 8) ? CE_SCATTERS_PER_TASK : block_num;
+			for (i = 0; i < n; i++) {
+				ce_task_addr_set(0, (src_phy + data_len_offset), ptask->ce_sg[i].src_addr);
+				ptask->ce_sg[i].src_len = block_size_word;
+				ce_task_addr_set(0, (dst_phy + data_len_offset), ptask->ce_sg[i].dst_addr);
+				ptask->ce_sg[i].dst_len = block_size_word;
+				ptask->data_len += block_size;
+				data_len_offset += block_size;
+			}
+			block_num = block_num - n;
+		}
+
+		SS_DBG("block_num =%d i =%d\n", block_num, i);
+
+		/*the last no engure block size*/
+		if ((block_num == 0) && (last_size == 0)) {	/*block size aglin */
+			ptask->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+			alloc_flag = 0;
+			//ptask->next = NULL;
+			break;
+		} else if ((block_num == 0) && (last_size != 0)) {
+			SS_DBG("last_size =%d data_len_offset= %d\n", last_size, data_len_offset);
+			/* not block size aglin */
+			if ((i < CE_SCATTERS_PER_TASK) && (data_len_offset < length)) {
+				last_data_len = length - data_len_offset;
+				ce_task_addr_set(0, (src_phy + data_len_offset), ptask->ce_sg[i].src_addr);
+				ptask->ce_sg[i].src_len = (last_data_len >> 2);
+				ce_task_addr_set(0, (dst_phy + data_len_offset), ptask->ce_sg[i].dst_addr);
+				ptask->ce_sg[i].dst_len = (last_data_len >> 2);
+				ptask->data_len += last_data_len;
+				ptask->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+				//ptask->next = NULL;
+				break;
+			}
+		}
+
+		if (req->dir == SS_DIR_ENCRYPT) {
+			tmp_addr = ce_task_addr_get(ptask->ce_sg[7].dst_addr);
+			tmp_addr = tmp_addr + (ptask->ce_sg[i].dst_len << 2) - 16;
+			ce_task_addr_set(0, tmp_addr, (u8 *)next_iv_phy);
+			//next_iv_phy = ptask->dst[7].addr + (ptask->dst[7].len << 2) - 16;
+		} else {
+			tmp_addr = ce_task_addr_get(ptask->ce_sg[7].src_addr);
+			tmp_addr = tmp_addr + (ptask->ce_sg[7].src_len << 2) - 16;
+			ce_task_addr_set(0, tmp_addr, (u8 *)next_iv_phy);
+			//next_iv_phy = ptask->src[7].addr + (ptask->src[7].len << 2) - 16;
+		}
+		alloc_flag = 1;
+		prev = ptask;
+
+	}
+	return 0;
+}
+
+static int aes_crypto_start(crypto_aes_req_ctx_t *req, u8 *src_buffer,
+							u32 src_length, u8 *dst_buffer)
+{
+	int ret = 0;
+	int channel_id = req->channel_id;
+	u32 padding_flag = ce_cdev->flows[req->channel_id].buf_pendding;
+	phys_addr_t key_phy = 0;
+	phys_addr_t src_phy = 0;
+	phys_addr_t dst_phy = 0;
+	ce_task_desc_t *task = NULL;
+
+	task = ce_alloc_task();
+	if (!task) {
+		return -1;
+	}
+
+	/*task_mode_set*/
+	ce_aes_config(req, task);
+
+	/*task_key_set*/
+	if (req->key_length) {
+		key_phy = dma_map_single(ce_cdev->pdevice,
+					req->key_buffer, req->key_length, DMA_MEM_TO_DEV);
+		SS_DBG("key = 0x%px, key_phy_addr = 0x%px\n", req->key_buffer, (void *)key_phy);
+		ss_key_set(req->key_buffer, req->key_length, task);
+	}
+
+	SS_DBG("ion_flag = %d padding_flag =%d", req->ion_flag, padding_flag);
+	/*task_iv_set*/
+	if (req->ion_flag && padding_flag) {
+		task_iv_init(req, task, NO_DMA_MAP);
+	} else {
+		task_iv_init(req, task, DMA_MEM_TO_DEV);
+	}
+
+	/*task_data_set*/
+	/*only the last src_buf is malloc*/
+	if (req->ion_flag && (!padding_flag)) {
+		src_phy = req->src_phy;
+	} else {
+		src_phy = dma_map_single(ce_cdev->pdevice, src_buffer, src_length, DMA_MEM_TO_DEV);
+	}
+	SS_DBG("src = 0x%px, src_phy_addr = 0x%px\n", src_buffer, (void *)src_phy);
+
+	/*the dst_buf is from user*/
+	if (req->ion_flag) {
+		dst_phy = req->dst_phy;
+	} else {
+		dst_phy = dma_map_single(ce_cdev->pdevice, dst_buffer, src_length, DMA_MEM_TO_DEV);
+	}
+	SS_DBG("dst = 0x%px, dst_phy_addr = 0x%px\n", dst_buffer, (void *)dst_phy);
+
+	ce_task_data_init(req, src_phy, dst_phy, src_length, task);
+	/*ce_print_task_info(task);*/
+
+	/*start ce*/
+	ss_pending_clear(channel_id);
+	ss_irq_enable(channel_id);
+
+	init_completion(&ce_cdev->flows[channel_id].done);
+	ss_ctrl_start(task, SS_METHOD_AES, req->aes_mode);
+	/*ce_reg_print();*/
+
+
+	ret = wait_for_completion_timeout(&ce_cdev->flows[channel_id].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ce_task_destroy(task);
+		ce_reset();
+		ret = -ETIMEDOUT;
+	}
+
+	ss_irq_disable(channel_id);
+	ce_task_destroy(task);
+
+	/*key*/
+	if (req->key_length) {
+		dma_unmap_single(ce_cdev->pdevice,
+			key_phy, req->key_length, DMA_DEV_TO_MEM);
+	}
+
+	/*iv*/
+	if (req->ion_flag && padding_flag) {
+		;
+	} else {
+		task_iv_init(req, task, DMA_DEV_TO_MEM);
+	}
+
+	/*data*/
+	if (req->ion_flag && (!padding_flag)) {
+		;
+	} else {
+		dma_unmap_single(ce_cdev->pdevice,
+				src_phy, src_length, DMA_DEV_TO_MEM);
+	}
+
+	if (req->ion_flag) {
+		;
+	} else {
+		dma_unmap_single(ce_cdev->pdevice,
+				dst_phy, src_length, DMA_DEV_TO_MEM);
+	}
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+
+	if (ss_flow_err(channel_id)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(channel_id));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int do_aes_crypto(crypto_aes_req_ctx_t *req_ctx)
+{
+	u32 last_block_size = 0;
+	u32 block_num = 0;
+	u32 padding_size = 0;
+	u32 first_encypt_size = 0;
+	u8 data_block[AES_BLOCK_SIZE];
+	int channel_id = req_ctx->channel_id;
+	int ret;
+
+	ret = check_aes_ctx_vaild(req_ctx);
+	if (ret) {
+		return -1;
+	}
+
+	memset(data_block, 0x0, AES_BLOCK_SIZE);
+	ce_cdev->flows[channel_id].buf_pendding = 0;
+
+	if (req_ctx->dir == SS_DIR_DECRYPT) {
+		ret = aes_crypto_start(req_ctx, req_ctx->src_buffer,
+								req_ctx->src_length, req_ctx->dst_buffer);
+		if (ret) {
+			SS_ERR("aes decrypt fail\n");
+			return -2;
+		}
+		req_ctx->dst_length = req_ctx->src_length;
+	} else {
+		block_num = req_ctx->src_length / AES_BLOCK_SIZE;
+		last_block_size = req_ctx->src_length % AES_BLOCK_SIZE;
+		padding_size = AES_BLOCK_SIZE - last_block_size;
+
+		if (block_num > 0) {
+			SS_DBG("block_num = %d\n", block_num);
+			first_encypt_size = block_num * AES_BLOCK_SIZE;
+			SS_DBG("src_phy = 0x%lx, dst_phy = 0x%lx\n", req_ctx->src_phy, req_ctx->dst_phy);
+			ret = aes_crypto_start(req_ctx, req_ctx->src_buffer,
+						first_encypt_size,
+						req_ctx->dst_buffer
+						);
+			if (ret) {
+				SS_ERR("aes encrypt fail\n");
+				return -2;
+			}
+			req_ctx->dst_length = block_num * AES_BLOCK_SIZE;
+			/*not align 16byte*/
+			if (last_block_size) {
+				SS_DBG("last_block_size = %d\n", last_block_size);
+				SS_DBG("padding_size = %d\n", padding_size);
+				ce_cdev->flows[channel_id].buf_pendding = padding_size;
+				if (req_ctx->ion_flag) {
+					SS_ERR("ion memery must be 16 byte algin\n");
+				} else {
+					memcpy(data_block, req_ctx->src_buffer + first_encypt_size, last_block_size);
+					memset(data_block + last_block_size, padding_size, padding_size);
+				}
+				if (SS_AES_MODE_CBC == req_ctx->aes_mode) {
+					if (req_ctx->ion_flag) {
+						req_ctx->iv_phy = req_ctx->dst_phy + first_encypt_size - AES_BLOCK_SIZE;
+					} else {
+						req_ctx->iv_buf = req_ctx->dst_buffer + first_encypt_size - AES_BLOCK_SIZE;
+					}
+				}
+
+				ret = aes_crypto_start(req_ctx, data_block, AES_BLOCK_SIZE,
+										req_ctx->dst_buffer + first_encypt_size
+										);
+				if (ret) {
+					SS_ERR("aes encrypt fail\n");
+					return -2;
+				}
+				req_ctx->dst_length = req_ctx->dst_length + AES_BLOCK_SIZE;
+			}
+		} else {
+			SS_DBG("padding_size = %d\n", padding_size);
+			ce_cdev->flows[channel_id].buf_pendding = padding_size;
+			if (req_ctx->ion_flag) {
+				SS_ERR("ion memery must be 16 byte algin\n");
+			} else {
+				memcpy(data_block, req_ctx->src_buffer, req_ctx->src_length);
+				memset(data_block + last_block_size, padding_size, padding_size);
+			}
+			ret = aes_crypto_start(req_ctx, data_block, AES_BLOCK_SIZE,
+									req_ctx->dst_buffer
+									);
+			if (ret) {
+				SS_ERR("aes encrypt fail\n");
+				return -2;
+			}
+
+			req_ctx->dst_length = (block_num + 1) * AES_BLOCK_SIZE;
+		}
+	}
+	SS_ERR("do_aes_crypto sucess\n");
+	return 0;
+}
+
+
diff --git a/drivers/crypto/sunxi-ce/v5/sunxi_ce_proc.c b/drivers/crypto/sunxi-ce/v5/sunxi_ce_proc.c
new file mode 100644
index 000000000..059b6969d
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v5/sunxi_ce_proc.c
@@ -0,0 +1,1565 @@
+/*
+ * The driver of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2018 Allwinner.
+ *
+ * <xupengliu@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+#include <linux/highmem.h>
+#include <linux/dmaengine.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/rng.h>
+#include <crypto/des.h>
+
+#include <crypto/aead.h>
+#include <crypto/internal/aead.h>
+
+#include "../sunxi_ce.h"
+#include "../sunxi_ce_proc.h"
+#include "sunxi_ce_reg.h"
+
+void ce_print_new_task_desc(ce_new_task_desc_t *task)
+{
+	int i;
+	u64 phy_addr;
+
+#ifndef SUNXI_CE_DEBUG
+	return;
+#endif
+
+	printk("---------------------task_info--------------------\n");
+	printk("task->comm_ctl = 0x%x\n", task->comm_ctl);
+	printk("task->main_cmd = 0x%x\n", task->main_cmd);
+	printk("task->data_len = 0x%llx\n", (u64)ce_task_addr_get(task->data_len));
+	printk("task->key_addr = 0x%llx\n", (u64)ce_task_addr_get(task->key_addr));
+	printk("task->iv_addr = 0x%llx\n", (u64)ce_task_addr_get(task->iv_addr));
+
+	for (i = 0; i < 8; i++) {
+		phy_addr = (u64)ce_task_addr_get(task->ce_sg[i].src_addr);
+		if (phy_addr) {
+			printk("task->src[%d].addr = 0x%llx\n", i, phy_addr);
+			printk("task->src[%d].len = 0x%x\n", i, task->ce_sg[i].src_len);
+		}
+	}
+
+	for (i = 0; i < 8; i++) {
+		phy_addr = (u64)ce_task_addr_get(task->ce_sg[i].dst_addr);
+		if (phy_addr) {
+			printk("task->dst[%d].addr = 0x%llx\n", i, phy_addr);
+			printk("task->dst[%d].len = 0x%x\n", i, task->ce_sg[i].dst_len);
+		}
+	}
+	printk("task->task_phy_addr = 0x%llx\n", (u64)task->task_phy_addr);
+}
+
+void ce_print_task_desc(ce_task_desc_t *task)
+{
+	int i;
+	u64 phy_addr;
+
+#ifndef SUNXI_CE_DEBUG
+		return;
+#endif
+
+	printk("---------------------task_info--------------------\n");
+	printk("task->comm_ctl = 0x%x\n", task->comm_ctl);
+	printk("task->sym_ctl = 0x%x\n", task->sym_ctl);
+	printk("task->asym_ctl = 0x%x\n", task->asym_ctl);
+	printk("task->key_addr = 0x%llx\n", (u64)ce_task_addr_get(task->key_addr));
+	printk("task->iv_addr = 0x%llx\n", (u64)ce_task_addr_get(task->iv_addr));
+	printk("task->ctr_addr = 0x%llx\n", (u64)ce_task_addr_get(task->ctr_addr));
+	printk("task->data_len = 0x%x\n", task->data_len);
+
+
+	for (i = 0; i < 8; i++) {
+		phy_addr = (u64)ce_task_addr_get(task->ce_sg[i].src_addr);
+		if (phy_addr) {
+			printk("task->src[%d].addr = 0x%llx\n", i, phy_addr);
+			printk("task->src[%d].len = 0x%x\n", i, task->ce_sg[i].src_len);
+		}
+	}
+
+	for (i = 0; i < 8; i++) {
+		phy_addr = (u64)ce_task_addr_get(task->ce_sg[i].dst_addr);
+		if (phy_addr) {
+			printk("task->dst[%d].addr = 0x%llx\n", i, phy_addr);
+			printk("task->dst[%d].len = 0x%x\n", i, task->ce_sg[i].dst_len);
+		}
+	}
+	printk("task->task_phy_addr = 0x%llx\n", (u64)task->task_phy_addr);
+}
+
+/* ss_new_task_desc_init used for hash/rng alg */
+void ss_new_task_desc_init(ce_new_task_desc_t *task, u32 flow)
+{
+	memset(task, 0, sizeof(ce_new_task_desc_t));
+
+	task->comm_ctl |= (flow << CE_CTL_CHAN_MASK);
+	task->comm_ctl |= CE_CTL_IE_MASK;
+}
+
+void ss_task_desc_init(ce_task_desc_t *task, u32 flow)
+{
+	memset(task, 0, sizeof(ce_task_desc_t));
+	task->chan_id = flow;
+	task->comm_ctl |= CE_COMM_CTL_TASK_INT_MASK;
+}
+
+static int ss_sg_len(struct scatterlist *sg, int total)
+{
+	int nbyte = 0;
+	struct scatterlist *cur = sg;
+
+	while (cur != NULL) {
+		SS_DBG("cur: 0x%px, len: %d, is_last: %ld\n",
+			cur, cur->length, sg_is_last(cur));
+		nbyte += cur->length;
+
+		cur = sg_next(cur);
+	}
+
+	return nbyte;
+}
+
+static int ss_aes_align_size(int type, int mode)
+{
+	if ((type == SS_METHOD_ECC))
+		return 4;
+	else if ((CE_IS_AES_MODE(type, mode, CTS))
+			|| (CE_IS_AES_MODE(type, mode, CFB))
+			|| (CE_IS_AES_MODE(type, mode, XTS))
+			|| (CE_METHOD_IS_HMAC(type)))
+		return 1;
+	else if ((type == SS_METHOD_DES) || (type == SS_METHOD_3DES))
+		return DES_BLOCK_SIZE;
+	else
+		return AES_BLOCK_SIZE;
+}
+
+static int ss_copy_from_user(void *to, struct scatterlist *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(from);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg 0x%px (%d).\n", from, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%px, sg_addr = 0x%px, size = %d\n", vaddr, from, size);
+	memcpy(to, vaddr + from->offset, size);
+	kunmap(ppage);
+	return 0;
+}
+
+static int ss_copy_to_user(struct scatterlist *to, void *from, u32 size)
+{
+	void *vaddr = NULL;
+	struct page *ppage = sg_page(to);
+
+	vaddr = kmap(ppage);
+	if (vaddr == NULL) {
+		WARN(1, "Fail to map the last sg: 0x%px (%d).\n", to, size);
+		return -1;
+	}
+
+	SS_DBG("vaddr = 0x%px, sg_addr = 0x%px, size = %d\n", vaddr, to, size);
+	memcpy(vaddr+to->offset, from, size);
+	kunmap(ppage);
+	return 0;
+}
+static int ss_aead_sg_config(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int type, int mode, int tail, u32 flags)
+{
+	int cnt = 0;
+	int last_sg_len = 0;
+	struct scatterlist *cur = info->sg;
+	phys_addr_t addr_phy;
+
+	while (cur != NULL) {
+		if (cnt >= CE_SCATTERS_PER_TASK-1) {
+			WARN(1, "Too many scatter: %d\n", cnt);
+			return -1;
+		}
+
+		addr_phy = sg_dma_address(cur);
+		if (flags == SRC_FLAG) {
+			ce_task_addr_set(0, addr_phy, scatter[cnt + 1].src_addr);
+			scatter[cnt + 1].src_len = sg_dma_len(cur);
+		} else {
+			ce_task_addr_set(0, addr_phy, scatter[cnt + 1].dst_addr);
+			scatter[cnt + 1].dst_len = sg_dma_len(cur);
+		}
+
+		info->last_sg = cur;
+		last_sg_len = sg_dma_len(cur);
+		SS_DBG("%d cur: 0x%p, scatter: addr 0x%px, len %d\n",
+				cnt, cur, (void *)addr_phy, sg_dma_len(cur));
+		cnt++;
+		cur = sg_next(cur);
+	}
+
+	info->nents = cnt;
+	if (tail == 0) {
+		info->has_padding = 0;
+		return 0;
+	}
+
+	/* CTS/CTR/CFB/OFB need algin with word/block, so replace the last sg.*/
+
+	last_sg_len += ss_aes_align_size(0, mode) - tail;
+	info->padding = kzalloc(last_sg_len, GFP_KERNEL);
+	if (info->padding == NULL) {
+		SS_ERR("Failed to kmalloc(%d)!\n", last_sg_len);
+		return -ENOMEM;
+	}
+	SS_DBG("AES(%d)-%d padding: 0x%p, tail = %d/%d, cnt = %d\n",
+		type, mode, info->padding, tail, last_sg_len, cnt);
+
+	ss_copy_from_user(info->padding,
+		info->last_sg, last_sg_len - ss_aes_align_size(0, mode) + tail);
+
+	if (flags == SRC_FLAG) {
+		ce_task_addr_set(0, virt_to_phys(info->padding), scatter[cnt].src_addr);
+		scatter[cnt].src_len = last_sg_len;
+	} else {
+		ce_task_addr_set(0, virt_to_phys(info->padding), scatter[cnt].dst_addr);
+		scatter[cnt].dst_len = last_sg_len;
+	}
+
+	info->has_padding = 1;
+	return 0;
+}
+
+static int ss_sg_config(ce_scatter_t *scatter, ss_dma_info_t *info,
+						int type, int mode, int tail, int hash_type, u32 flags)
+{
+	int cnt = 0;
+	int last_sg_len = 0;
+	struct scatterlist *cur = info->sg;
+	phys_addr_t addr_phy;
+
+	while (cur != NULL) {
+		if (cnt >= CE_SCATTERS_PER_TASK) {
+			WARN(1, "Too many scatter: %d\n", cnt);
+			return -1;
+		}
+
+		addr_phy = sg_dma_address(cur);
+		if (flags == SRC_FLAG) {
+			ce_task_addr_set(0, addr_phy, scatter[cnt].src_addr);
+			scatter[cnt].src_len = sg_dma_len(cur);
+		} else {
+			ce_task_addr_set(0, addr_phy, scatter[cnt].dst_addr);
+			scatter[cnt].dst_len = sg_dma_len(cur);
+		}
+		info->last_sg = cur;
+		last_sg_len = sg_dma_len(cur);
+		SS_DBG("%d cur: 0x%px, scatter: addr 0x%llx, len %d\n",
+				cnt, cur, (u64)addr_phy, sg_dma_len(cur));
+		cnt++;
+		cur = sg_next(cur);
+	}
+
+#ifdef SS_HASH_HW_PADDING
+		if (CE_METHOD_IS_HMAC(type)) {
+			if (flags == SRC_FLAG) {
+				scatter[cnt-1].src_len += tail;
+			} else {
+				scatter[cnt-1].dst_len += tail;
+			}
+			info->has_padding = 0;
+			return 0;
+		}
+#endif
+
+	info->nents = cnt;
+	if (tail == 0) {
+		info->has_padding = 0;
+		return 0;
+	}
+
+	if (hash_type) {
+		if (flags == SRC_FLAG) {
+			scatter[cnt-1].src_len -= tail;
+		} else {
+			scatter[cnt-1].dst_len -= tail;
+		}
+		return 0;
+	}
+
+	/* CTS/CTR/CFB/OFB need algin with word/block, so replace the last sg.*/
+	last_sg_len += ss_aes_align_size(0, mode) - tail;
+	info->padding = kzalloc(last_sg_len, GFP_KERNEL);
+	if (info->padding == NULL) {
+		SS_ERR("Failed to kmalloc(%d)!\n", last_sg_len);
+		return -ENOMEM;
+	}
+	SS_DBG("AES(%d)-%d padding: 0x%px, tail = %d/%d, cnt = %d\n",
+		type, mode, info->padding, tail, last_sg_len, cnt);
+
+	ss_copy_from_user(info->padding,
+		info->last_sg, last_sg_len - ss_aes_align_size(0, mode) + tail);
+
+	if (flags == SRC_FLAG) {
+		ce_task_addr_set(0, virt_to_phys(info->padding), scatter[cnt-1].src_addr);
+		scatter[cnt-1].src_len = last_sg_len;
+	} else {
+		ce_task_addr_set(0, virt_to_phys(info->padding), scatter[cnt-1].dst_addr);
+		scatter[cnt-1].dst_len = last_sg_len;
+	}
+
+	info->has_padding = 1;
+	return 0;
+}
+
+static void ss_aes_unpadding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int tail, u32 flags)
+{
+	int last_sg_len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	/* Only the dst sg need to be recovered. */
+	if (info->dir == DMA_DEV_TO_MEM) {
+		if (flags == SRC_FLAG) {
+			last_sg_len = scatter[index].src_len;
+		} else {
+			last_sg_len = scatter[index].dst_len;
+		}
+		last_sg_len -= ss_aes_align_size(0, mode) - tail;
+		ss_copy_to_user(info->last_sg, info->padding, last_sg_len);
+	}
+
+	kfree(info->padding);
+	info->padding = NULL;
+	info->has_padding = 0;
+}
+
+static void ss_aes_map_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir, u32 flags)
+{
+	int len = 0;
+	int index = info->nents - 1;
+
+	if (info->has_padding == 0)
+		return;
+
+	if (flags == SRC_FLAG) {
+		len = scatter[index].src_len;
+	} else {
+		len = scatter[index].dst_len;
+	}
+
+	SS_DBG("AES padding: 0x%p, len: %d, dir: %d\n",
+		info->padding, len, dir);
+
+	/* task address in words, so phys-to-virt need to change*/
+	dma_map_single(&ss_dev->pdev->dev, info->padding, len, dir);
+	info->dir = dir;
+}
+
+static void ss_aead_map_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir, u32 flags)
+{
+	int len = 0;
+	int index = info->nents;
+
+	if (info->has_padding == 0)
+		return;
+
+	if (flags == SRC_FLAG) {
+		len = scatter[index].src_len;
+	} else {
+		len = scatter[index].dst_len;
+	}
+
+	SS_DBG("AES padding: 0x%p, len: %d, dir: %d\n",
+		info->padding, len, dir);
+
+	/* task address in words, so phys-to-virt need to change*/
+	dma_map_single(&ss_dev->pdev->dev, info->padding, len, dir);
+	info->dir = dir;
+}
+
+static void ss_aes_unmap_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir, u32 flags)
+{
+	int len = 0;
+	int index = info->nents - 1;
+	phys_addr_t phy_addr;
+
+	if (info->has_padding == 0)
+		return;
+
+	if (flags == SRC_FLAG) {
+		len = scatter[index].src_len;
+		phy_addr = ce_task_addr_get(scatter[index].src_addr);
+	} else {
+		len = scatter[index].dst_len;
+		phy_addr = ce_task_addr_get(scatter[index].dst_addr);
+	}
+
+	SS_DBG("AES padding: 0x%px, len: %d, dir: %d\n", (void *)phy_addr, len, dir);
+	dma_unmap_single(&ss_dev->pdev->dev, phy_addr, len, dir);
+}
+static void ss_aead_unmap_padding(ce_scatter_t *scatter,
+	ss_dma_info_t *info, int mode, int dir, u32 flags)
+{
+	int len = 0;
+	int index = info->nents;
+	phys_addr_t phy_addr;
+
+	if (info->has_padding == 0)
+		return;
+
+	if (flags == SRC_FLAG) {
+		len = scatter[index].src_len;
+		phy_addr = ce_task_addr_get(scatter[index].src_addr);
+	} else {
+		len = scatter[index].dst_len;
+		phy_addr = ce_task_addr_get(scatter[index].dst_addr);
+	}
+
+	SS_DBG("AES padding: 0x%px, len: %d, dir: %d\n",
+		(void *)phy_addr, len, dir);
+	dma_unmap_single(&ss_dev->pdev->dev, phy_addr, len, dir);
+}
+
+void ss_change_clk(int type)
+{
+#ifdef SS_RSA_CLK_ENABLE
+	if ((type == SS_METHOD_RSA) || (type == SS_METHOD_ECC))
+		ss_clk_set(ss_dev->rsa_clkrate);
+	else
+		ss_clk_set(ss_dev->gen_clkrate);
+#endif
+}
+
+void ss_hash_rng_change_clk(void)
+{
+#ifdef SS_RSA_CLK_ENABLE
+		ss_clk_set(ss_dev->gen_clkrate);
+#endif
+}
+
+static int ss_hmac_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int i = 0;
+	int src_len = len;
+	int align_size;
+	int flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	ss_hash_rng_change_clk();
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	if (CE_METHOD_IS_HMAC(req_ctx->type) && (req_ctx->type == SS_METHOD_HMAC_SHA1))
+		ss_hmac_method_set(SS_METHOD_SHA1, task);
+	else if (CE_METHOD_IS_HMAC(req_ctx->type) && (req_ctx->type == SS_METHOD_HMAC_SHA256))
+		ss_hmac_method_set(SS_METHOD_SHA256, task);
+	else
+		ss_hash_method_set(req_ctx->type, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d / %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = 0x%llx\n", ctx->key, (u64)phy_addr);
+	phy_addr = virt_to_phys(ctx->iv);
+	SS_DBG("ctx->iv addr, vir = 0x%px, phy = 0x%llx\n", ctx->iv, (u64)phy_addr);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%llx\n", task, (u64)phy_addr);
+
+	ss_rng_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, src_len);
+	src_len = ss_sg_len(req_ctx->dma_src.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->ce_sg, &req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, src_len%align_size, 1, SRC_FLAG);
+	ss_aes_map_padding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV, SRC_FLAG);
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->ce_sg, &req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, len%align_size, 1, DST_FLAG);
+	ss_aes_map_padding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM, DST_FLAG);
+
+	/* data_len set and last_flag set */
+	ss_hmac_sha1_last(task);
+	ss_hash_data_len_set((src_len - SHA256_BLOCK_SIZE) * 8, task);
+
+	/*  for openssl add SHA256_BLOCK_SIZE after data*/
+	task->ce_sg[0].src_len = task->ce_sg[0].src_len - SHA256_BLOCK_SIZE;
+#if 0
+	/* addr should set in word, src_len and dst_len set in bytes */
+	for (i = 0; i < 8; i++) {
+		task->ce_sg[i].dst_len = (task->ce_sg[i].dst_len);
+	}
+#endif
+
+	ce_print_new_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+			sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, len % align_size, DST_FLAG);
+	ss_aes_unmap_padding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM, DST_FLAG);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, src_len % align_size, SRC_FLAG);
+	ss_aes_unmap_padding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV, SRC_FLAG);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	return 0;
+}
+static int ss_aead_start(ss_aead_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx)
+{
+	int ret = 0;
+	int gcm_iv_mode;
+	int i = 0;
+	int in_len = ctx->cryptlen;
+	int data_len;
+	int align_size = 0;
+	u32 flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+	int more;
+
+	ss_change_clk(req_ctx->type);
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	ss_aead_mode_set(req_ctx->mode, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, in_len);
+
+	/* hardware gcm may be only support continus data, so maybe need software to fix it. */
+	/* set iv_addr for task descriptor */
+	memset(ctx->task_iv, 0, sizeof(ctx->task_iv));
+	if (req_ctx->dir == SS_DIR_DECRYPT) {
+		if (!ctx->assoclen) {
+			strncpy(ctx->tag, ((char *)sg_dma_address(req_ctx->dma_src.sg) +
+					(sg_dma_len(req_ctx->dma_src.sg) - ctx->tag_len)), ctx->tag_len);
+		} else {
+			strncpy(ctx->tag, ((char *)sg_dma_address(sg_next(req_ctx->dma_src.sg)) +
+					(sg_dma_len(sg_next(req_ctx->dma_src.sg)) - ctx->tag_len)), ctx->tag_len);
+		}
+		for (i = TAG_START; i < ctx->tag_len + TAG_START; i++) {
+			ctx->task_iv[i] = ctx->tag[i-TAG_START];  /* only decrypt need */
+		}
+	}
+
+	ctx->task_iv[IV_SIZE_START] = (ctx->iv_size * 8) & 0xff;
+	ctx->task_iv[IV_SIZE_START+1] = ((ctx->iv_size * 8)>>8) & 0xff;
+	ctx->task_iv[IV_SIZE_START+2] = ((ctx->iv_size * 8)>>16) & 0xff;
+	ctx->task_iv[IV_SIZE_START+3] = ((ctx->iv_size * 8)>>24) & 0xff;
+
+	ctx->task_iv[AAD_SIZE_START] = (ctx->assoclen * 8) & 0xff;
+	ctx->task_iv[AAD_SIZE_START+1] = ((ctx->assoclen * 8)>>8) & 0xff;
+	ctx->task_iv[AAD_SIZE_START+2] = ((ctx->assoclen * 8)>>16) & 0xff;
+	ctx->task_iv[AAD_SIZE_START+3] = ((ctx->assoclen * 8)>>24) & 0xff;
+
+	ctx->task_iv[PT_SIZE_START] = (ctx->cryptlen * 8) & 0xff;
+	ctx->task_iv[PT_SIZE_START+1] = ((ctx->cryptlen * 8)>>8) & 0xff;
+	ctx->task_iv[PT_SIZE_START+2] = ((ctx->cryptlen * 8)>>16) & 0xff;
+	ctx->task_iv[PT_SIZE_START+3] = ((ctx->cryptlen * 8)>>24) & 0xff;
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = %pa\n", ctx->key, &phy_addr);
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	phy_addr = virt_to_phys(ctx->task_iv);
+	SS_DBG("ctx->task_iv vir = 0x%px, phy = 0x%pa\n", ctx->task_iv, &phy_addr);
+	ss_iv_set(ctx->task_iv, sizeof(ctx->task_iv), task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->task_iv, sizeof(ctx->task_iv), DMA_MEM_TO_DEV);
+
+	phy_addr = virt_to_phys(ctx->task_ctr);
+	SS_DBG("ctx->task_ctr vir = 0x%px, phy = 0x%pa\n", ctx->task_ctr, &phy_addr);
+	ss_gcm_cnt_set(ctx->task_ctr, sizeof(ctx->task_ctr), task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->task_ctr, sizeof(ctx->task_ctr), DMA_DEV_TO_MEM);
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, in_len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	phy_addr = virt_to_phys(ctx->iv);
+	ss_gcm_src_config(&(task->ce_sg[0]), phy_addr,
+			DIV_ROUND_UP(ctx->iv_size, align_size)*align_size);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->iv, sizeof(ctx->iv_size), DMA_DEV_TO_MEM);
+
+	ss_aead_sg_config(task->ce_sg, &req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, (in_len % align_size), SRC_FLAG);
+	ss_aead_map_padding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV, SRC_FLAG);
+
+	if (req_ctx->dir == SS_DIR_DECRYPT) {
+		task->ce_sg[req_ctx->dma_src.nents].src_len =
+			((task->ce_sg[req_ctx->dma_src.nents].src_len << 2) - ctx->tag_len);
+	}
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, in_len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->ce_sg,	&req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, in_len % align_size, 0, DST_FLAG);
+	ss_aes_map_padding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM, DST_FLAG);
+
+	ss_tag_len_set((ctx->tag_len) * 8, task);
+	if (ctx->iv_size == 12)
+		gcm_iv_mode = 1;
+	else
+		gcm_iv_mode = 3;
+	ss_gcm_iv_mode(task, gcm_iv_mode);
+	ss_cts_last(task);
+	more = (req_ctx->dir) ? 0 : DIV_ROUND_UP(ctx->tag_len, align_size) * align_size;
+	ss_gcm_reserve_set(task, ctx->iv_size, ctx->assoclen, in_len);
+	data_len = (DIV_ROUND_UP(in_len, align_size)*align_size +
+			DIV_ROUND_UP(ctx->iv_size, align_size)*align_size +
+			DIV_ROUND_UP(ctx->assoclen, align_size)*align_size + more);
+	ss_data_len_set(data_len, task);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task, sizeof(ce_task_desc_t),
+		DMA_MEM_TO_DEV);
+
+	SS_DBG("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x, data_len:%d\n",
+		task->comm_ctl, task->sym_ctl, task->asym_ctl, task->data_len);
+
+	ss_ctrl_start(task, req_ctx->type, req_ctx->mode);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, in_len % align_size, DST_FLAG);
+	ss_aes_unmap_padding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM, DST_FLAG);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, in_len % align_size, SRC_FLAG);
+	ss_aead_unmap_padding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV, SRC_FLAG);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->task_iv), sizeof(ctx->task_iv), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->iv),
+			ctx->iv_size, DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->task_ctr), sizeof(ctx->task_ctr), DMA_DEV_TO_MEM);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int ss_aes_start(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx, int len)
+{
+	int ret = 0;
+	int src_len = len;
+	int align_size = 0;
+	u32 flow = ctx->comm.flow;
+	phys_addr_t phy_addr = 0;
+	ce_task_desc_t *task = &ss_dev->flows[flow].task;
+
+	ss_change_clk(req_ctx->type);
+	ss_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+#ifdef SS_XTS_MODE_ENABLE
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_method_set(req_ctx->dir, SS_METHOD_RAES, task);
+	else
+#endif
+	ss_method_set(req_ctx->dir, req_ctx->type, task);
+
+	if ((req_ctx->type == SS_METHOD_RSA)
+		|| (req_ctx->type == SS_METHOD_DH)) {
+		ss_rsa_width_set(len, task);
+		ss_rsa_op_mode_set(req_ctx->mode, task);
+	} else if (req_ctx->type == SS_METHOD_ECC) {
+		ss_ecc_width_set(len >> 1, task);
+		ss_ecc_op_mode_set(req_ctx->mode, task);
+	} else
+		ss_aes_mode_set(req_ctx->mode, task);
+
+#ifdef SS_CFB_MODE_ENABLE
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode == SS_AES_MODE_CFB))
+		ss_cfb_bitwidth_set(req_ctx->bitwidth, task);
+#endif
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = %pa\n", ctx->key, &phy_addr);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+#ifdef SS_XTS_MODE_ENABLE
+	SS_DBG("The current Key:\n");
+	ss_print_hex(ctx->key, ctx->key_size, ctx->key);
+
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS))
+		ss_key_set(ctx->key, ctx->key_size/2, task);
+	else
+#endif
+	ss_key_set(ctx->key, ctx->key_size, task);
+	ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		phy_addr = virt_to_phys(ctx->iv);
+		SS_DBG("ctx->iv vir = 0x%px, phy = 0x%pa\n", ctx->iv, &phy_addr);
+		ss_iv_set(ctx->iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->iv, ctx->iv_size, DMA_MEM_TO_DEV);
+
+		phy_addr = virt_to_phys(ctx->next_iv);
+		SS_DBG("ctx->next_iv addr, vir = 0x%px, phy = 0x%pa\n",
+			ctx->next_iv, &phy_addr);
+		ss_cnt_set(ctx->next_iv, ctx->iv_size, task);
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->next_iv, ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+
+	align_size = ss_aes_align_size(req_ctx->type, req_ctx->mode);
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, src_len);
+	if ((req_ctx->type == SS_METHOD_ECC)
+		|| ((req_ctx->type == SS_METHOD_RSA) &&
+			(req_ctx->mode == CE_RSA_OP_M_MUL)))
+		src_len = ss_sg_len(req_ctx->dma_src.sg, len);
+
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->ce_sg,	&req_ctx->dma_src,
+		req_ctx->type, req_ctx->mode, src_len%align_size, 0, SRC_FLAG);
+	ss_aes_map_padding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV, SRC_FLAG);
+
+	/* Prepare the dst scatterlist */
+	req_ctx->dma_dst.nents = ss_sg_cnt(req_ctx->dma_dst.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+	ss_sg_config(task->ce_sg,	&req_ctx->dma_dst,
+		req_ctx->type, req_ctx->mode, len%align_size, 0, DST_FLAG);
+	ss_aes_map_padding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM, DST_FLAG);
+
+#ifdef SS_SUPPORT_CE_V3_1
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		ss_data_len_set(len, task);
+/*if (len < SZ_4K)  A bad way to determin the last packet of CTS mode. */
+			ss_cts_last(task);
+	} else
+		ss_data_len_set(
+			DIV_ROUND_UP(src_len, align_size)*align_size/4, task);
+#else
+	if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS)) {
+		/* A bad way to determin the last packet. */
+		/* if (len < SZ_4K) */
+		ss_cts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, XTS)) {
+		ss_xts_first(task);
+		ss_xts_last(task);
+		ss_data_len_set(src_len, task);
+	} else if (req_ctx->type == SS_METHOD_RSA)
+		ss_data_len_set(len * 3, task);
+	else
+		ss_data_len_set(DIV_ROUND_UP(src_len, align_size) * align_size, task);
+#endif
+
+	ce_print_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task, sizeof(ce_task_desc_t),
+		DMA_MEM_TO_DEV);
+
+	SS_DBG("preCE, COMM: 0x%08x, SYM: 0x%08x, ASYM: 0x%08x, data_len:%d\n",
+		task->comm_ctl, task->sym_ctl, task->asym_ctl, task->data_len);
+
+	ss_ctrl_start(task, req_ctx->type, req_ctx->mode);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_task_desc_t), DMA_MEM_TO_DEV);
+
+	/* Unpadding and unmap the dst sg. */
+	ss_aes_unpadding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, len % align_size, DST_FLAG);
+	ss_aes_unmap_padding(task->ce_sg,
+		&req_ctx->dma_dst, req_ctx->mode, DMA_DEV_TO_MEM, DST_FLAG);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_dst.sg, req_ctx->dma_dst.nents, DMA_DEV_TO_MEM);
+
+	/* Unpadding and unmap the src sg. */
+	ss_aes_unpadding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, src_len % align_size, SRC_FLAG);
+	ss_aes_unmap_padding(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->mode, DMA_MEM_TO_DEV, SRC_FLAG);
+	dma_unmap_sg(&ss_dev->pdev->dev,
+		req_ctx->dma_src.sg, req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+
+	if (ctx->iv_size > 0) {
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->iv),
+			ctx->iv_size, DMA_MEM_TO_DEV);
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->next_iv),
+			ctx->iv_size, DMA_DEV_TO_MEM);
+	}
+	/* Backup the next IV from ctr_descriptor, except CBC/CTS/XTS mode. */
+	if (CE_METHOD_IS_AES(req_ctx->type)
+		&& (req_ctx->mode != SS_AES_MODE_CBC)
+		&& (req_ctx->mode != SS_AES_MODE_CTS)
+		&& (req_ctx->mode != SS_AES_MODE_XTS))
+		memcpy(ctx->iv, ctx->next_iv, ctx->iv_size);
+
+	dma_unmap_single(&ss_dev->pdev->dev,
+		virt_to_phys(ctx->key), ctx->key_size, DMA_MEM_TO_DEV);
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* verify the key_len */
+int ss_aes_key_valid(struct crypto_ablkcipher *tfm, int len)
+{
+	if (unlikely(len > SS_RSA_MAX_SIZE)) {
+		SS_ERR("Unsupported key size: %d\n", len);
+		tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return 0;
+}
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+static void ss_rsa_preprocess(ss_aes_ctx_t *ctx,
+	ss_aes_req_ctx_t *req_ctx, int len)
+{
+	struct scatterlist sg = {0};
+	ss_aes_req_ctx_t *tmp_req_ctx = NULL;
+
+	if (!((req_ctx->type == SS_METHOD_RSA) &&
+		(req_ctx->mode != CE_RSA_OP_M_MUL)))
+		return;
+
+	tmp_req_ctx = kmalloc(sizeof(ss_aes_req_ctx_t), GFP_KERNEL);
+	if (tmp_req_ctx == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", sizeof(ss_aes_req_ctx_t));
+		return;
+	}
+
+	memcpy(tmp_req_ctx, req_ctx, sizeof(ss_aes_req_ctx_t));
+	tmp_req_ctx->mode = CE_RSA_OP_M_MUL;
+
+	sg_init_one(&sg, ctx->key, ctx->iv_size*2);
+	tmp_req_ctx->dma_src.sg = &sg;
+
+	ss_aes_start(ctx, tmp_req_ctx, len);
+
+	SS_DBG("The preporcess of RSA complete!\n\n");
+	kfree(tmp_req_ctx);
+}
+#endif
+
+static int ss_rng_start(ss_aes_ctx_t *ctx, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	int i = 0;
+	int flow = ctx->comm.flow;
+	int rng_len = 0;
+	char *buf = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	if (trng)
+		rng_len = DIV_ROUND_UP(dlen, 32)*32; /* align with 32 Bytes */
+	else
+		rng_len = DIV_ROUND_UP(dlen, 20)*20; /* align with 20 Bytes */
+
+	if (rng_len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", rng_len);
+		rng_len = SS_RNG_MAX_LEN;
+	}
+
+	buf = kmalloc(rng_len, GFP_KERNEL);
+	if (buf == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", rng_len);
+		return -ENOMEM;
+	}
+
+	ss_hash_rng_change_clk();
+
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	if (trng)
+		ss_rng_method_set(SS_METHOD_SHA256, SS_METHOD_TRNG, task);
+	else
+		ss_rng_method_set(SS_METHOD_SHA1, SS_METHOD_PRNG, task);
+
+	phy_addr = virt_to_phys(ctx->key);
+	SS_DBG("ctx->key addr, vir = 0x%px, phy = 0x%pa\n", ctx->key, &phy_addr);
+
+	if (trng == 0) {
+		/* Must set the seed addr in PRNG, key_len 5 words stable*/
+		ctx->key_size = 5 * sizeof(int);
+		ss_rng_key_set(ctx->key, ctx->key_size, task);
+		ce_task_data_len_set(0, task->data_len);
+		ctx->comm.flags &= ~SS_FLAG_NEW_KEY;
+		dma_map_single(&ss_dev->pdev->dev,
+			ctx->key, ctx->key_size, DMA_MEM_TO_DEV);
+	}
+	phy_addr = virt_to_phys(buf);
+	SS_DBG("buf addr, vir = 0x%px, phy = 0x%pa\n", buf, &phy_addr);
+
+	/* Prepare the dst scatterlist */
+	ce_task_addr_set(buf, 0, task->ce_sg[0].dst_addr);
+	task->ce_sg[0].dst_len = (rng_len + 3) >> 2;
+	dma_map_single(&ss_dev->pdev->dev, buf, rng_len, DMA_DEV_TO_MEM);
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n", flow, dlen, rng_len);
+
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+	/* addr should set in word, src_len and dst_len set in bytes */
+	for (i = 0; i < 8; i++) {
+		task->ce_sg[i].src_len = (task->ce_sg[i].src_len) << 2;
+		task->ce_sg[i].dst_len = (task->ce_sg[i].dst_len) << 2;
+	}
+
+	ce_print_new_task_desc(task);
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(buf),
+		rng_len, DMA_DEV_TO_MEM);
+	if (trng == 0)
+		dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->key),
+			ctx->key_size, DMA_MEM_TO_DEV);
+	memcpy(rdata, buf, dlen);
+	kfree(buf);
+	ss_irq_disable(flow);
+	ret = dlen;
+
+	return ret;
+}
+
+int ss_rng_get_random(struct crypto_rng *tfm, u8 *rdata, u32 dlen, u32 trng)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	ss_aes_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, data = 0x%px, len = %d, trng = %d\n",
+		ctx->comm.flow, data, len, trng);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	ss_dev_lock();
+	ret = ss_rng_start(ctx, data, len, trng);
+	ss_dev_unlock();
+
+	SS_DBG("Get %d byte random.\n", ret);
+
+	return ret;
+}
+
+static int ss_drbg_start(ss_drbg_ctx_t *ctx, u8 *src, u32 slen, u8 *rdata, u32 dlen, u32 mode)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	int rng_len = dlen;
+	char *buf = NULL;
+	char *entropy = NULL;
+	char *person = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	if (rng_len > SS_RNG_MAX_LEN) {
+		SS_ERR("The RNG length is too large: %d\n", rng_len);
+		rng_len = SS_RNG_MAX_LEN;
+	}
+
+	if (ctx->entropt_size < 80 / 8) {
+		SS_ERR("The DRBG length is too small: %d, less than 80 bit\n", ctx->entropt_size);
+		return -EINVAL;
+	}
+
+	buf = kmalloc(rng_len, GFP_KERNEL);
+	if (buf == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", rng_len);
+		return -ENOMEM;
+	}
+
+	entropy = kmalloc(ctx->entropt_size, GFP_KERNEL);
+	if (entropy == NULL) {
+		SS_ERR("Failed to malloc(%d)\n", ctx->entropt_size);
+		return -ENOMEM;
+	}
+	memcpy(entropy, ctx->entropt, ctx->entropt_size);
+
+	if (ctx->person_size) {
+		person = kmalloc(ctx->person_size, GFP_KERNEL);
+		if (buf == NULL) {
+			SS_ERR("Failed to malloc(%d)\n", ctx->person_size);
+			return -ENOMEM;
+		}
+	}
+	ss_hash_rng_change_clk();
+
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_rng_method_set(mode, SS_METHOD_DRBG, task);
+
+	phy_addr = virt_to_phys(ctx->entropt);
+	SS_DBG("ctx->entropt, vir = 0x%px, phy = 0x%pa\n", ctx->entropt, &phy_addr);
+
+	phy_addr = virt_to_phys(ctx->person);
+	SS_DBG("ctx->person, vir = 0x%px, phy = 0x%pa\n", ctx->person, &phy_addr);
+
+	phy_addr = virt_to_phys(buf);
+	SS_DBG("buf addr, vir = 0x%px, phy = %pa\n", buf, &phy_addr);
+
+	/* Prepare the dst scatterlist */
+	ce_task_addr_set(entropy, 0, task->ce_sg[0].src_addr);
+	//task->ce_sg[0].src_addr = virt_to_phys(entropy) >> 2;	/*address in words*/
+	task->ce_sg[0].src_len = ctx->entropt_size;
+	ce_task_addr_set(ctx->nonce, 0, task->ce_sg[1].src_addr);
+	//task->src[1].addr = (virt_to_phys(ctx->nonce)) >> 2;	/* in software, not used*/
+	task->ce_sg[1].src_len = 0;
+	ce_task_addr_set(person, 0, task->ce_sg[2].src_addr);
+	//task->src[2].addr = virt_to_phys(person) >> 2;	/*address in words*/
+	task->ce_sg[2].src_len = ctx->person_size;
+	ce_task_addr_set(src, 0, task->ce_sg[3].src_addr);
+	//task->src[3].addr = virt_to_phys(src) >> 2;	/*address in words*/
+	task->ce_sg[3].src_len = slen;
+
+	dma_map_single(&ss_dev->pdev->dev, entropy, ctx->entropt_size, DMA_MEM_TO_DEV);
+	dma_map_single(&ss_dev->pdev->dev, ctx->nonce, ctx->nonce_size, DMA_MEM_TO_DEV);
+	dma_map_single(&ss_dev->pdev->dev, person, ctx->person_size, DMA_MEM_TO_DEV);
+	dma_map_single(&ss_dev->pdev->dev, src, slen, DMA_MEM_TO_DEV);
+
+	/* Prepare the dst scatterlist */
+	ce_task_addr_set(buf, 0, task->ce_sg[0].dst_addr);
+	//task->dst[0].addr = virt_to_phys(buf) >> 2;	/*address in words*/
+	task->ce_sg[0].dst_len  = rng_len;
+
+	dma_map_single(&ss_dev->pdev->dev, buf, rng_len, DMA_DEV_TO_MEM);
+
+	SS_DBG("Flow: %d, Request: %d, Aligned: %d\n", flow, dlen, rng_len);
+
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+	ce_print_new_task_desc(task);
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_ICR));
+
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+		ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(entropy),
+					ctx->entropt_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(ctx->nonce),
+					ctx->nonce_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(person),
+					ctx->person_size, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(src),
+					slen, DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(buf),
+					rng_len, DMA_DEV_TO_MEM);
+
+	memcpy(rdata, buf, dlen);
+	kfree(buf);
+	if (ctx->person_size)
+		kfree(person);
+	kfree(entropy);
+	ss_irq_disable(flow);
+	ret = dlen;
+
+	return ret;
+}
+
+int ss_drbg_get_random(struct crypto_rng *tfm, const u8 *src, u32 slen, u8 *rdata, u32 dlen, u32 mode)
+{
+	int ret = 0;
+	u8 *data = rdata;
+	u32 len = dlen;
+	u8 *src_t;
+	ss_drbg_ctx_t *ctx = crypto_rng_ctx(tfm);
+
+	SS_DBG("flow = %d, src = 0x%px, slen = %d, data = 0x%px, len = %d, hash_mode = %d\n",
+		ctx->comm.flow, src, slen, data, len, mode);
+	if (ss_dev->suspend) {
+		SS_ERR("SS has already suspend.\n");
+		return -EAGAIN;
+	}
+
+	src_t = kzalloc(slen, GFP_KERNEL);
+	if (src_t == NULL) {
+		SS_ERR("Failed to kmalloc(%d)\n", slen);
+		return -ENOMEM;
+	}
+	memcpy(src_t, src, slen);
+
+	ss_dev_lock();
+	ret = ss_drbg_start(ctx, src_t, slen, data, len, mode);
+	ss_dev_unlock();
+
+	kfree(src_t);
+	SS_DBG("Get %d byte random.\n", ret);
+
+	return ret;
+}
+
+
+u32 ss_hash_start(ss_hash_ctx_t *ctx,
+		ss_aes_req_ctx_t *req_ctx, u32 len, u32 last)
+{
+	int ret = 0;
+	int flow = ctx->comm.flow;
+	u32 blk_size = ss_hash_blk_size(req_ctx->type);
+	char *digest = NULL;
+	phys_addr_t phy_addr = 0;
+	ce_new_task_desc_t *task = (ce_new_task_desc_t *)&ss_dev->flows[flow].task;
+
+	/* Total len is too small, so process it in the padding data later. */
+	if ((last == 0) && (len > 0) && (len < blk_size)) {
+		ctx->cnt += len;
+		return 0;
+	}
+	ss_hash_rng_change_clk();
+
+	digest = kzalloc(SHA512_DIGEST_SIZE, GFP_KERNEL);
+	if (digest == NULL) {
+		SS_ERR("Failed to kmalloc(%d)\n", SHA512_DIGEST_SIZE);
+		return -ENOMEM;
+	}
+
+	ss_new_task_desc_init(task, flow);
+
+	ss_pending_clear(flow);
+	ss_irq_enable(flow);
+
+	ss_hash_method_set(req_ctx->type, task);
+
+	SS_DBG("Flow: %d, Dir: %d, Method: %d, Mode: %d, len: %d / %d\n", flow,
+		req_ctx->dir, req_ctx->type, req_ctx->mode, len, ctx->cnt);
+	SS_DBG("IV address = 0x%px, size = %d\n", ctx->md, ctx->md_size);
+	phy_addr = virt_to_phys(task);
+	SS_DBG("Task addr, vir = 0x%px, phy = 0x%pa\n", task, &phy_addr);
+
+	ss_hash_iv_set(ctx->md, ctx->md_size, task);
+	ss_hash_iv_mode_set(1, task);
+	dma_map_single(&ss_dev->pdev->dev,
+		ctx->md, ctx->md_size, DMA_MEM_TO_DEV);
+
+	if (last == 1) {
+		ss_hmac_sha1_last(task);
+		ss_hash_data_len_set(ctx->tail_len * 8, task);/*bits*/
+	} else {
+		ss_hash_data_len_set(((len - (len % blk_size)) * 8), task);
+	}
+
+	/* Prepare the src scatterlist */
+	req_ctx->dma_src.nents = ss_sg_cnt(req_ctx->dma_src.sg, len);
+	dma_map_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+	ss_sg_config(task->ce_sg,
+		&req_ctx->dma_src, req_ctx->type, 0, len % blk_size, 1, SRC_FLAG);
+
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		task->ce_sg[0].src_len = ctx->tail_len;/*byte*/
+		SS_DBG("cnt %d, tail_len %d.\n", ctx->cnt, ctx->tail_len);
+		ctx->cnt <<= 3; /* Translate to bits in the last pakcket */
+		ss_hash_data_len_set(ctx->cnt, task);/*bits*/
+	}
+#endif
+
+	/* Prepare the dst scatterlist */
+	ce_task_addr_set(digest, 0, task->ce_sg[0].dst_addr);
+	task->ce_sg[0].dst_len = ctx->md_size;
+
+	if (last == 1) {
+		if (req_ctx->type == SS_METHOD_SHA224)
+			task->ce_sg[0].dst_len  = SHA224_DIGEST_SIZE;
+		if (req_ctx->type == SS_METHOD_SHA384)
+			task->ce_sg[0].dst_len  = SHA384_DIGEST_SIZE;
+	}
+
+	dma_map_single(&ss_dev->pdev->dev,
+		digest, SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	phy_addr = virt_to_phys(digest);
+	SS_DBG("digest addr, vir = 0x%px, phy = 0x%pa\n", digest, &phy_addr);
+
+
+	/* Start CE controller. */
+	init_completion(&ss_dev->flows[flow].done);
+	dma_map_single(&ss_dev->pdev->dev, task,
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+
+	ce_print_new_task_desc(task);
+	SS_DBG("Before CE, COMM_CTL: 0x%08x, ICR: 0x%08x\n",
+		task->comm_ctl, ss_reg_rd(CE_REG_ICR));
+	ss_hash_rng_ctrl_start(task);
+
+	ret = wait_for_completion_timeout(&ss_dev->flows[flow].done,
+		msecs_to_jiffies(SS_WAIT_TIME));
+	if (ret == 0) {
+		SS_ERR("Timed out\n");
+		ss_reset();
+		ret = -ETIMEDOUT;
+	}
+	ss_irq_disable(flow);
+
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(task),
+		sizeof(ce_new_task_desc_t), DMA_MEM_TO_DEV);
+	dma_unmap_single(&ss_dev->pdev->dev, virt_to_phys(digest),
+		SHA512_DIGEST_SIZE, DMA_DEV_TO_MEM);
+	dma_unmap_sg(&ss_dev->pdev->dev, req_ctx->dma_src.sg,
+		req_ctx->dma_src.nents, DMA_MEM_TO_DEV);
+#ifdef SS_HASH_HW_PADDING
+	if (last == 1) {
+		ctx->cnt >>= 3;
+	}
+#endif
+
+	SS_DBG("After CE, TSR: 0x%08x, ERR: 0x%08x\n",
+			ss_reg_rd(CE_REG_TSR), ss_reg_rd(CE_REG_ERR));
+	SS_DBG("After CE, dst data:\n");
+	ss_print_hex(digest, SHA512_DIGEST_SIZE, digest);
+
+	if (ss_flow_err(flow)) {
+		SS_ERR("CE return error: %d\n", ss_flow_err(flow));
+		kfree(digest);
+		return -EINVAL;
+	}
+
+	/* Backup the MD to ctx->md. */
+	memcpy(ctx->md, digest, ctx->md_size);
+
+	if (last == 0)
+		ctx->cnt += len;
+	kfree(digest);
+	return 0;
+}
+
+void ss_load_iv(ss_aes_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx,
+	char *buf, int size)
+{
+	if (buf == NULL)
+		return;
+
+	/* Only AES/DES/3DES-ECB don't need IV. */
+	if (CE_METHOD_IS_AES(req_ctx->type) &&
+		(req_ctx->mode == SS_AES_MODE_ECB))
+		return;
+
+	/* CBC/CTS/GCM need update the IV eachtime. */
+	if ((ctx->cnt == 0)
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CBC))
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, CTS))
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, GCM))) {
+		SS_DBG("IV address = 0x%px, size = %d\n", buf, size);
+		ctx->iv_size = size;
+		memcpy(ctx->iv, buf, ctx->iv_size);
+	}
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, ctx->iv_size, ctx->iv);
+}
+
+void ss_aead_load_iv(ss_aead_ctx_t *ctx, ss_aes_req_ctx_t *req_ctx,
+	char *buf, int size)
+{
+	if (buf == NULL)
+		return;
+
+	/* CBC/CTS/GCM need update the IV eachtime. */
+	if ((ctx->cnt == 0)
+		|| (CE_IS_AES_MODE(req_ctx->type, req_ctx->mode, GCM))) {
+		SS_DBG("IV address = 0x%px, size = %d\n", buf, size);
+		ctx->iv_size = size;
+		memcpy(ctx->iv, buf, ctx->iv_size);
+	}
+
+	SS_DBG("The current IV:\n");
+	ss_print_hex(ctx->iv, ctx->iv_size, ctx->iv);
+}
+
+int ss_aead_one_req(sunxi_ss_t *sss, struct aead_request *req)
+{
+	int ret = 0;
+	struct crypto_aead *tfm = NULL;
+	ss_aead_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = 0x%px, dst = 0x%px\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_aead_reqtfm(req);
+	req_ctx = aead_request_ctx(req);
+	ctx = crypto_aead_ctx(tfm);
+
+	ss_aead_load_iv(ctx, req_ctx, req->iv, crypto_aead_ivsize(tfm));
+
+	memset(ctx->task_ctr, 0, sizeof(ctx->task_ctr));
+	ctx->tag_len = tfm->authsize;
+	ctx->assoclen = req->assoclen;
+	ctx->cryptlen = req->cryptlen;
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+	ret = ss_aead_start(ctx, req_ctx);
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+	return ret;
+}
+
+int ss_aes_one_req(sunxi_ss_t *sss, struct ablkcipher_request *req)
+{
+	int ret = 0;
+	struct crypto_ablkcipher *tfm = NULL;
+	ss_aes_ctx_t *ctx = NULL;
+	ss_aes_req_ctx_t *req_ctx = NULL;
+
+	SS_ENTER();
+	if (!req->src || !req->dst) {
+		SS_ERR("Invalid sg: src = 0x%px, dst = 0x%px\n", req->src, req->dst);
+		return -EINVAL;
+	}
+
+	ss_dev_lock();
+
+	tfm = crypto_ablkcipher_reqtfm(req);
+	req_ctx = ablkcipher_request_ctx(req);
+	ctx = crypto_ablkcipher_ctx(tfm);
+
+	ss_load_iv(ctx, req_ctx, req->info, crypto_ablkcipher_ivsize(tfm));
+
+	req_ctx->dma_src.sg = req->src;
+	req_ctx->dma_dst.sg = req->dst;
+
+#ifdef SS_RSA_PREPROCESS_ENABLE
+	ss_rsa_preprocess(ctx, req_ctx, req->nbytes);
+#endif
+
+	if (CE_METHOD_IS_HMAC(req_ctx->type)) {
+		ret = ss_hmac_start(ctx, req_ctx, req->nbytes);
+	} else {
+		ret = ss_aes_start(ctx, req_ctx, req->nbytes);
+	}
+	if (ret < 0)
+		SS_ERR("ss_aes_start fail(%d)\n", ret);
+
+	ss_dev_unlock();
+
+#ifdef SS_CTR_MODE_ENABLE
+	if (req_ctx->mode == SS_AES_MODE_CTR) {
+		SS_DBG("CNT: %08x %08x %08x %08x\n",
+			*(int *)&ctx->iv[0], *(int *)&ctx->iv[4],
+			*(int *)&ctx->iv[8], *(int *)&ctx->iv[12]);
+	}
+#endif
+
+	ctx->cnt += req->nbytes;
+	return ret;
+}
+
+irqreturn_t sunxi_ss_irq_handler(int irq, void *dev_id)
+{
+	int i;
+	int pending = 0;
+	sunxi_ss_t *sss = (sunxi_ss_t *)dev_id;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&sss->lock, flags);
+
+	pending = ss_pending_get();
+	SS_DBG("pending: %#x\n", pending);
+	for (i = 0; i < SS_FLOW_NUM; i++) {
+		if (pending & (CE_CHAN_PENDING << (2 * i))) {
+			SS_DBG("Chan %d completed. pending: %#x\n", i, pending);
+			ss_pending_clear(i);
+			complete(&sss->flows[i].done);
+		}
+	}
+
+	spin_unlock_irqrestore(&sss->lock, flags);
+	return IRQ_HANDLED;
+}
diff --git a/drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.c b/drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.c
new file mode 100644
index 000000000..11232c005
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.c
@@ -0,0 +1,509 @@
+/*
+ * The interface function of controlling the SS register.
+ *
+ * Copyright (C) 2018 Allwinner.
+ *
+ *<xupengliu@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+
+#include "../sunxi_ce.h"
+#include "sunxi_ce_reg.h"
+
+inline u32 ss_readl(u32 offset)
+{
+	return readl(ss_membase() + offset);
+}
+
+inline void ss_writel(u32 offset, u32 val)
+{
+	writel(val, ss_membase() + offset);
+}
+
+u32 ss_reg_rd(u32 offset)
+{
+	return ss_readl(offset);
+}
+
+void ss_reg_wr(u32 offset, u32 val)
+{
+	ss_writel(offset, val);
+}
+
+void ss_keyselect_set(int select, ce_task_desc_t *task)
+{
+	task->sym_ctl |= select << CE_SYM_CTL_KEY_SELECT_SHIFT;
+}
+
+void ss_keysize_set(int size, ce_task_desc_t *task)
+{
+	int type = CE_AES_KEY_SIZE_128;
+
+	switch (size) {
+	case AES_KEYSIZE_192:
+		type = CE_AES_KEY_SIZE_192;
+		break;
+	case AES_KEYSIZE_256:
+		type = CE_AES_KEY_SIZE_256;
+		break;
+	default:
+		break;
+	}
+
+	task->sym_ctl |= (type << CE_SYM_CTL_KEY_SIZE_SHIFT);
+}
+
+void ce_task_addr_set(u8 *vir_addr, phys_addr_t phy_addr, u8 *dst)
+{
+	phys_addr_t phy;
+
+	if (phy_addr) {
+		phy = phy_addr;
+	} else {
+		phy = virt_to_phys(vir_addr);
+	}
+
+	if (phy > 0xffffffff) {
+		phy = phy & CE_ADDR_MASK;
+		memcpy(dst, (void *)&phy, 5);
+	} else {
+		memcpy(dst, (void *)&phy, 4);
+	}
+}
+
+phys_addr_t ce_task_addr_get(u8 *dst)
+{
+	phys_addr_t phy;
+	if (dst[4]) {
+		phy = (*((u64 *)dst)) & CE_ADDR_MASK;
+	} else {
+		phy = *((u32 *)dst);
+	}
+
+	return phy;
+}
+void ce_task_data_len_set(u32 len, u8 *dst)
+{
+	dst[0] = len & 0xff;
+	dst[1] = (len >> 8) & 0xff;
+	dst[2] = (len >> 16) & 0xff;
+	dst[3] = (len >> 24) & 0xff;
+	dst[4] = 0;
+}
+
+/* key: phsical address. */
+void ss_key_set(char *key, int size, ce_task_desc_t *task)
+{
+	int i = 0;
+	int key_sel = CE_KEY_SELECT_INPUT;
+	struct {
+		int type;
+		char desc[AES_MIN_KEY_SIZE];
+	} keys[] = {
+		{CE_KEY_SELECT_SSK,		   CE_KS_SSK},
+		{CE_KEY_SELECT_HUK,		   CE_KS_HUK},
+		{CE_KEY_SELECT_RSSK,	   CE_KS_RSSK},
+
+		{CE_KEY_SELECT_INTERNAL_0, CE_KS_INTERNAL_0},
+		{CE_KEY_SELECT_INTERNAL_1, CE_KS_INTERNAL_1},
+		{CE_KEY_SELECT_INTERNAL_2, CE_KS_INTERNAL_2},
+		{CE_KEY_SELECT_INTERNAL_3, CE_KS_INTERNAL_3},
+		{CE_KEY_SELECT_INTERNAL_4, CE_KS_INTERNAL_4},
+		{CE_KEY_SELECT_INTERNAL_5, CE_KS_INTERNAL_5},
+		{CE_KEY_SELECT_INTERNAL_6, CE_KS_INTERNAL_6},
+		{CE_KEY_SELECT_INTERNAL_7, CE_KS_INTERNAL_7},
+		{CE_KEY_SELECT_INPUT, ""} };
+
+	while (keys[i].type != CE_KEY_SELECT_INPUT) {
+		if (strncasecmp(key, keys[i].desc, AES_MIN_KEY_SIZE) == 0) {
+			key_sel = keys[i].type;
+			memset(key, 0, size);
+			break;
+		}
+		i++;
+	}
+	SS_DBG("The key select: %d\n", key_sel);
+
+	ss_keyselect_set(key_sel, task);
+	ss_keysize_set(size, task);
+
+	ce_task_addr_set(key, 0, task->key_addr);
+}
+
+void ss_pending_clear(int flow)
+{
+	int val = 0;
+
+	switch (flow) {
+	case (0):
+		val = CE_CHAN_PENDING << 0;
+		break;
+	case (1):
+		val = CE_CHAN_PENDING << 2;
+		break;
+	case (2):
+		val = CE_CHAN_PENDING << 4;
+		break;
+	case (3):
+		val = CE_CHAN_PENDING << 6;
+		break;
+
+	default:
+		break;
+	}
+	ss_writel(CE_REG_ISR, val);
+}
+
+int ss_pending_get(void)
+{
+	return ss_readl(CE_REG_ISR);
+}
+
+void ss_irq_enable(int flow)
+{
+	int val = ss_readl(CE_REG_ICR);
+
+	val |= CE_CHAN_INT_ENABLE << flow;
+	ss_writel(CE_REG_ICR, val);
+}
+
+void ss_irq_disable(int flow)
+{
+	int val = ss_readl(CE_REG_ICR);
+
+	val &= ~(CE_CHAN_INT_ENABLE << flow);
+	ss_writel(CE_REG_ICR, val);
+}
+
+void ss_md_get(char *dst, char *src, int size)
+{
+	memcpy(dst, src, size);
+}
+
+/* iv: phsical address. */
+void ss_iv_set(char *iv, int size, ce_task_desc_t *task)
+{
+	ce_task_addr_set(iv, 0, task->iv_addr);
+}
+
+void ss_iv_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->comm_ctl |= mode << CE_COMM_CTL_IV_MODE_SHIFT;
+}
+
+void ss_cntsize_set(int size, ce_task_desc_t *task)
+{
+	task->sym_ctl |= size << CE_SYM_CTL_CTR_SIZE_SHIFT;
+}
+
+void ss_cnt_set(char *cnt, int size, ce_task_desc_t *task)
+{
+	ce_task_addr_set(cnt, 0, task->ctr_addr);
+	ss_cntsize_set(CE_CTR_SIZE_128, task);
+}
+
+void ss_gcm_cnt_set(char *cnt, int size, ce_task_desc_t *task)
+{
+	ce_task_addr_set(cnt, 0, task->ctr_addr);
+	ss_cntsize_set(CE_CTR_SIZE_32, task);
+}
+
+void ss_cts_last(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_CTS_LAST;
+}
+
+void ss_tag_len_set(u32 len, ce_task_desc_t *task)
+{
+	task->comm_ctl |= len << CE_COM_CTL_GCM_TAGLEN_SHIFT;
+}
+
+void ss_gcm_iv_mode(ce_task_desc_t *task, int iv_mode)
+{
+	task->sym_ctl |= iv_mode << CE_SYM_CTL_GCM_IV_MODE_SHIFT;
+}
+
+void ss_gcm_reserve_set(ce_task_desc_t *task, int iv_len, int aad_len, int pt_len)
+{
+	task->reserved[0] = iv_len * 8;
+	task->reserved[1] = aad_len * 8;
+	task->reserved[2] = pt_len * 8;
+}
+
+void ss_gcm_src_config(ce_scatter_t *sg, u32 addr, u32 len)
+{
+	ce_task_addr_set(0, addr, sg->src_addr);
+	sg->src_len = len;
+	//src->addr = addr >> 2;	/* address in words*/
+	//src->len = len >> 2;	/* in word*/
+}
+
+void ss_xts_first(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_XTS_FIRST;
+}
+
+void ss_xts_last(ce_task_desc_t *task)
+{
+	task->sym_ctl |= CE_SYM_CTL_AES_XTS_LAST;
+}
+
+void ss_method_set(int dir, int type, ce_task_desc_t *task)
+{
+	task->comm_ctl |= dir << CE_COMM_CTL_OP_DIR_SHIFT;
+	task->comm_ctl |= type << CE_COMM_CTL_METHOD_SHIFT;
+}
+
+void ss_aes_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->sym_ctl |= mode << CE_SYM_CTL_OP_MODE_SHIFT;
+}
+
+void ss_aead_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->sym_ctl |= mode << CE_SYM_CTL_OP_MODE_SHIFT;
+	task->sym_ctl |= 1 << 2;	/*set gcm mode counter width*/
+}
+
+void ss_cfb_bitwidth_set(int bitwidth, ce_task_desc_t *task)
+{
+	int val = 0;
+
+	switch (bitwidth) {
+	case 1:
+		val = CE_CFB_WIDTH_1;
+		break;
+	case 8:
+		val = CE_CFB_WIDTH_8;
+		break;
+	case 64:
+		val = CE_CFB_WIDTH_64;
+		break;
+	case 128:
+		val = CE_CFB_WIDTH_128;
+		break;
+	default:
+		break;
+	}
+	task->sym_ctl |= val << CE_SYM_CTL_CFB_WIDTH_SHIFT;
+}
+
+void ss_sha_final(void)
+{
+	/* unsupported. */
+}
+
+void ss_check_sha_end(void)
+{
+	/* unsupported. */
+}
+
+void ss_rsa_width_set(int size, ce_task_desc_t *task)
+{
+	task->asym_ctl |= DIV_ROUND_UP(size, 4);
+}
+
+void ss_rsa_op_mode_set(int mode, ce_task_desc_t *task)
+{
+	/* Consider !2 as M_EXP, for compatible with the previous SOC. */
+	if (mode == CE_RSA_OP_M_MUL)
+		task->asym_ctl |= CE_RSA_OP_M_MUL<<CE_ASYM_CTL_RSA_OP_SHIFT;
+	else
+		task->asym_ctl |= CE_RSA_OP_M_EXP<<CE_ASYM_CTL_RSA_OP_SHIFT;
+}
+
+void ss_ecc_width_set(int size, ce_task_desc_t *task)
+{
+	task->asym_ctl |= DIV_ROUND_UP(size, 4);
+}
+
+void ss_ecc_op_mode_set(int mode, ce_task_desc_t *task)
+{
+	task->asym_ctl |= mode<<CE_ASYM_CTL_RSA_OP_SHIFT;
+}
+
+void ss_ctrl_start(ce_task_desc_t *task, int type, int mode)
+{
+	phys_addr_t task_phy;
+
+	task_phy = virt_to_phys(task);
+	if (task_phy > 0xffffffff) {
+		ss_writel(CE_REG_TSK0, task_phy & 0xffffffff);
+		ss_writel(CE_REG_TSK1, (task_phy >> 32));
+	} else {
+		ss_writel(CE_REG_TSK0, task_phy);
+	}
+
+	task->task_phy_addr = task_phy;
+	if (CE_METHOD_IS_AES(type) && (mode == SS_AES_MODE_XTS))
+		ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_RAES_TYPE_SHIFT);
+	else if (CE_METHOD_IS_AES(type) && (mode != SS_AES_MODE_XTS))
+		ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_SYMM_TYPE_SHIFT);
+	else
+		ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_ASYM_TYPE_SHIFT);
+}
+
+void ss_ctrl_stop(void)
+{
+	/* unsupported */
+}
+
+int ss_flow_err(int flow)
+{
+	return ss_readl(CE_REG_ERR) & CE_REG_ESR_CHAN_MASK(flow);
+}
+
+void ss_wait_idle(void)
+{
+#ifdef SS_SUPPORT_CE_V3_1
+	while ((ss_readl(CE_REG_TSR) & CE_REG_TSR_BUSY_MASK) ==
+		CE_REG_TSR_BUSY) {
+		SS_DBG("Need wait for the hardware.\n");
+		msleep(20);
+	}
+#else
+	while ((ss_readl(CE_REG_TSR) & 0xff) != 0x0) {
+		SS_DBG("Need wait for the hardware.\n");
+		msleep(20);
+	}
+#endif
+}
+
+void ss_data_len_set(int len, ce_task_desc_t *task)
+{
+	task->data_len = len;
+}
+
+int ss_reg_print(char *buf, int len)
+{
+	return snprintf(buf, len,
+		"The SS control register:\n"
+		"[TSK0] 0x%02x = 0x%08x\n"
+		"[TSK1] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[CTL] 0x%02x = 0x%08x\n"
+#endif
+		"[ICR] 0x%02x = 0x%08x, [ISR] 0x%02x = 0x%08x\n"
+		"[TLR] 0x%02x = 0x%08x\n"
+		"[TSR] 0x%02x = 0x%08x\n"
+		"[ERR] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[CSS] 0x%02x = 0x%08x, [CDS] 0x%02x = 0x%08x\n"
+#endif
+		"[CSA] 0x%02x = 0x%08x, [CDA] 0x%02x = 0x%08x\n"
+#ifdef SS_SUPPORT_CE_V3_1
+		"[TPR] 0x%02x = 0x%08x\n"
+#else
+		"[HCSA] 0x%02x = 0x%08x\n"
+		"[HCDA] 0x%02x = 0x%08x\n"
+		"[ACSA] 0x%02x = 0x%08x\n"
+		"[ACDA] 0x%02x = 0x%08x\n"
+		"[XCSA] 0x%02x = 0x%08x\n"
+		"[XCDA] 0x%02x = 0x%08x\n"
+		"[VER] 0x%02x = 0x%08x\n"
+#endif
+		,
+		CE_REG_TSK0, ss_readl(CE_REG_TSK0),
+		CE_REG_TSK1, ss_readl(CE_REG_TSK1),
+#ifdef SS_SUPPORT_CE_V3_1
+		CE_REG_CTL, ss_readl(CE_REG_CTL),
+#endif
+		CE_REG_ICR, ss_readl(CE_REG_ICR),
+		CE_REG_ISR, ss_readl(CE_REG_ISR),
+		CE_REG_TLR, ss_readl(CE_REG_TLR),
+		CE_REG_TSR, ss_readl(CE_REG_TSR),
+		CE_REG_ERR, ss_readl(CE_REG_ERR),
+#ifdef SS_SUPPORT_CE_V3_1
+		CE_REG_CSS, ss_readl(CE_REG_CSS),
+		CE_REG_CDS, ss_readl(CE_REG_CDS),
+#endif
+		CE_REG_CSA, ss_readl(CE_REG_CSA),
+		CE_REG_CDA, ss_readl(CE_REG_CDA)
+#ifdef SS_SUPPORT_CE_V3_1
+		,
+		CE_REG_TPR, ss_readl(CE_REG_TPR)
+#else
+		,
+		CE_REG_HCSA, ss_readl(CE_REG_HCSA),
+		CE_REG_HCDA, ss_readl(CE_REG_HCDA),
+		CE_REG_ACSA, ss_readl(CE_REG_ACSA),
+		CE_REG_ACDA, ss_readl(CE_REG_ACDA),
+		CE_REG_XCSA, ss_readl(CE_REG_XCSA),
+		CE_REG_XCDA, ss_readl(CE_REG_XCDA),
+		CE_REG_VER, ss_readl(CE_REG_VER)
+#endif
+		);
+}
+
+/* key: phsical address. */
+void ss_rng_key_set(char *key, int size, ce_new_task_desc_t *task)
+{
+	ce_task_addr_set(key, 0, task->key_addr);
+}
+
+/* iv: phsical address. */
+void ss_hash_iv_set(char *iv, int size, ce_new_task_desc_t *task)
+{
+	ce_task_addr_set(iv, 0, task->iv_addr);
+}
+
+void ss_hash_iv_mode_set(int mode, ce_new_task_desc_t *task)
+{
+	task->comm_ctl |= mode << CE_CTL_IV_MODE_SHIFT;
+}
+
+void ss_hmac_sha1_last(ce_new_task_desc_t *task)
+{
+	task->comm_ctl |= CE_CTL_HMAC_SHA1_LAST;
+}
+
+void ss_hmac_method_set(int type, ce_new_task_desc_t *task)
+{
+	task->main_cmd |= type << CE_CMD_HASH_METHOD_SHIFT;
+	task->main_cmd |= 0x1 << CE_CMD_HMAC_METHOD_SHIFT;
+}
+
+void ss_hash_method_set(int type, ce_new_task_desc_t *task)
+{
+	task->main_cmd |= type << CE_CMD_HASH_METHOD_SHIFT;
+}
+
+void ss_rng_method_set(int hash_type, int type, ce_new_task_desc_t *task)
+{
+	task->main_cmd |= (type << CE_CMD_RNG_METHOD_SHIFT);
+	task->main_cmd |= (hash_type << CE_CMD_HASH_METHOD_SHIFT);
+	if (type == SS_METHOD_DRBG) {
+		task->comm_ctl |= CE_CTL_HMAC_SHA1_LAST;
+		/* for drbg, need set sub_cmd bit[28:16] */
+		task->main_cmd |= (2 << 16 | 0 << 20 | BIT(28));
+	}
+}
+
+void ss_hash_rng_ctrl_start(ce_new_task_desc_t *task)
+{
+	phys_addr_t task_phy;
+
+	task_phy = virt_to_phys(task);
+	if (task_phy > 0xffffffff) {
+		ss_writel(CE_REG_TSK0, task_phy & 0xffffffff);
+		ss_writel(CE_REG_TSK1, (task_phy >> 32));
+	} else {
+		ss_writel(CE_REG_TSK0, task_phy);
+	}
+	ss_writel(CE_REG_TLR, 0x1 << CE_REG_TLR_HASH_RBG_TYPE_SHIFT);
+	task->task_phy_addr = task_phy;
+}
+
+void ss_hash_data_len_set(int len, ce_new_task_desc_t *task)
+{
+	task->comm_ctl |= 0 << 13;
+	ce_task_data_len_set(len, task->data_len);
+}
+
diff --git a/drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.h b/drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.h
new file mode 100644
index 000000000..5aa7b4225
--- /dev/null
+++ b/drivers/crypto/sunxi-ce/v5/sunxi_ce_reg.h
@@ -0,0 +1,301 @@
+/*
+ * The register macro of SUNXI SecuritySystem controller.
+ *
+ * Copyright (C) 2018 Allwinner.
+ *
+ * <xupengliu@allwinnertech.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _SUNXI_SECURITY_SYSTEM_REG_H_
+#define _SUNXI_SECURITY_SYSTEM_REG_H_
+
+/* CE: Crypto Engine, start using CE from sun8iw7/sun8iw9 */
+#define CE_REG_TSK0			0x00
+#define CE_REG_TSK1			0x04
+#define CE_REG_ICR			0x08
+#define CE_REG_ISR			0x0C
+#define CE_REG_TLR			0x10
+#define CE_REG_TSR			0x14
+#define CE_REG_ERR			0x18
+
+#define CE_REG_DRL			0x1c
+
+#define CE_REG_CSA			0x24
+#define CE_REG_CDA			0x28
+#define CE_REG_HCSA			0x34
+#define CE_REG_HCDA			0x38
+#define CE_REG_ACSA			0x44
+#define CE_REG_ACDA			0x48
+#define CE_REG_XCSA			0x54
+#define CE_REG_XCDA			0x58
+#define CE_REG_VER			0x90
+
+#define CE_CHAN_INT_ENABLE		1
+
+
+#define CE_REG_TLR_METHOD_TYPE_SHIFT		8
+
+#define CE_REG_ESR_ERR_UNSUPPORT	0
+#define CE_REG_ESR_ERR_LEN			1
+#define CE_REG_ESR_ERR_KEYSRAM		2
+
+#define CE_REG_ESR_ERR_ADDR			5
+
+
+#define CE_REG_ESR_CHAN_SHIFT		8
+#define CE_REG_ESR_CHAN_MASK(flow)	(0xFF << (CE_REG_ESR_CHAN_SHIFT*flow))
+
+/* About the hash/RBG control word */
+#define CE_CTL_CHAN_MASK	0
+
+#define CE_CHAN_PENDING	0x3
+
+#define CE_REG_TLR_SYMM_TYPE_SHIFT		0
+#define CE_REG_TLR_HASH_RBG_TYPE_SHIFT	1
+#define CE_REG_TLR_ASYM_TYPE_SHIFT		2
+#define CE_REG_TLR_RAES_TYPE_SHIFT		3
+
+#define CE_CMD_HASH_METHOD_SHIFT	0
+#define CE_CMD_RNG_METHOD_SHIFT	8
+#define CE_CMD_HMAC_METHOD_SHIFT	4
+
+#define CE_CTL_IV_MODE_SHIFT	8
+#define CE_CTL_HMAC_SHA1_LAST	BIT(12)
+
+#define CE_CTL_IE_SHIFT		16
+#define CE_CTL_IE_MASK 	(0x1 << CE_CTL_IE_SHIFT)
+
+#define SS_METHOD_MD5				0
+#define SS_METHOD_SHA1			1
+#define SS_METHOD_SHA224			2
+#define SS_METHOD_SHA256			3
+#define SS_METHOD_SHA384			4
+#define SS_METHOD_SHA512			5
+#define SS_METHOD_SM3				6
+
+#define SS_METHOD_PRNG			1
+#define SS_METHOD_TRNG			2
+#define SS_METHOD_DRBG			3
+
+/* About the common control word */
+#define CE_COMM_CTL_TASK_INT_SHIFT	31
+#define CE_COMM_CTL_TASK_INT_MASK	(0x1 << CE_COMM_CTL_TASK_INT_SHIFT)
+
+#define CE_CBC_MAC_LEN_SHIFT		17
+
+#define CE_HASH_IV_DEFAULT			0
+#define CE_HASH_IV_INPUT			1
+#define CE_COMM_CTL_IV_MODE_SHIFT	16
+
+#define CE_HMAC_SHA1_LAST			BIT(15)
+
+#define SS_DIR_ENCRYPT				0
+#define SS_DIR_DECRYPT				1
+#define CE_COMM_CTL_OP_DIR_SHIFT	8
+
+#define SS_METHOD_AES				0x0
+#define SS_METHOD_DES				0x1
+#define SS_METHOD_3DES			0x2
+#define SS_METHOD_SM4				0x3
+
+#define SS_METHOD_HMAC_SHA1		0x10	/*to distinguish hmac,
+										but this is not in hardware in fact*/
+#define SS_METHOD_HMAC_SHA256	0x11
+
+#define SS_METHOD_RSA				0x20
+#define SS_METHOD_DH				SS_METHOD_RSA
+#define SS_METHOD_ECC				0x21
+#define SS_METHOD_SM2				0x22
+#define SS_METHOD_RAES				0x30	/* XTS mode */
+
+#define CE_COMM_CTL_METHOD_SHIFT		0
+#define CE_COMM_CTL_METHOD_MASK		0x7F
+
+#define CE_METHOD_IS_HASH(type) ((type == SS_METHOD_MD5) \
+				|| (type == SS_METHOD_SHA1) \
+				|| (type == SS_METHOD_SHA224) \
+				|| (type == SS_METHOD_SHA256) \
+				|| (type == SS_METHOD_SHA384) \
+				|| (type == SS_METHOD_SHA512) \
+				|| (type == SS_METHOD_SM3))
+
+#define CE_METHOD_IS_AES(type) ((type == SS_METHOD_AES) \
+				|| (type == SS_METHOD_DES) \
+				|| (type == SS_METHOD_3DES) \
+				|| (type == SS_METHOD_SM4))
+
+#define CE_METHOD_IS_HMAC(type) ((type == SS_METHOD_HMAC_SHA1) \
+				|| (type == SS_METHOD_HMAC_SHA256))
+
+/* About the symmetric control word */
+
+#define CE_KEY_SELECT_INPUT			0
+#define CE_KEY_SELECT_SSK			1
+#define CE_KEY_SELECT_HUK			2
+#define CE_KEY_SELECT_RSSK			3
+#define CE_KEY_SELECT_INTERNAL_0	8
+#define CE_KEY_SELECT_INTERNAL_1	9
+#define CE_KEY_SELECT_INTERNAL_2	10
+#define CE_KEY_SELECT_INTERNAL_3	11
+#define CE_KEY_SELECT_INTERNAL_4	12
+#define CE_KEY_SELECT_INTERNAL_5	13
+#define CE_KEY_SELECT_INTERNAL_6	14
+#define CE_KEY_SELECT_INTERNAL_7	15
+#define CE_SYM_CTL_KEY_SELECT_SHIFT	20
+
+/* The identification string to indicate the key source. */
+#define CE_KS_SSK			"KEY_SEL_SSK"
+#define CE_KS_HUK			"KEY_SEL_HUK"
+#define CE_KS_RSSK			"KEY_SEL_RSSK"
+#define CE_KS_INTERNAL_0	"KEY_SEL_INTRA_0"
+#define CE_KS_INTERNAL_1	"KEY_SEL_INTRA_1"
+#define CE_KS_INTERNAL_2	"KEY_SEL_INTRA_2"
+#define CE_KS_INTERNAL_3	"KEY_SEL_INTRA_3"
+#define CE_KS_INTERNAL_4	"KEY_SEL_INTRA_4"
+#define CE_KS_INTERNAL_5	"KEY_SEL_INTRA_5"
+#define CE_KS_INTERNAL_6	"KEY_SEL_INTRA_6"
+#define CE_KS_INTERNAL_7	"KEY_SEL_INTRA_7"
+
+#define CE_CFB_WIDTH_1				0
+#define CE_CFB_WIDTH_8				1
+#define CE_CFB_WIDTH_64				2
+#define CE_CFB_WIDTH_128			3
+
+#define CE_SYM_CTL_CFB_WIDTH_SHIFT	18
+#define CE_SYM_CTL_GCM_IV_MODE_SHIFT	4
+
+#define CE_SYM_CTL_AES_CTS_LAST		BIT(16)
+
+#define CE_SYM_CTL_AES_XTS_LAST		BIT(13)
+#define CE_SYM_CTL_AES_XTS_FIRST	BIT(12)
+#define CE_COM_CTL_GCM_TAGLEN_SHIFT	17
+
+#define TAG_START       48
+#define IV_SIZE_START   64
+#define AAD_SIZE_START  72
+#define PT_SIZE_START   80
+
+#define SS_AES_MODE_ECB				0
+#define SS_AES_MODE_CBC				1
+#define SS_AES_MODE_CTR				2
+#define SS_AES_MODE_CTS				3
+#define SS_AES_MODE_OFB				4
+#define SS_AES_MODE_CFB				5
+#define SS_AES_MODE_CBC_MAC			6
+#define SS_AES_MODE_OCB				7
+#define SS_AES_MODE_GCM				8
+#define SS_AES_MODE_XTS				9
+
+#define CE_SYM_CTL_OP_MODE_SHIFT	8
+
+#define CE_CTR_SIZE_16				0
+#define CE_CTR_SIZE_32				1
+#define CE_CTR_SIZE_64				2
+#define CE_CTR_SIZE_128				3
+#define CE_SYM_CTL_CTR_SIZE_SHIFT	2
+
+#define CE_AES_KEY_SIZE_128			0
+#define CE_AES_KEY_SIZE_192			1
+#define CE_AES_KEY_SIZE_256			2
+#define CE_SYM_CTL_KEY_SIZE_SHIFT	0
+
+#define CE_IS_AES_MODE(type, mode, M) (CE_METHOD_IS_AES(type) \
+					&& (mode == SS_AES_MODE_##M))
+
+/* About the asymmetric control word */
+#define CE_RSA_OP_M_EXP		0 /* modular exponentiation */
+
+#define CE_RSA_OP_M_ADD                 1 /* modular add */
+#define CE_RSA_OP_M_MINUS               2 /* modular minus */
+#define CE_RSA_OP_M_MUL                 3 /* modular multiplication */
+
+#define CE_ASYM_CTL_RSA_OP_SHIFT		16
+
+#define CE_ECC_OP_POINT_ADD             0 /* point add */
+#define CE_ECC_OP_POINT_DBL             1 /* point double */
+#define CE_ECC_OP_POINT_MUL             2 /* point multiplication */
+#define CE_ECC_OP_POINT_VER             3 /* point verification */
+#define CE_ECC_OP_ENC                   4 /* encryption */
+#define CE_ECC_OP_DEC                   5 /* decryption */
+#define CE_ECC_OP_SIGN                  6 /* sign */
+#define CE_ECC_OP_VERIFY                7 /* verification */
+
+#define SS_SEED_SIZE			24
+
+
+/* Function declaration */
+
+u32 ss_reg_rd(u32 offset);
+void ss_reg_wr(u32 offset, u32 val);
+
+void ss_key_set(char *key, int size, ce_task_desc_t *task);
+
+int ss_pending_get(void);
+void ss_pending_clear(int flow);
+void ss_irq_enable(int flow);
+void ss_irq_disable(int flow);
+
+void ss_iv_set(char *iv, int size, ce_task_desc_t *task);
+void ss_iv_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_cnt_set(char *cnt, int size, ce_task_desc_t *task);
+void ss_cnt_get(int flow, char *cnt, int size);
+
+void ss_md_get(char *dst, char *src, int size);
+void ss_sha_final(void);
+void ss_check_sha_end(void);
+
+void ss_rsa_width_set(int size, ce_task_desc_t *task);
+void ss_rsa_op_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_ecc_width_set(int size, ce_task_desc_t *task);
+void ss_ecc_op_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_cts_last(ce_task_desc_t *task);
+
+void ss_xts_first(ce_task_desc_t *task);
+void ss_xts_last(ce_task_desc_t *task);
+
+void ss_method_set(int dir, int type, ce_task_desc_t *task);
+
+void ss_aes_mode_set(int mode, ce_task_desc_t *task);
+
+void ss_aead_mode_set(int mode, ce_task_desc_t *task);
+void ss_tag_len_set(u32 len, ce_task_desc_t *task);
+void ss_gcm_src_config(ce_scatter_t *src, u32 addr, u32 len);
+void ss_gcm_reserve_set(ce_task_desc_t *task, int iv_len, int aad_len, int pt_len);
+void ss_gcm_cnt_set(char *cnt, int size, ce_task_desc_t *task);
+void ss_gcm_iv_mode(ce_task_desc_t *task, int iv_mode);
+
+void ss_cfb_bitwidth_set(int bitwidth, ce_task_desc_t *task);
+
+void ss_wait_idle(void);
+void ss_ctrl_start(ce_task_desc_t *task, int type, int mode);
+void ss_ctrl_stop(void);
+int ss_flow_err(int flow);
+
+void ss_data_len_set(int len, ce_task_desc_t *task);
+
+int ss_reg_print(char *buf, int len);
+void ss_keyselect_set(int select, ce_task_desc_t *task);
+void ss_keysize_set(int size, ce_task_desc_t *task);
+
+void ss_rng_key_set(char *key, int size, ce_new_task_desc_t *task);
+void ss_hash_iv_set(char *iv, int size, ce_new_task_desc_t *task);
+void ss_hash_iv_mode_set(int mode, ce_new_task_desc_t *task);
+void ss_hmac_sha1_last(ce_new_task_desc_t *task);
+void ss_hmac_method_set(int type, ce_new_task_desc_t *task);
+void ss_hash_method_set(int type, ce_new_task_desc_t *task);
+void ss_rng_method_set(int hash_type, int type, ce_new_task_desc_t *task);
+void ss_hash_rng_ctrl_start(ce_new_task_desc_t *task);
+void ss_hash_data_len_set(int len, ce_new_task_desc_t *task);
+
+void ce_task_data_len_set(u32 len, u8 *dst);
+void ce_task_addr_set(u8 *vir_addr, phys_addr_t phy_addr, u8 *dst);
+phys_addr_t ce_task_addr_get(u8 *dst);
+
+#endif /* end of _SUNXI_SECURITY_SYSTEM_REG_H_ */
diff --git a/drivers/crypto/virtio/Kconfig b/drivers/crypto/virtio/Kconfig
index 01b625e4e..6d3deb025 100644
--- a/drivers/crypto/virtio/Kconfig
+++ b/drivers/crypto/virtio/Kconfig
@@ -5,7 +5,6 @@ config CRYPTO_DEV_VIRTIO
 	select CRYPTO_AEAD
 	select CRYPTO_BLKCIPHER
 	select CRYPTO_ENGINE
-	default m
 	help
 	  This driver provides support for virtio crypto device. If you
 	  choose 'M' here, this module will be called virtio_crypto.
-- 
2.17.1

