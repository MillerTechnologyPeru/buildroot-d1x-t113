From 8333ece5b585b9bc74ac4e2a91b691c682ae85a9 Mon Sep 17 00:00:00 2001
From: YuzukiTsuru <gloomyghost@gloomyghost.com>
Date: Fri, 25 Mar 2022 17:04:31 +0800
Subject: [PATCH 47/93] add sunxi mtd support

---
 drivers/mtd/Kconfig                           |  167 +-
 drivers/mtd/Makefile                          |    2 +
 drivers/mtd/awnand/Kconfig                    |   39 +
 drivers/mtd/awnand/Makefile                   |    8 +
 drivers/mtd/awnand/rawnand/Kconfig            |   46 +
 drivers/mtd/awnand/rawnand/Makefile           |   14 +
 drivers/mtd/awnand/rawnand/aw_rawnand_base.c  | 2350 +++++++++++++++++
 drivers/mtd/awnand/rawnand/aw_rawnand_bbt.c   |  426 +++
 drivers/mtd/awnand/rawnand/aw_rawnand_ids.c   |   37 +
 drivers/mtd/awnand/rawnand/aw_rawnand_nfc.c   | 1544 +++++++++++
 drivers/mtd/awnand/rawnand/aw_rawnand_nfc.h   |  571 ++++
 .../awnand/rawnand/aw_rawnand_securestorage.c |   30 +
 drivers/mtd/awnand/rawnand/aw_rawnand_spl.c   |  922 +++++++
 drivers/mtd/awnand/spinand/Kconfig            |   55 +
 drivers/mtd/awnand/spinand/Makefile           |   15 +
 drivers/mtd/awnand/spinand/physic/Makefile    |    4 +
 drivers/mtd/awnand/spinand/physic/bbt.c       |   84 +
 drivers/mtd/awnand/spinand/physic/cache.c     |  477 ++++
 drivers/mtd/awnand/spinand/physic/core.c      |  244 ++
 drivers/mtd/awnand/spinand/physic/ecc.c       |  198 ++
 drivers/mtd/awnand/spinand/physic/id.c        |  998 +++++++
 drivers/mtd/awnand/spinand/physic/ops.c       |  832 ++++++
 drivers/mtd/awnand/spinand/physic/physic.h    |  202 ++
 drivers/mtd/awnand/spinand/secure-storage.c   |  389 +++
 drivers/mtd/awnand/spinand/sunxi-common.c     |   73 +
 drivers/mtd/awnand/spinand/sunxi-core.c       |  863 ++++++
 drivers/mtd/awnand/spinand/sunxi-debug.c      |  284 ++
 drivers/mtd/awnand/spinand/sunxi-nftl-core.c  |  516 ++++
 drivers/mtd/awnand/spinand/sunxi-spinand.h    |   46 +
 drivers/mtd/chips/Kconfig                     |    6 +-
 drivers/mtd/efi.h                             |  117 +
 drivers/mtd/mtdchar.c                         |  206 ++
 drivers/mtd/mtdcore.c                         |    4 +-
 drivers/mtd/mtdpart.c                         |    2 +
 drivers/mtd/nand/onenand/Makefile             |    2 +-
 drivers/mtd/nand/onenand/samsung_mtd.c        | 1006 +++++++
 drivers/mtd/spi-nor/spi-nor.c                 |   87 +-
 drivers/mtd/sunxipart.c                       |  220 ++
 drivers/mtd/ubi/Kconfig                       |    2 +-
 39 files changed, 13077 insertions(+), 11 deletions(-)
 create mode 100644 drivers/mtd/awnand/Kconfig
 create mode 100644 drivers/mtd/awnand/Makefile
 create mode 100644 drivers/mtd/awnand/rawnand/Kconfig
 create mode 100644 drivers/mtd/awnand/rawnand/Makefile
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_base.c
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_bbt.c
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_ids.c
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_nfc.c
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_nfc.h
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_securestorage.c
 create mode 100644 drivers/mtd/awnand/rawnand/aw_rawnand_spl.c
 create mode 100644 drivers/mtd/awnand/spinand/Kconfig
 create mode 100644 drivers/mtd/awnand/spinand/Makefile
 create mode 100644 drivers/mtd/awnand/spinand/physic/Makefile
 create mode 100644 drivers/mtd/awnand/spinand/physic/bbt.c
 create mode 100644 drivers/mtd/awnand/spinand/physic/cache.c
 create mode 100644 drivers/mtd/awnand/spinand/physic/core.c
 create mode 100644 drivers/mtd/awnand/spinand/physic/ecc.c
 create mode 100644 drivers/mtd/awnand/spinand/physic/id.c
 create mode 100644 drivers/mtd/awnand/spinand/physic/ops.c
 create mode 100644 drivers/mtd/awnand/spinand/physic/physic.h
 create mode 100644 drivers/mtd/awnand/spinand/secure-storage.c
 create mode 100644 drivers/mtd/awnand/spinand/sunxi-common.c
 create mode 100644 drivers/mtd/awnand/spinand/sunxi-core.c
 create mode 100644 drivers/mtd/awnand/spinand/sunxi-debug.c
 create mode 100644 drivers/mtd/awnand/spinand/sunxi-nftl-core.c
 create mode 100644 drivers/mtd/awnand/spinand/sunxi-spinand.h
 create mode 100644 drivers/mtd/efi.h
 create mode 100644 drivers/mtd/nand/onenand/samsung_mtd.c
 create mode 100644 drivers/mtd/sunxipart.c

diff --git a/drivers/mtd/Kconfig b/drivers/mtd/Kconfig
index 42d401ea6..849aa8bb2 100644
--- a/drivers/mtd/Kconfig
+++ b/drivers/mtd/Kconfig
@@ -23,6 +23,151 @@ config MTD_TESTS
 	  WARNING: some of the tests will ERASE entire MTD device which they
 	  test. Do not use these tests unless you really know what you do.
 
+config MTD_REDBOOT_PARTS
+	tristate "RedBoot partition table parsing"
+	---help---
+	  RedBoot is a ROM monitor and bootloader which deals with multiple
+	  'images' in flash devices by putting a table one of the erase
+	  blocks on the device, similar to a partition table, which gives
+	  the offsets, lengths and names of all the images stored in the
+	  flash.
+
+	  If you need code which can detect and parse this table, and register
+	  MTD 'partitions' corresponding to each image in the table, enable
+	  this option.
+
+	  You will still need the parsing functions to be called by the driver
+	  for your particular device. It won't happen automatically. The
+	  SA1100 map driver (CONFIG_MTD_SA1100) has an option for this, for
+	  example.
+
+if MTD_REDBOOT_PARTS
+
+config MTD_REDBOOT_DIRECTORY_BLOCK
+	int "Location of RedBoot partition table"
+	default "-1"
+	---help---
+	  This option is the Linux counterpart to the
+	  CYGNUM_REDBOOT_FIS_DIRECTORY_BLOCK RedBoot compile time
+	  option.
+
+	  The option specifies which Flash sectors holds the RedBoot
+	  partition table.  A zero or positive value gives an absolute
+	  erase block number. A negative value specifies a number of
+	  sectors before the end of the device.
+
+	  For example "2" means block number 2, "-1" means the last
+	  block and "-2" means the penultimate block.
+
+config MTD_REDBOOT_PARTS_UNALLOCATED
+	bool "Include unallocated flash regions"
+	help
+	  If you need to register each unallocated flash region as a MTD
+	  'partition', enable this option.
+
+config MTD_REDBOOT_PARTS_READONLY
+	bool "Force read-only for RedBoot system images"
+	help
+	  If you need to force read-only for 'RedBoot', 'RedBoot Config' and
+	  'FIS directory' images, enable this option.
+
+endif # MTD_REDBOOT_PARTS
+
+config MTD_CMDLINE_PARTS
+	tristate "Command line partition table parsing"
+	depends on MTD
+	---help---
+	  Allow generic configuration of the MTD partition tables via the kernel
+	  command line. Multiple flash resources are supported for hardware where
+	  different kinds of flash memory are available.
+
+	  You will still need the parsing functions to be called by the driver
+	  for your particular device. It won't happen automatically. The
+	  SA1100 map driver (CONFIG_MTD_SA1100) has an option for this, for
+	  example.
+
+	  The format for the command line is as follows:
+
+	  mtdparts=<mtddef>[;<mtddef]
+	  <mtddef>  := <mtd-id>:<partdef>[,<partdef>]
+	  <partdef> := <size>[@offset][<name>][ro]
+	  <mtd-id>  := unique id used in mapping driver/device
+	  <size>    := standard linux memsize OR "-" to denote all
+	  remaining space
+	  <name>    := (NAME)
+
+	  Due to the way Linux handles the command line, no spaces are
+	  allowed in the partition definition, including mtd id's and partition
+	  names.
+
+	  Examples:
+
+	  1 flash resource (mtd-id "sa1100"), with 1 single writable partition:
+	  mtdparts=sa1100:-
+
+	  Same flash, but 2 named partitions, the first one being read-only:
+	  mtdparts=sa1100:256k(ARMboot)ro,-(root)
+
+	  If unsure, say 'N'.
+
+config MTD_AFS_PARTS
+	tristate "ARM Firmware Suite partition parsing"
+	depends on (ARM || ARM64)
+	---help---
+	  The ARM Firmware Suite allows the user to divide flash devices into
+	  multiple 'images'. Each such image has a header containing its name
+	  and offset/size etc.
+
+	  If you need code which can detect and parse these tables, and
+	  register MTD 'partitions' corresponding to each image detected,
+	  enable this option.
+
+	  You will still need the parsing functions to be called by the driver
+	  for your particular device. It won't happen automatically. The
+	  'physmap' map driver (CONFIG_MTD_PHYSMAP) does this, for example.
+
+config MTD_OF_PARTS
+	tristate "OpenFirmware partitioning information support"
+	default y
+	depends on OF
+	help
+	  This provides a partition parsing function which derives
+	  the partition map from the children of the flash node,
+	  as described in Documentation/devicetree/bindings/mtd/partition.txt.
+
+config MTD_AR7_PARTS
+	tristate "TI AR7 partitioning support"
+	---help---
+	  TI AR7 partitioning support
+
+config MTD_BCM63XX_PARTS
+	tristate "BCM63XX CFE partitioning support"
+	depends on BCM63XX || BMIPS_GENERIC || COMPILE_TEST
+	select CRC32
+	help
+	  This provides partions parsing for BCM63xx devices with CFE
+	  bootloaders.
+
+config MTD_BCM47XX_PARTS
+	tristate "BCM47XX partitioning support"
+	depends on BCM47XX || ARCH_BCM_5301X
+	help
+	  This provides partitions parser for devices based on BCM47xx
+	  boards.
+
+config MTD_SUNXI_PARTS
+	tristate "SUNXI partitioning support"
+	depends on ARCH_SUNXI
+	help
+	  This provides partitions parser for devices based on SUNXI
+	  boards.
+config UBOOT_DISP_ENABLE
+       bool "SUNXI Uboot Disp Enable"
+       depends on MTD_SUNXI_PARTS
+       default n
+       help
+         SUNXI Uboot Disp Enable,Boot1 Too Big Need Change Mbr offset
+
 menu "Partition parsers"
 source "drivers/mtd/parsers/Kconfig"
 endmenu
@@ -35,6 +180,14 @@ comment "User Modules And Translation Layers"
 config MTD_BLKDEVS
 	tristate
 
+config MTD_CHAR
+       tristate "Direct char device access to MTD devices"
+       help
+         This provides a character device for each MTD device present in
+         the system, allowing the user to read and write directly to the
+         memory chips, and also use ioctl() to obtain information about
+         the device, or to erase parts of it.
+
 config MTD_BLOCK
 	tristate "Caching block device access to MTD devices"
 	depends on BLOCK
@@ -170,13 +323,23 @@ config MTD_OOPS
 	  buffer in a flash partition where it can be read back at some
 	  later point.
 
+config MTD_PSTORE
+	tristate "Log panic/oops to an MTD buffer based on pstore"
+	depends on PSTORE_BLK
+	help
+	  This enables panic and oops messages to be logged to a circular
+	  buffer in a flash partition where it can be read back as files after
+	  mounting pstore filesystem.
+
+	  If unsure, say N.
+
 config MTD_SWAP
 	tristate "Swap on MTD device support"
 	depends on MTD && SWAP
 	select MTD_BLKDEVS
 	help
 	  Provides volatile block device driver on top of mtd partition
-	  suitable for swapping.  The mapping of written blocks is not saved.
+          suitable for swapping.  The mapping of written blocks is not saved.
 	  The driver provides wear leveling by storing erase counter into the
 	  OOB.
 
@@ -201,6 +364,8 @@ source "drivers/mtd/devices/Kconfig"
 
 source "drivers/mtd/nand/Kconfig"
 
+source "drivers/mtd/awnand/Kconfig"
+
 source "drivers/mtd/lpddr/Kconfig"
 
 source "drivers/mtd/spi-nor/Kconfig"
diff --git a/drivers/mtd/Makefile b/drivers/mtd/Makefile
index 56cc60ccc..2932f609d 100644
--- a/drivers/mtd/Makefile
+++ b/drivers/mtd/Makefile
@@ -8,6 +8,7 @@ obj-$(CONFIG_MTD)		+= mtd.o
 mtd-y				:= mtdcore.o mtdsuper.o mtdconcat.o mtdpart.o mtdchar.o
 
 obj-y				+= parsers/
+obj-$(CONFIG_MTD_SUNXI_PARTS)	+= sunxipart.o
 
 # 'Users' - code which presents functionality to userspace.
 obj-$(CONFIG_MTD_BLKDEVS)	+= mtd_blkdevs.o
@@ -29,4 +30,5 @@ obj-y		+= chips/ lpddr/ maps/ devices/ nand/ tests/
 
 obj-$(CONFIG_MTD_SPI_NOR)	+= spi-nor/
 obj-$(CONFIG_MTD_UBI)		+= ubi/
+obj-y				+= awnand/
 obj-$(CONFIG_MTD_HYPERBUS)	+= hyperbus/
diff --git a/drivers/mtd/awnand/Kconfig b/drivers/mtd/awnand/Kconfig
new file mode 100644
index 000000000..15559bed5
--- /dev/null
+++ b/drivers/mtd/awnand/Kconfig
@@ -0,0 +1,39 @@
+
+menu "sunxi-nand"
+choice
+	prompt "AWNAND CHOICE"
+	default AW_MTD_SPINAND
+	optional
+
+config AW_MTD_SPINAND
+	tristate "Allwinner MTD SPINAND Device Support"
+	depends on MTD
+	depends on ARCH_SUNXI
+	select AW_SPINAND_PHYSICAL_LAYER
+	select AW_SPINAND_SECURE_STORAGE
+	select MTD_UBI
+	help
+	  Enables support for SPINAND Flash chips on Allwinner SoCs.
+	  It's different with Allwinner's privately nand driver that it use
+	  ubi system rather than Allwinner's NFTL.
+
+config AW_MTD_RAWNAND
+	tristate "Allwinner MTD RAWNAND Device Support"
+	depends on MTD
+	depends on ARCH_SUNXI
+	select AW_RAWNAND_SECURE_STORAGE
+	select MTD_UBI
+	help
+	  Enables support for RAWNAND Flash chips on Allwinner SoCs.
+	  It's different with Allwinner's privately nand driver that it use
+	  ubi system rather than Allwinner's NFTL.
+endchoice
+
+if AW_MTD_SPINAND
+source "drivers/mtd/awnand/spinand/Kconfig"
+endif
+
+if AW_MTD_RAWNAND
+source "drivers/mtd/awnand/rawnand/Kconfig"
+endif
+endmenu
diff --git a/drivers/mtd/awnand/Makefile b/drivers/mtd/awnand/Makefile
new file mode 100644
index 000000000..d3b1b4868
--- /dev/null
+++ b/drivers/mtd/awnand/Makefile
@@ -0,0 +1,8 @@
+#
+# Makefile for the AWNAND MTD
+#
+
+obj-$(CONFIG_AW_MTD_SPINAND) += spinand/
+obj-$(CONFIG_AW_MTD_RAWNAND) += rawnand/
+
+
diff --git a/drivers/mtd/awnand/rawnand/Kconfig b/drivers/mtd/awnand/rawnand/Kconfig
new file mode 100644
index 000000000..fb73693d8
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/Kconfig
@@ -0,0 +1,46 @@
+#
+# SPDX-License-Identifier: GPL-2.0+
+# (C) Copyright 2020 - 2021
+# Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+# cuizhikui <cuizhikui@allwinnertech.com>
+
+
+config AW_RAWNAND_PSTORE_MTD_PART
+	bool "create pstore mtd partition for aw ubi rawnand"
+	depends on AW_MTD_RAWNAND
+	select MTD_PSTORE
+	help
+	  Whether create pstore mtd partition, which is need by pstroe-blk.
+	  If you want linux kernel dump log to spinand when oops/panic, you
+	  should create pstreo mtd partition by this configure.
+
+	  If unsure, say no.
+
+config AW_RAWNAND_SIMULATE_MULTIPLANE
+	bool "enable simulate multiplane"
+	default y
+	help
+	  spinand do not support multiplane. In order to adapt to aw nand
+	  we simulate multiplane. If set, the common physical layer should
+	  merge two continuous physical block to 'super block' for logical
+	  layer.
+
+	  Merge pages in two adjacent blocks with the same page num to super
+	  page. Merge adjacent blocks to super block.
+
+	  *   phy-block0   phy-block1    = super block 0
+	  * |------------|------------|
+	  * | phy-page 0 | phy-page 0 |  = super page 0 on super block 0
+	  * | phy-page 1 | phy-page 1 |  = super page 1 on super block 0
+	  * |     ...    |     ...    |
+	  * |------------|------------|
+
+	  If unsure, say Y.
+
+config AW_RAWNAND_BURN_CHECK_BOOT0
+	bool "upload boot0 to check after download boot0 img"
+	depends on AW_MTD_RAWNAND
+
+config AW_RAWNAND_BURN_CHECK_UBOOT
+	bool "upload uboot to check after download uboot img"
+	depends on AW_MTD_RAWNAND
diff --git a/drivers/mtd/awnand/rawnand/Makefile b/drivers/mtd/awnand/rawnand/Makefile
new file mode 100644
index 000000000..2048b3336
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/Makefile
@@ -0,0 +1,14 @@
+#
+# SPDX-License-Identifier: GPL-2.0+
+# (C) Copyright 2020 - 2021
+# Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+# cuizhikui <cuizhikui@allwinnertech.com>
+#
+# Makefile for the RAWNAND(SLC) MTD
+#
+
+obj-$(CONFIG_AW_MTD_RAWNAND) += aw-rawnand-slc.o
+
+aw-rawnand-slc-objs += aw_rawnand_nfc.o aw_rawnand_base.o
+aw-rawnand-slc-objs += aw_rawnand_ids.o aw_rawnand_bbt.o
+aw-rawnand-slc-objs += aw_rawnand_securestorage.o aw_rawnand_spl.o
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_base.c b/drivers/mtd/awnand/rawnand/aw_rawnand_base.c
new file mode 100644
index 000000000..868735cae
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_base.c
@@ -0,0 +1,2350 @@
+/**
+ * SPDX-License-Identifier: GPL-2.0+
+ * aw_rawnand_base.c
+ *
+ * (C) Copyright 2020 - 2021
+ * Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/errno.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/partitions.h>
+#include <linux/mtd/aw-rawnand.h>
+#include "aw_rawnand_nfc.h"
+
+#define DRIVER_NAME	"awrawnand-mtd"
+
+
+#define VERSION "v1.00 2020-12-23 16:55"
+#define DISPLAY_VERSION		printk("awnand-mtd-version:%s\n", VERSION)
+
+struct aw_nand_chip awnand_chip;
+extern struct aw_nand_host aw_host;
+
+void dump_data(void *data, int len)
+{
+	int i = 0;
+	uint8_t *p = (uint8_t *)data;
+
+
+	for (i = 0; i < len; i++) {
+		if (len % 16 == 0)
+			printk("%08x: ", i);
+		printk("%02x ", p[i]);
+		if (len % 16 == 15)
+			printk("\n");
+
+	}
+	printk("\nlen@%d\n", len);
+}
+
+static int aw_rawnand_get_ecc_mode(int sparesize, int pagesize)
+{
+	int cal_ecc = (((sparesize / B_TO_KB(pagesize)) - 4) / 14) * 8;
+	int ecc_mode = 0;
+
+	if (cal_ecc >= 16 && cal_ecc < 24)
+		ecc_mode = BCH_16;
+	else if (cal_ecc >= 24 && cal_ecc < 28)
+		ecc_mode = BCH_24;
+	else if (cal_ecc >= 28 && cal_ecc < 32)
+		ecc_mode = BCH_28;
+	else if (cal_ecc >= 32 && cal_ecc < 40)
+		ecc_mode = BCH_32;
+	else if (cal_ecc >= 40 && cal_ecc < 44)
+		ecc_mode = BCH_40;
+	else if (cal_ecc >= 44 && cal_ecc < 48)
+		ecc_mode = BCH_44;
+	else if (cal_ecc >= 48 && cal_ecc < 52)
+		ecc_mode = BCH_48;
+	else if (cal_ecc >= 52 && cal_ecc < 56)
+		ecc_mode = BCH_52;
+	else if (cal_ecc >= 56 && cal_ecc < 60)
+		ecc_mode = BCH_56;
+	else if (cal_ecc >= 60 && cal_ecc < 64)
+		ecc_mode = BCH_60;
+	else if (cal_ecc >= 64 && cal_ecc < 68)
+		ecc_mode = BCH_64;
+	else if (cal_ecc >= 68 && cal_ecc < 72)
+		ecc_mode = BCH_68;
+	else if (cal_ecc >= 72 && cal_ecc < 76)
+		ecc_mode = BCH_72;
+	else if (cal_ecc >= 76 && cal_ecc < 80)
+		ecc_mode = BCH_76;
+	else if (cal_ecc >= 80)
+		ecc_mode = BCH_80;
+	else
+		ecc_mode = BCH_NO;
+
+	return ecc_mode;
+}
+
+/*get ecc mode interger operation may be cause 1 level ecc mode different,
+ * so test and correct*/
+static int aw_rawnand_ecc_mode_test(int sparesize, int pagesize, int ecc_mode, int *ret_ecc_mode)
+{
+	int ecc_bits = ecc_bits_tbl[ecc_mode];
+	int val = 0;
+	int res = 0;
+	int res2 = 0;
+	int ret = 0;
+
+	val = ((14 * ecc_bits) >> 3);
+	val *= B_TO_KB(pagesize);
+
+	res = sparesize - val;
+	/*at least reserve 8 Byte avalid spare*/
+	if (res > 8) {
+		val = ((14 * ecc_bits_tbl[ecc_mode+1]) >> 3);
+		val *= B_TO_KB(pagesize);
+		res2 = sparesize - val;
+		if (res2 >= 8) {
+			*ret_ecc_mode = ecc_mode + 1;
+		} else
+			*ret_ecc_mode = ecc_mode;
+	} else if (res < 0)
+		ret = -EINVAL;
+	else
+		*ret_ecc_mode = ecc_mode;
+
+	return ret;
+}
+
+static int aw_rawnand_get_ecc_bits(int bch_mode)
+{
+	int ecc_bits = 0;
+	switch (bch_mode) {
+	case BCH_16:
+		ecc_bits = 16;
+		break;
+	case BCH_24:
+		ecc_bits = 24;
+		break;
+	case BCH_28:
+		ecc_bits = 28;
+		break;
+	case BCH_32:
+		ecc_bits = 32;
+		break;
+	case BCH_40:
+		ecc_bits = 40;
+		break;
+	case BCH_44:
+		ecc_bits = 44;
+		break;
+	case BCH_48:
+		ecc_bits = 48;
+		break;
+	case BCH_52:
+		ecc_bits = 52;
+		break;
+	case BCH_56:
+		ecc_bits = 56;
+		break;
+	case BCH_60:
+		ecc_bits = 60;
+		break;
+	case BCH_64:
+		ecc_bits = 64;
+		break;
+	case BCH_68:
+		ecc_bits = 68;
+		break;
+	case BCH_72:
+		ecc_bits = 72;
+		break;
+	case BCH_76:
+		ecc_bits = 76;
+		break;
+	case BCH_80:
+		ecc_bits = 80;
+		break;
+	default:
+		awrawnand_err("get ecc bits err\n");
+		ecc_bits = -1;
+		break;
+	}
+
+	return ecc_bits;
+}
+
+void aw_rawnand_select_chip(struct mtd_info *mtd, int c)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	awrawnand_chip_trace("Enter %s c@%d\n", __func__, c);
+
+	if (unlikely(c > MAX_CHIPS) && unlikely(c != -1)) {
+		awrawnand_err("chip@%d exceed\n", c);
+	}
+
+	if (c != -1) {
+		chip->selected_chip.chip_no = c;
+		chip->selected_chip.ceinfo[c].ce_no = c;
+		chip->selected_chip.ceinfo[c].relate_rb_no = c;
+		aw_host_nfc_chip_select(&host->nfc_reg, c);
+	} else {
+		chip->selected_chip.chip_no = c;
+		/*select invalid CE to selected chip*/
+		aw_host_nfc_chip_select(&host->nfc_reg, 0xf);
+	}
+	awrawnand_chip_trace("Exit %s c@%d\n", __func__, c);
+}
+
+static int aw_rawnand_chip_reset(struct mtd_info *mtd, int c)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+
+	NORMAL_REQ_CMD(req, RAWNAND_CMD_RESET);
+
+	awrawnand_chip_trace("Enter %s c@%d\n", __func__, c);
+
+	chip->select_chip(mtd, c);
+
+	ret = host->normal_op(chip, &req);
+	if (!ret)
+		ret = chip->dev_ready_wait(mtd);
+	chip->select_chip(mtd, -1);
+
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret ? 0 : -EIO;
+}
+
+/*
+ * it judges wheter device is busy or ready by RB line
+ * timeout 60s
+ * */
+static bool aw_rawnand_dev_ready_wait(struct mtd_info *mtd)
+{
+	int ret = -ETIMEDOUT;
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	unsigned long timeout = 0;
+
+	awrawnand_chip_trace("Enter %s\n", __func__);
+
+	timeout = jiffies + msecs_to_jiffies(60000);
+
+	do {
+		if (chip->selected_chip.chip_no != -1) {
+			ret = host->rb_ready(chip, host);
+			if (ret)
+				goto out;
+		}
+	} while (time_before(jiffies, timeout));
+
+	if (ret == false)
+		awrawnand_err("dev is busy(rb#)\n");
+
+	awrawnand_chip_trace("Exit %s\n", __func__);
+out:
+	return ret;
+}
+
+static int aw_rawnand_chip_status(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	uint8_t status = 0;
+	int ret = 0;
+
+	NORMAL_REQ_CMD_WITH_ADDR0_DATA_IN(req, RAWNAND_CMD_STATUS, &status, 1);
+
+	awrawnand_chip_trace("Enter %s\n", __func__);
+
+	if (chip->selected_chip.chip_no != -1) {
+		ret = host->rb_ready(chip, host);
+		if (!ret) {
+			awrawnand_err("dev is busy, to get status fail\n");
+			goto out;
+		}
+		ret = host->normal_op(chip, &req);
+		if (ret)
+			awrawnand_err("%s cmd@%d fail\n", __func__, RAWNAND_CMD_STATUS);
+	}
+
+out:
+	awrawnand_chip_trace("Exit %s status@%x\n", __func__, status);
+	return status;
+}
+
+static int aw_rawnand_read_id(struct mtd_info *mtd, struct aw_nand_chip *chip, uint8_t *id)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	int ret = 0;
+
+	/*send read id command@90h*/
+	NORMAL_REQ_CMD_WITH_ADDR1_DATA_IN(req, RAWNAND_CMD_READID, 0x00, id, RAWNAND_MAX_ID_LEN);
+
+	awrawnand_chip_trace("Enter %s \n", __func__);
+
+	ret = host->normal_op(chip, &req);
+	if (ret) {
+		awrawnand_err("read id fail\n");
+	}
+
+	awrawnand_chip_trace("Exit %s \n", __func__);
+
+	return ret;
+}
+
+int aw_rawnand_chip_erase(struct mtd_info *mtd, int page)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	uint8_t status = 0;
+
+	uint8_t row1 = page & 0xff;
+	uint8_t row2 = (page >> 8) & 0xff;
+	uint8_t row3 = (page >> 16) & 0xff;
+
+	NORMAL_REQ_CMD_WITH_ADDR_N3(req, RAWNAND_CMD_ERASE1, row1, row2, row3);
+	NORMAL_REQ_CMD(req2, RAWNAND_CMD_ERASE2);
+
+	awrawnand_chip_trace("Enter %s block@%d\n", __func__, page >> chip->pages_per_blk_shift);
+
+	ret = host->normal_op(chip, &req);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE1, page);
+
+	ret = host->normal_op(chip, &req2);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE2, page);
+
+	if (!chip->dev_ready_wait(mtd))
+		awrawnand_err("device is busy after erase cmd@%x page@%d\n", RAWNAND_CMD_ERASE2, page);
+
+	status = chip->dev_status(mtd);
+
+	if (status & RAWNAND_STATUS_FAIL) {
+		ret = 0;
+		awrawnand_err("%s erase block@%d fail\n", __func__, page >> chip->pages_per_blk_shift);
+	}
+
+	awrawnand_chip_trace("Exit %s \n", __func__);
+
+	return ret;
+}
+
+int aw_rawnand_chip_real_multi_erase(struct mtd_info *mtd, int page)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	uint8_t status = 0;
+
+	int blkA = ((page >> chip->pages_per_blk_shift) << 1);
+	int pageA = (blkA << chip->pages_per_blk_shift);
+	int pageB = (pageA + (1 << chip->pages_per_blk_shift));
+
+	uint8_t rowA_1 = pageA & 0xff;
+	uint8_t rowA_2 = (pageA >> 8) & 0xff;
+	uint8_t rowA_3 = (pageA >> 16) & 0xff;
+
+	uint8_t rowB_1 = pageB & 0xff;
+	uint8_t rowB_2 = (pageB >> 8) & 0xff;
+	uint8_t rowB_3 = (pageB >> 16) & 0xff;
+
+	NORMAL_REQ_CMD_WITH_ADDR_N3(reqA, RAWNAND_CMD_ERASE1, rowA_1, rowA_2, rowA_3);
+
+	NORMAL_REQ_CMD_WITH_ADDR_N3(reqB, RAWNAND_CMD_ERASE1, rowB_1, rowB_2, rowB_3);
+
+	NORMAL_REQ_CMD(req2, RAWNAND_CMD_ERASE2);
+
+	awrawnand_chip_trace("Enter %s block@[%d:%d]\n", __func__,
+			blkA, blkA + 1);
+
+	ret = host->normal_op(chip, &reqA);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE1, page);
+
+	ret = host->normal_op(chip, &reqB);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE1, page);
+
+	ret = host->normal_op(chip, &req2);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE2, page);
+
+	if (!chip->dev_ready_wait(mtd))
+		awrawnand_err("device is busy after erase cmd@%x page@%d\n", RAWNAND_CMD_ERASE2, page);
+
+	status = chip->dev_status(mtd);
+
+	if (status & RAWNAND_STATUS_FAIL) {
+		ret = 0;
+		awrawnand_err("%s erase block@%d fail\n", __func__, page >> chip->pages_per_blk_shift);
+	}
+
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+
+	return ret;
+}
+
+int aw_rawnand_chip_real_onfi_multi_erase(struct mtd_info *mtd, int page)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	uint8_t status = 0;
+
+	int blkA = ((page >> chip->pages_per_blk_shift) << 1);
+	int pageA = (blkA << chip->pages_per_blk_shift);
+	int pageB = (pageA + (1 << chip->pages_per_blk_shift));
+
+	uint8_t rowA_1 = pageA & 0xff;
+	uint8_t rowA_2 = (pageA >> 8) & 0xff;
+	uint8_t rowA_3 = (pageA >> 16) & 0xff;
+
+	uint8_t rowB_1 = pageB & 0xff;
+	uint8_t rowB_2 = (pageB >> 8) & 0xff;
+	uint8_t rowB_3 = (pageB >> 16) & 0xff;
+
+	NORMAL_REQ_CMD_WITH_ADDR_N3(reqA, RAWNAND_CMD_ERASE1, rowA_1, rowA_2, rowA_3);
+
+	NORMAL_REQ_CMD(req2, RAWNAND_CMD_MULTIERASE);
+
+	NORMAL_REQ_CMD_WITH_ADDR_N3(reqB, RAWNAND_CMD_ERASE1, rowB_1, rowB_2, rowB_3);
+
+	NORMAL_REQ_CMD(req3, RAWNAND_CMD_ERASE2);
+
+	awrawnand_chip_trace("Enter %s block@[%d:%d]\n", __func__,
+			blkA, blkA + 1);
+
+	ret = host->normal_op(chip, &reqA);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE1, page);
+
+	ret = host->normal_op(chip, &req2);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE2, page);
+
+	if (!chip->dev_ready_wait(mtd))
+		awrawnand_err("device is busy after erase cmd@%x page@%d\n", RAWNAND_CMD_ERASE2, page);
+
+	ret = host->normal_op(chip, &reqB);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE1, page);
+
+	ret = host->normal_op(chip, &req3);
+	if (ret)
+		awrawnand_err("%s cmd@%d page@%d fail\n", __func__, RAWNAND_CMD_ERASE2, page);
+
+	if (!chip->dev_ready_wait(mtd))
+		awrawnand_err("device is busy after erase cmd@%x page@%d\n", RAWNAND_CMD_ERASE2, page);
+
+	status = chip->dev_status(mtd);
+
+	if (status & RAWNAND_STATUS_FAIL) {
+		ret = 0;
+		awrawnand_err("%s erase block@%d fail\n", __func__, page >> chip->pages_per_blk_shift);
+	}
+
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+
+	return ret;
+}
+
+int aw_rawnand_chip_simu_multi_erase(struct mtd_info *mtd, int page)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+	int blkA = ((page >> chip->pages_per_blk_shift) << 1);
+	int pageA = (blkA << chip->pages_per_blk_shift);
+	int pageB = (pageA + (1 << chip->pages_per_blk_shift));
+	int ret = 0;
+
+	awrawnand_chip_trace("Enter %s block@[%d:%d]\n", __func__,
+			blkA, blkA + 1);
+
+	ret = aw_rawnand_chip_erase(mtd, pageA);
+	if (!ret) {
+		ret = aw_rawnand_chip_erase(mtd, pageB);
+	}
+
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+
+	return ret;
+}
+
+int aw_rawnand_chip_multi_erase(struct mtd_info *mtd, int page)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+	if (RAWNAND_HAS_MULTI_ERASE(chip))
+		ret = aw_rawnand_chip_real_multi_erase(mtd, page);
+	else if (RAWNAND_HAS_MULTI_ONFI_ERASE(chip))
+		ret = aw_rawnand_chip_real_onfi_multi_erase(mtd, page);
+	else
+		ret = aw_rawnand_chip_simu_multi_erase(mtd, page);
+
+	return ret;
+}
+
+int aw_rawnand_chip_write_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	uint8_t status = 0;
+	int row_cycles = chip->row_cycles;
+
+
+	BATCH_REQ_WRITE(req, page, row_cycles, mdata, mlen, sdata, slen);
+
+	awrawnand_chip_trace("Enter %s page@%d\n", __func__, page);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &req);
+	if (ret == ECC_ERR) {
+		awrawnand_err("%s write page@%d fail\n", __func__, 0);
+		goto out;
+	}
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	status = chip->dev_status(mtd);
+	if (status & RAWNAND_STATUS_FAIL) {
+		awrawnand_err("write page@%d fail\n", page);
+		ret = -EIO;
+	}
+
+out:
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+int aw_rawnand_chip_cache_write_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	int row_cycles = chip->row_cycles;
+
+
+	BATCH_REQ_CACHE_WRITE(req, page, row_cycles, mdata, mlen, sdata, slen);
+
+	awrawnand_chip_trace("Enter %s page@%d\n", __func__, page);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &req);
+	if (ret == ECC_ERR) {
+		awrawnand_err("%s write page@%d fail\n", __func__, 0);
+		goto out;
+	}
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+out:
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+/**
+ * aw_rawnand_chip_real_multi_write_page - multi plane write
+ * @mtd: MTD structure
+ * @chip: aw nand chip strucutre
+ * @mdata: data
+ * @mlen: mdatalen, is equal to 2*chip->pagesize
+ * @sdata: spare data
+ * @slen: spare len(chip->avalid_sparesize)
+ * @page: pageno
+ * **/
+int aw_rawnand_chip_real_multi_write_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	uint8_t status = 0;
+	int row_cycles = chip->row_cycles;
+	int blkA = ((page >> chip->pages_per_blk_shift) << 1);
+	int pageA = ((blkA << chip->pages_per_blk_shift) + (page & chip->pages_per_blk_mask));
+	int pageB = (pageA + (1 << chip->pages_per_blk_shift));
+
+	BATCH_REQ_MULTI_WRITE(reqA, pageA, row_cycles, mdata, chip->pagesize, sdata, slen, PLANE_A);
+
+	BATCH_REQ_WRITE(reqB, pageB, row_cycles, (mdata + chip->pagesize),
+			chip->pagesize, sdata, slen);
+
+	awrawnand_chip_trace("Enter %s page@[%d:%d]\n", __func__, pageA, pageB);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &reqA);
+	if (ret == ECC_ERR) {
+		awrawnand_err("%s write page@%d fail\n", __func__, 0);
+		goto out;
+	}
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &reqB);
+	if (ret == ECC_ERR) {
+		awrawnand_err("%s write page@%d fail\n", __func__, 0);
+		goto out;
+	}
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	status = chip->dev_status(mtd);
+	if (status & RAWNAND_STATUS_FAIL) {
+		awrawnand_err("write page@%d fail\n", page);
+		ret = -EIO;
+	}
+
+out:
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+
+}
+
+int aw_rawnand_chip_simu_multi_write_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	int ret = 0;
+
+	int blkA = ((page >> chip->pages_per_blk_shift) << 1);
+	int pageA = ((blkA << chip->pages_per_blk_shift) + (page & chip->pages_per_blk_mask));
+	int pageB = (pageA + (1 << chip->pages_per_blk_shift));
+
+	awrawnand_chip_trace("Enter %s page@[%d:%d]\n", __func__, pageA, pageB);
+
+	ret = aw_rawnand_chip_write_page(mtd, chip, mdata, mlen, sdata, slen, pageA);
+	if (!ret) {
+		ret = aw_rawnand_chip_write_page(mtd, chip, (mdata + chip->pagesize),
+				chip->pagesize, sdata, slen, pageB);
+	}
+
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+
+	return ret;
+}
+
+int aw_rawnand_chip_multi_write_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	int ret = 0;
+	if (RAWNAND_HAS_MULTI_WRITE(chip))
+		ret = aw_rawnand_chip_real_multi_write_page(mtd, chip, mdata, mlen, sdata, slen, page);
+	else
+		ret = aw_rawnand_chip_simu_multi_write_page(mtd, chip, mdata, mlen, sdata, slen, page);
+
+	return ret;
+}
+
+int aw_rawnand_chip_read_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	int row_cycles = chip->row_cycles;
+
+	int ret = 0;
+
+	BATCH_REQ_READ(req, page, row_cycles, mdata, mlen, sdata, slen);
+
+	awrawnand_chip_trace("Enter %s page@%d\n", __func__, page);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy read page@%d fail\n", page);
+		ret = -EIO;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &req);
+	if (ret == ECC_ERR)
+		awrawnand_err("read page@%d fail\n", page);
+
+out:
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+int aw_rawnand_chip_read_page_spare(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	int row_cycles = chip->row_cycles;
+
+	int ret = 0;
+
+	BATCH_REQ_READ_ONLY_SPARE(req, page, row_cycles, sdata, slen);
+
+	awrawnand_chip_trace("Enter %s page@%d\n", __func__, page);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy read page@%d spare fail\n", page);
+		ret = -EIO;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &req);
+	if (ret == ECC_ERR)
+		awrawnand_err("read page@%d spare fail\n", page);
+
+out:
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+int aw_rawnand_chip_simu_multi_read_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	int row_cycles = chip->row_cycles;
+
+	int ret = 0;
+	int blkA = ((page >> chip->pages_per_blk_shift) << 1);
+	int pageA = ((blkA << chip->pages_per_blk_shift) + (page & chip->pages_per_blk_mask));
+	int pageB = (pageA + (1 << chip->pages_per_blk_shift));
+
+	BATCH_REQ_READ(reqA, pageA, row_cycles, mdata, chip->pagesize, sdata, slen);
+
+	BATCH_REQ_READ(reqB, pageB, row_cycles, (mdata + chip->pagesize),
+			chip->pagesize, sdata, slen);
+
+	awrawnand_chip_trace("Enter %s page@[%d:%d]\n", __func__, pageA, pageB);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy read page@%d fail\n", pageA);
+		ret = -EIO;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &reqA);
+	if (ret == ECC_ERR)
+		awrawnand_err("read pageA@%d fail\n", pageA);
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy read page@%d fail\n", pageB);
+		ret = -EIO;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &reqB);
+	if (ret == ECC_ERR)
+		awrawnand_err("read pageB@%d fail\n", pageB);
+
+out:
+	awrawnand_chip_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+
+int aw_rawnand_chip_multi_read_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	int ret = 0;
+
+	ret =  aw_rawnand_chip_simu_multi_read_page(mtd, chip,
+			mdata, mlen, sdata, slen, page);
+
+	return ret;
+}
+
+int aw_rawnand_setup_read_retry(struct mtd_info *mtd, struct aw_nand_chip *chip)
+{
+	int ret = 0;
+
+	/*to do*/
+
+	return ret;
+}
+#if 0
+int aw_rawnand_setup_data_interface(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		int chipnr, const struct nand_data_interface *conf)
+{
+	int ret = 0;
+
+	int c = 0;
+
+	chip->select(chip, c);
+	ret = aw_host_set_interface(&chip->host);
+	chip->select(chip, -1);
+
+	return ret;
+}
+#endif
+
+int aw_rawnand_set_feature(struct aw_nand_chip *chip,
+		int feature_addr, uint8_t *feature_para)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int ret = 0;
+	int retry = 0;
+
+	/*set feature*/
+	NORMAL_REQ_CMD_WITH_ADDR1_DATA_OUT(req, RAWNAND_CMD_SET_FEATURES, feature_addr, feature_para,
+			FEATURES_PARA_LEN);
+
+	awrawnand_chip_trace("Enter %s [%x:%x]\n", __func__, feature_addr, *feature_para);
+retry0:
+	if (!chip->dev_ready_wait(mtd)) {
+		retry++;
+		if (retry < 3)
+			goto retry0;
+		else
+			goto out_ready_fail;
+	}
+	aw_host_nfc_repeat_mode_enable(&host->nfc_reg);
+	aw_host_nfc_randomize_disable(&host->nfc_reg);
+
+	ret = host->normal_op(chip, &req);
+	if (ret)
+		goto out_set_feature_fail;
+
+	aw_host_nfc_repeat_mode_disable(&host->nfc_reg);
+
+	awrawnand_chip_trace("Exit %s-%d ret@%d\n", __func__, __LINE__, ret);
+	return ret;
+
+out_ready_fail:
+	awrawnand_err("device is busy\n");
+	awrawnand_chip_trace("Exit %s-%d ret@%d\n", __func__, __LINE__, ret);
+	return ret;
+out_set_feature_fail:
+	aw_host_nfc_repeat_mode_disable(&host->nfc_reg);
+	awrawnand_err("set feature fail\n");
+	awrawnand_chip_trace("Exit %s-%d ret@%d\n", __func__, __LINE__, ret);
+	return ret;
+}
+
+int aw_rawnand_get_feature(struct aw_nand_chip *chip,
+		int feature_addr, uint8_t *feature_para)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int ret = 0;
+	int retry = 0;
+
+	/*get feature*/
+	NORMAL_REQ_CMD_WITH_ADDR1_DATA_IN(req, RAWNAND_CMD_GET_FEATURES, feature_addr, feature_para,
+			FEATURES_PARA_LEN);
+
+	awrawnand_chip_trace("Enter %s [%x:%x]\n", __func__, feature_addr, *feature_para);
+retry0:
+	if (!chip->dev_ready_wait(mtd)) {
+		retry++;
+		if (retry < 3)
+			goto retry0;
+		else
+			goto out_ready_fail;
+	}
+
+	aw_host_nfc_repeat_mode_enable(&host->nfc_reg);
+	aw_host_nfc_randomize_disable(&host->nfc_reg);
+
+	ret = host->normal_op(chip, &req);
+	if (ret)
+		goto out_get_feature_fail;
+
+	aw_host_nfc_repeat_mode_disable(&host->nfc_reg);
+	awrawnand_chip_trace("Enter %s [%x:%x]\n", __func__, feature_addr, *feature_para);
+	return ret;
+
+out_ready_fail:
+	awrawnand_err("device is busy\n");
+	awrawnand_chip_trace("Enter %s [%x:%x]\n", __func__, feature_addr, *feature_para);
+	return ret;
+
+out_get_feature_fail:
+	aw_host_nfc_repeat_mode_disable(&host->nfc_reg);
+	awrawnand_err("get feature fail\n");
+	awrawnand_chip_trace("Enter %s [%x:%x]\n", __func__, feature_addr, *feature_para);
+	return ret;
+}
+
+void aw_rawnand_chip_init_pre(struct aw_nand_chip *chip)
+{
+
+	struct rawnand_data_interface *itf = &chip->data_interface;
+
+	if (!chip->select_chip)
+		chip->select_chip = aw_rawnand_select_chip;
+
+	if (!chip->dev_ready_wait)
+		chip->dev_ready_wait = aw_rawnand_dev_ready_wait;
+
+	if (!chip->dev_status)
+		chip->dev_status = aw_rawnand_chip_status;
+
+	if (!chip->block_bad)
+		chip->block_bad = aw_rawnand_chip_block_bad;
+
+	if (!chip->block_markbad)
+		chip->block_markbad = aw_rawnand_chip_block_markbad;
+
+	if (!chip->scan_bbt)
+		chip->scan_bbt = aw_rawnand_chip_scan_bbt;
+
+	if (!chip->erase)
+		chip->erase = aw_rawnand_chip_erase;
+
+	if (!chip->multi_erase)
+		chip->multi_erase = aw_rawnand_chip_multi_erase;
+
+	if (!chip->write_page)
+		chip->write_page = aw_rawnand_chip_write_page;
+
+	if (!chip->multi_write_page)
+		chip->multi_write_page = aw_rawnand_chip_multi_write_page;
+
+	if (!chip->cache_write_page)
+		chip->cache_write_page = aw_rawnand_chip_cache_write_page;
+
+	if (!chip->read_page)
+		chip->read_page = aw_rawnand_chip_read_page;
+
+	if (!chip->multi_read_page)
+		chip->multi_read_page = aw_rawnand_chip_multi_read_page;
+
+	if (!chip->read_page_spare)
+		chip->read_page_spare = aw_rawnand_chip_read_page_spare;
+
+	if (!chip->setup_read_retry)
+		chip->setup_read_retry = aw_rawnand_setup_read_retry;
+#if 0
+	if (!chip->setup_data_interface)
+		chip->setup_data_interface = aw_rawnand_setup_data_interface;
+#endif
+	if (!itf->set_feature)
+		chip->data_interface.set_feature = aw_rawnand_set_feature;
+
+	if (!itf->get_feature)
+		chip->data_interface.get_feature = aw_rawnand_get_feature;
+#if 0
+	if (!itf->onfi_set_feature)
+		chip->data_interface.onfi_set_feature = aw_rawnand_onfi_set_feature;
+
+	if (!itf->onfi_get_feature)
+		chip->data_interface.onfi_get_feature = aw_rawnand_onfi_get_feature;
+
+	if (!itf->toggle_set_feature)
+		chip->data_interface.toggle_set_feature = aw_rawnand_toggle_set_feature;
+
+	if (!itf->toggle_get_feature)
+		chip->data_interface.toggle_get_feature = aw_rawnand_toggle_get_feature;
+#endif
+	chip->selected_chip.chip_no = -1;
+}
+
+static bool aw_rawnand_is_valid_id(uint8_t id)
+{
+	bool ret = false;
+	switch (id) {
+	case RAWNAND_MFR_TOSHIBA:
+	case RAWNAND_MFR_SAMSUNG:
+	case RAWNAND_MFR_FUJITSU:
+	case RAWNAND_MFR_NATIONAL:
+	case RAWNAND_MFR_RENESAS:
+	case RAWNAND_MFR_STMICRO:
+	case RAWNAND_MFR_HYNIX:
+	case RAWNAND_MFR_MICRON:
+	case RAWNAND_MFR_AMD:
+	case RAWNAND_MFR_MACRONIX:
+	case RAWNAND_MFR_EON:
+	case RAWNAND_MFR_SANDISK:
+	case RAWNAND_MFR_INTEL:
+	case RAWNAND_MFR_ATO:
+	case RAWNAND_MFR_GIGA:
+	/*case RAWNAND_MFR_SPANSION:*/ /*the same AMD*/
+	/*case RAWNAND_MFR_ESMT:*/ /*the same GIGA*/
+	/*case RAWNAND_MFR_MXIC:*/ /*the same MACRONIX*/
+	/*case RAWNAND_MFR_FORESEE:*/ /*the same SAMSUNG*/
+	case RAWNAND_MFR_WINBOND:
+		ret = true;
+		break;
+	default:
+		pr_warn("Unrecognized id@%02x\n", id);
+		break;
+	}
+	return ret;
+}
+
+static inline void aw_rawnand_display_chip_id(uint8_t *id, int chip)
+{
+	awrawnand_print("detect the chip@%d:%02x %02x %02x %02x %02x %02x %02x %02x\n", chip,
+			id[0], id[1], id[2], id[3], id[4], id[5], id[6], id[7]);
+}
+
+static bool aw_rawnand_detect_device(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int c = 0;
+	int ret = 0;
+	uint8_t id[RAWNAND_MAX_ID_LEN];
+
+	for (c = 0; c < MAX_CHIPS; c++) {
+		/*chip should be the same*/
+		memset(id, 0, RAWNAND_MAX_ID_LEN);
+		ret = aw_rawnand_chip_reset(mtd, c);
+		chip->select_chip(mtd, c);
+		ret = aw_rawnand_read_id(mtd, chip, id);
+		if (ret)
+			goto out_read_id_fail;
+		if (aw_rawnand_is_valid_id(id[0])) {
+			chip->chips++;
+			chip->ceinfo[c].ce_no = c;
+			chip->ceinfo[c].relate_rb_no = c;
+			memcpy(chip->id, id, RAWNAND_MAX_ID_LEN);
+			aw_rawnand_display_chip_id(chip->id, c);
+		}
+		chip->select_chip(mtd, -1);
+	}
+
+	return true;
+
+out_read_id_fail:
+	awrawnand_err("read id fail\n");
+	return false;
+
+}
+
+static bool aw_rawnand_device_match_idtab(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_flash_dev *dev = NULL;
+	int m = 0;
+	int d = 0;
+	for (m = 0; m < aw_nand_manufs.nm; m++) {
+		struct rawnand_manufacture *manuf = &aw_nand_manufs.manuf[m];
+		if (chip->id[0] != manuf->id)
+			continue;
+		for (d = 0; d < manuf->ndev; d++) {
+			dev = &manuf->dev[d];
+			if (chip->id[1] == dev->dev_id) {
+				if (!memcmp(chip->id, dev->id, dev->id_len)) {
+					chip->dev = &manuf->dev[d];
+					return true;
+				}
+			}
+		}
+	}
+	return false;
+}
+
+static int  aw_rawnand_scan_device(struct mtd_info *mtd)
+{
+	int ret = -ENODEV;
+
+	if (aw_rawnand_detect_device(mtd)) {
+		if (aw_rawnand_device_match_idtab(mtd))
+			ret = 0;
+		else
+			awrawnand_err("dev can't found in idtab\n");
+	}
+
+	return ret;
+}
+
+static int aw_rawnand_mtd_erase(struct mtd_info *mtd, struct erase_info *instr)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int page = 0, c = 0, block = 0;
+	int pages_per_block = 1 << chip->simu_pages_per_blk_shift;
+	int page_in_chip = 0;
+
+	uint64_t len = instr->len;
+
+	awrawnand_mtd_trace("Enter %s addr@%llx\n", __func__, instr->addr);
+
+	if (check_offs_len(mtd, instr->addr, instr->len)) {
+		return -EINVAL;
+	}
+
+	page = instr->addr >> chip->simu_pagesize_shift;
+	page_in_chip = page & chip->simu_chip_pages_mask;
+	c = instr->addr >> chip->simu_chip_shift;
+
+	mutex_lock(&chip->lock);
+
+	chip->select_chip(mtd, c);
+	instr->state = MTD_ERASING;
+	do {
+#if SIMULATE_MULTIPLANE
+		ret = chip->multi_erase(mtd, page_in_chip);
+#else
+		ret = chip->erase(mtd, page_in_chip);
+#endif
+		if (ret) {
+			awrawnand_err("erase block@%d fail\n", block);
+			instr->state = MTD_ERASE_FAILED;
+			instr->fail_addr = instr->addr;
+			goto out;
+		}
+
+		len -= chip->simu_erasesize;
+
+		page += pages_per_block;
+		page_in_chip = page & chip->simu_chip_pages_mask;
+
+		/*check, if we cross next chip*/
+		if (!page_in_chip) {
+			chip->select_chip(mtd, -1);
+			chip->select_chip(mtd, ++c);
+		}
+
+	} while (len > 0);
+
+	instr->state = MTD_ERASE_DONE;
+
+out:
+	chip->select_chip(mtd, -1);
+
+	mutex_unlock(&chip->lock);
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+
+	return ret;
+}
+
+static int aw_rawnand_mtd_panic_write (struct mtd_info *mtd, loff_t to, size_t len,
+			     size_t *retlen, const u_char *buf)
+{
+	int ret = 0;
+
+
+	return ret;
+}
+
+
+/**
+ * aw_rawnand_mtd_read - read data without oob
+ * @mtd: mtd device structure
+ * @from: offset to read from
+ * @ops: oob operation descrition structure
+ * */
+static int aw_rawnand_mtd_read(struct mtd_info *mtd, loff_t from, size_t len,
+		      size_t *retlen, u_char *buf)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int chipnr = 0, page = 0, page_in_chip = 0;
+	int had_readlen = 0;
+	uint8_t *mdata = (uint8_t *)buf;
+	unsigned int max_bitflips = 0;
+	bool ecc_failed = false;
+	struct aw_nand_chip_cache *buffer = &chip->buffer;
+	uint8_t *pagebuf = buffer->pagebuf;
+	int col = from & chip->simu_pagesize_mask;
+	int mlen = min_t(unsigned int, (mtd->writesize - col), len);
+
+	awrawnand_mtd_trace("Enter %s [%llx:%lx]\n", __func__, from, len);
+
+	*retlen = 0;
+
+	chipnr = (int)(from >> chip->simu_chip_shift);
+	page = (int)(from >> chip->simu_pagesize_shift);
+	page_in_chip = page & chip->simu_chip_pages_mask;
+
+	mutex_lock(&chip->lock);
+
+	chip->select_chip(mtd, chipnr);
+
+	do {
+		if (page == buffer->pageno) {
+			memcpy(mdata, pagebuf + col, mlen);
+			mtd->ecc_stats.failed = 0;
+			max_bitflips = buffer->bitflips;
+
+		} else {
+
+			/*
+			 *int len_to_read = (B_TO_KB(mlen) + (MOD(mlen, 1024) ? 1 : 0));
+			 *[>ret = chip->read_page(mtd, chip, pagebuf, buffer->page_len, NULL, 0,<]
+			 *len_to_read = (len_to_read << 10);
+			 */
+#if SIMULATE_MULTIPLANE
+			ret = chip->multi_read_page(mtd, chip, pagebuf, buffer->page_len, NULL, 0,
+					page_in_chip);
+#else
+			ret = chip->read_page(mtd, chip, pagebuf, buffer->page_len, NULL, 0,
+					page_in_chip);
+#endif
+			if (ret == ECC_LIMIT) {
+				ret = mtd->bitflip_threshold;
+				mtd->ecc_stats.corrected += chip->bitflips;
+				max_bitflips = max_t(unsigned int, max_bitflips, ret);
+				awrawnand_info("ecc limit from@0x%llx len@0x%lx in page@%d\n",
+						from, len, page);
+
+			} else if (ret == ECC_ERR) {
+				ecc_failed = true;
+				mtd->ecc_stats.failed++;
+				awrawnand_err("ecc err from@0x%llx len@0x%lx in page@%d\n",
+						from, len, page);
+			} else if (ret < 0) {
+				awrawnand_err("read from @0x%llx len@0x%lx fail in page@%d\n",
+						from, len, page);
+				break;
+			}
+			/*dump_data(pagebuf, buffer->page_len);*/
+
+			mtd->ecc_stats.failed = 0;
+			buffer->bitflips = max_bitflips;
+			memcpy(mdata, pagebuf + col, mlen);
+
+		}
+
+		had_readlen += mlen;
+		len -= mlen;
+		mdata += mlen;
+		/*buf += mlen;*/
+		mlen = min_t(unsigned int, len, mtd->writesize);
+
+		col = 0;
+		page++;
+		page_in_chip = page & chip->simu_chip_pages_mask;
+
+		if (!page_in_chip) {
+			chip->select_chip(mtd, -1);
+			chip->select_chip(mtd, ++chipnr);
+		}
+
+	} while (mlen);
+
+	buffer->pageno = INVALID_CACHE;
+
+	chip->select_chip(mtd, -1);
+
+	*retlen = had_readlen;
+
+	mutex_unlock(&chip->lock);
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+
+	if (ret < 0)
+		return ret;
+
+	if (ecc_failed)
+		return -EBADMSG;
+
+	return max_bitflips;
+}
+
+
+
+
+/**
+ * aw_rawnand_mtd_read_oob - read data with oob
+ * @mtd: MTD device structure
+ * @from: offset to read from
+ * @ops: oob operation descrition structure
+ * */
+static int aw_rawnand_mtd_read_oob(struct mtd_info *mtd, loff_t from,
+			  struct mtd_oob_ops *ops)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int chipnr = 0, page = 0, page_in_chip = 0;
+	int len = ops->len;
+	int ooblen = ops->ooblen;
+	int had_readlen = 0, had_read_ooblen = 0;
+	int slen = min_t(uint32_t, ooblen, chip->avalid_sparesize);
+	uint8_t *mdata = ops->datbuf;
+	uint8_t *sdata = ops->oobbuf;
+	unsigned int max_bitflips = 0;
+	bool ecc_failed = false;
+	struct aw_nand_chip_cache *buffer = &chip->buffer;
+	uint8_t *pagebuf = buffer->pagebuf;
+	uint8_t *oobbuf = buffer->oobbuf;
+	loff_t col = from & chip->simu_pagesize_mask;
+	/*int mlen = mtd->writesize - col;*/
+	/*int mlen = min((mtd->writesize - col), len);*/
+	int mlen = min_t(unsigned int, (mtd->writesize - col), len);
+
+	awrawnand_mtd_trace("Enter %s [%llx:%lx]\n", __func__, from, ops->len);
+
+	ops->retlen = ops->oobretlen = 0;
+
+	chipnr = (int)(from >> chip->simu_chip_shift);
+	page = (int)(from >> chip->simu_pagesize_shift);
+	page_in_chip = page & chip->simu_chip_pages_mask;
+
+
+	if (likely(!sdata))
+		slen = 0;
+
+	mutex_lock(&chip->lock);
+
+	chip->select_chip(mtd, chipnr);
+
+	do {
+
+		if (page == buffer->pageno) {
+			memcpy(mdata, pagebuf + col, mlen);
+			if (buffer->oobno != INVALID_CACHE)
+				memcpy(sdata, oobbuf, slen);
+			mtd->ecc_stats.failed = 0;
+			max_bitflips = buffer->bitflips;
+
+		} else {
+#if SIMULATE_MULTIPLANE
+			ret = chip->multi_read_page(mtd, chip, pagebuf, buffer->page_len,
+					oobbuf, buffer->oob_len, page_in_chip);
+
+#else
+			ret = chip->read_page(mtd, chip, pagebuf, buffer->page_len,
+					oobbuf, buffer->oob_len, page_in_chip);
+#endif
+			if (ret == ECC_LIMIT) {
+				ret = mtd->bitflip_threshold;
+				mtd->ecc_stats.corrected += chip->bitflips;
+				max_bitflips = max_t(unsigned int, max_bitflips, ret);
+				awrawnand_info("ecc limit from@%llx len@%lx in page@%d\n",
+						from, ops->len, page);
+
+			} else if (ret == ECC_ERR) {
+				ecc_failed = true;
+				mtd->ecc_stats.failed++;
+				awrawnand_err("ecc err from@%llx len@%lx in page@%d &&\n",
+						from, ops->len, page);
+				awrawnand_err("main data len@%d\n", buffer->page_len);
+				dump_data(pagebuf, buffer->page_len);
+				awrawnand_err("spare len@%d\n", buffer->oob_len);
+				dump_data(oobbuf, buffer->oob_len);
+			} else if (ret < 0) {
+				awrawnand_err("read from @%llx len@%lx fail in page@%d $$\n",
+						from, ops->len, page);
+				break;
+			}
+
+			mtd->ecc_stats.failed = 0;
+			buffer->bitflips = max_bitflips;
+			memcpy(mdata, pagebuf + col, mlen);
+			memcpy(sdata, oobbuf, slen);
+
+		}
+
+		had_readlen += mlen;
+		len -= mlen;
+		mdata += mlen;
+		mlen = min_t(unsigned int, len, mtd->writesize);
+
+		awrawnand_ubi_trace("had read len@%u\n", had_readlen);
+
+		if (unlikely(slen)) {
+			had_read_ooblen += slen;
+			ooblen -= slen;
+			if (unlikely(ooblen > 0))
+				sdata += slen;
+			else {
+				slen = 0;
+				sdata = NULL;
+			}
+		}
+
+		awrawnand_ubi_trace("had read oob len@%u\n", had_read_ooblen);
+		col = 0;
+		page++;
+		page_in_chip = page & chip->simu_chip_pages_mask;
+
+		if (!page_in_chip) {
+			chip->select_chip(mtd, -1);
+			chip->select_chip(mtd, ++chipnr);
+		}
+
+	} while (mlen);
+
+	buffer->pageno = INVALID_CACHE;
+	buffer->oobno = INVALID_CACHE;
+
+	chip->select_chip(mtd, -1);
+
+	mutex_unlock(&chip->lock);
+
+	ops->retlen = had_readlen;
+	if (ops->oobbuf)
+		ops->oobretlen = had_read_ooblen;
+
+	/*mutex_unlock(&chip->lock);*/
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+
+	if (ret < 0)
+		return ret;
+
+	if (ecc_failed)
+		return -EBADMSG;
+
+	return max_bitflips;
+}
+
+/**
+ * aw_rawnand_mtd_write_oob - write data
+ * @mtd: MTD device structure
+ * @to: offset to write to
+ * @ops: oob operation descrition structure
+ * */
+static int aw_rawnand_mtd_write_oob(struct mtd_info *mtd, loff_t to,
+			   struct mtd_oob_ops *ops)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int chipnr = 0;
+	int block = 0;
+	int page = 0;
+	int npage = 0;
+	int page_end = 0;
+	int page_in_chip = 0;
+	int page_in_chip_pre = 0;
+	int sub_page = 0;
+	int sub_page_in_chip = 0;
+	int write_sub_page = 0;
+	int len = ops->len;
+	int ooblen = ops->ooblen;
+	uint8_t *mdata = ops->datbuf;
+	uint8_t *sdata = ops->oobbuf;
+	int slen = min_t(unsigned int, ops->ooblen, mtd->oobavail);
+	struct aw_nand_chip_cache *buffer = &chip->buffer;
+	uint8_t *pagebuf = buffer->pagebuf;
+	uint8_t *oobbuf = buffer->oobbuf;
+	int mlen = min_t(unsigned int, ops->len, mtd->writesize);
+	int blockA = 0;
+
+	awrawnand_mtd_trace("Enter %s [%llx:%lx]\n", __func__, to, ops->len);
+	if (check_ofs_mtd_oob_ops(mtd, to, ops, 1)) {
+		awrawnand_mtd_trace("Exit %s \n", __func__);
+		return -EINVAL;
+	}
+
+	/*printk("mtdwoob:[%llu:%lu]\n", to, ops->len);*/
+	block = to >> mtd->erasesize_shift;
+	blockA = block << 1;
+	if (to & mtd->erasesize_mask)
+		sub_page = ((blockA + 1) << chip->pages_per_blk_shift);
+	else
+		sub_page = blockA << chip->pages_per_blk_shift;
+
+	if (ops->len < chip->simu_pagesize)
+		write_sub_page = 1;
+	else {
+
+		page = (int)(to >> chip->simu_pagesize_shift);
+		page_in_chip_pre = page & chip->simu_chip_pages_mask;
+
+		npage = (ops->len >> chip->simu_pagesize_shift);
+		if (ops->len & chip->simu_pagesize_mask) {
+			awrawnand_err("write len@%lu not align to simu pagesize\n", ops->len);
+			npage++;
+		}
+
+		page_end = npage + page;
+	}
+
+	mutex_lock(&chip->lock);
+
+	memcpy(pagebuf, mdata, mlen);
+
+	if (likely(!sdata))
+		slen = 0;
+	else
+		memcpy(oobbuf, sdata, slen);
+
+	chipnr = (int)(to >> chip->simu_chip_shift);
+
+
+	chip->select_chip(mtd, chipnr);
+
+
+	if (write_sub_page) {
+		sub_page_in_chip = sub_page & chip->chip_pages_mask;
+		buffer->pageno = INVALID_CACHE;
+		buffer->oobno = INVALID_CACHE;
+		ret = chip->write_page(mtd, chip, pagebuf, buffer->sub_page_len,
+				oobbuf, slen, sub_page_in_chip);
+	} else {
+		/*last page use program(0x10)*/
+		for (; page < page_end - 1; page++) {
+
+			page_in_chip = page & chip->simu_chip_pages_mask;
+
+			/*check, if we cross next chip*/
+			if ((!page_in_chip) && (page_in_chip != page_in_chip_pre)) {
+				chip->select_chip(mtd, -1);
+				chip->select_chip(mtd, ++chipnr);
+			}
+
+			page_in_chip++;
+			/*block last page should use program instr 0x10
+			 * (ONFI4.2: also can use 0x15(cache program instr), but
+			 * most flash datasheet only describle 0x10)*/
+			if (!(page_in_chip & chip->pages_per_blk_mask)) {
+				page_in_chip--;
+#if SIMULATE_MULTIPLANE
+				ret = chip->multi_write_page(mtd, chip, pagebuf, buffer->page_len,
+						oobbuf, slen, page_in_chip);
+#else
+				ret = chip->write_page(mtd, chip, pagebuf, buffer->page_len,
+						oobbuf, slen, page_in_chip);
+#endif
+				if (ret) {
+					awrawnand_err("write page@%d fail\n", page);
+					ops->retlen = 0;
+					goto out;
+				}
+
+			} else {
+
+				page_in_chip--;
+				if (RAWNAND_HAS_CACHEPROG(chip)) {
+					ret = chip->cache_write_page(mtd, chip, pagebuf, buffer->page_len,
+							oobbuf, slen, page_in_chip);
+				} else {
+#if SIMULATE_MULTIPLANE
+					ret = chip->multi_write_page(mtd, chip, pagebuf, buffer->page_len,
+							oobbuf, slen, page_in_chip);
+#else
+					ret = chip->write_page(mtd, chip, pagebuf, buffer->page_len,
+							oobbuf, slen, page_in_chip);
+#endif
+				}
+				if (ret) {
+					awrawnand_err("write page@%d fail\n", page);
+					ops->retlen = 0;
+					goto out;
+				}
+			}
+
+			/*len -= mlen;*/
+			mdata += buffer->page_len;
+			/*mlen = min_t(unsigned int, len, mtd->writesize);*/
+			memcpy(pagebuf, mdata, buffer->page_len);
+
+			if (unlikely(sdata)) {
+				ooblen -= slen;
+				if (likely(ooblen))
+					sdata += slen;
+				slen = min_t(uint32_t, ooblen, chip->avalid_sparesize);
+				sdata = slen ? sdata : NULL;
+				if (sdata)
+					memcpy(oobbuf, sdata, slen);
+				buffer->oobno = page;
+			}
+		}
+
+		page_in_chip = page & chip->simu_chip_pages_mask;
+
+		if ((!page_in_chip) && (page_in_chip != page_in_chip_pre)) {
+			chip->select_chip(mtd, -1);
+			chip->select_chip(mtd, ++chipnr);
+		}
+		/*write last page, use program(0x10)*/
+#if SIMULATE_MULTIPLANE
+		ret = chip->multi_write_page(mtd, chip, pagebuf, buffer->page_len,
+				oobbuf, slen, page_in_chip);
+#else
+		ret = chip->write_page(mtd, chip, pagebuf, buffer->page_len,
+				oobbuf, slen, page_in_chip);
+
+#endif
+
+	}
+
+out:
+	buffer->pageno = INVALID_CACHE;
+	buffer->oobno = INVALID_CACHE;
+
+	chip->select_chip(mtd, -1);
+
+	mutex_unlock(&chip->lock);
+
+	if (ret) {
+		awrawnand_err("write page@%d fail\n", page);
+		ops->retlen = 0;
+		ops->oobretlen = 0;
+	} else {
+		ops->retlen = len;
+		if (unlikely(sdata))
+			ops->oobretlen = slen;
+	}
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+/**
+ * aw_rawnand_mtd_write - write data without oob
+ * @mtd: mtd device structure
+ * @to: offset to write to
+ * @len: want to write data len
+ * @retlen: had writen len
+ * @buf: data buffer
+ * */
+static int aw_rawnand_mtd_write(struct mtd_info *mtd, loff_t to, size_t len,
+		       size_t *retlen, const u_char *buf)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int chipnr = 0;
+	int block = 0;
+	int page = 0;
+	int npage = 0;
+	int page_end = 0;
+	int page_in_chip = 0;
+	int page_in_chip_pre = 0;
+	int sub_page = 0;
+	int sub_page_in_chip = 0;
+	int write_sub_page = 0;
+	int wlen = len;
+	uint8_t *mdata = (uint8_t *)buf;
+	struct aw_nand_chip_cache *buffer = &chip->buffer;
+	uint8_t *pagebuf = buffer->pagebuf;
+	int mlen = min_t(unsigned int, wlen, mtd->writesize);
+	int blockA = 0;
+
+
+	awrawnand_mtd_trace("Enter %s [%llx:%lx]\n", __func__, to, len);
+
+	if (check_to_len(mtd, to, len)) {
+		return -EINVAL;
+	}
+
+
+	block = to >> mtd->erasesize_shift;
+	blockA = block << 1;
+	if (to & mtd->erasesize_mask)
+		sub_page = ((blockA + 1) << chip->pages_per_blk_shift);
+	else
+		sub_page = blockA << chip->pages_per_blk_shift;
+
+	if (wlen < chip->simu_pagesize)
+		write_sub_page = 1;
+	else {
+
+		page = (int)(to >> chip->simu_pagesize_shift);
+		page_in_chip_pre = page & chip->simu_chip_pages_mask;
+
+		npage = (wlen >> chip->simu_pagesize_shift);
+		if (wlen & chip->simu_pagesize_mask) {
+			awrawnand_err("write len@%d not align to simu pagesize\n", wlen);
+			npage++;
+		}
+
+		page_end = npage + page;
+	}
+
+	chipnr = (int)(to >> chip->simu_chip_shift);
+
+	mutex_lock(&chip->lock);
+
+	chip->select_chip(mtd, chipnr);
+
+	memcpy(pagebuf, mdata, mlen);
+
+	if (write_sub_page) {
+		sub_page_in_chip = sub_page & chip->chip_pages_mask;
+		buffer->pageno = INVALID_CACHE;
+		ret = chip->write_page(mtd, chip, pagebuf, buffer->sub_page_len,
+				NULL, 0, sub_page_in_chip);
+	} else {
+		/*last page use program(0x10)*/
+		for (; page < page_end - 1; page++) {
+
+			page_in_chip = page & chip->simu_chip_pages_mask;
+
+			/*check, if we cross next chip*/
+			if ((!page_in_chip) && (page_in_chip != page_in_chip_pre)) {
+				chip->select_chip(mtd, -1);
+				chip->select_chip(mtd, ++chipnr);
+			}
+
+			page_in_chip++;
+			/*block last page should use program instr 0x10
+			 * (ONFI4.2: also can use 0x15(cache program instr), but
+			 * most flash datasheet only describle 0x10)*/
+			if (!(page_in_chip & chip->pages_per_blk_mask)) {
+				page_in_chip--;
+#if SIMULATE_MULTIPLANE
+				ret = chip->multi_write_page(mtd, chip, pagebuf, buffer->page_len,
+						NULL, 0, page_in_chip);
+#else
+				ret = chip->write_page(mtd, chip, pagebuf, buffer->page_len,
+						NULL, 0, page_in_chip);
+#endif
+				if (ret) {
+					awrawnand_err("write page@%d fail\n", page);
+					*retlen = 0;
+					goto out;
+				}
+
+			} else {
+
+				page_in_chip--;
+				if (RAWNAND_HAS_CACHEPROG(chip)) {
+					ret = chip->cache_write_page(mtd, chip, pagebuf, buffer->page_len,
+							NULL, 0, page_in_chip);
+				} else {
+#if SIMULATE_MULTIPLANE
+					ret = chip->multi_write_page(mtd, chip, pagebuf, buffer->page_len,
+							NULL, 0, page_in_chip);
+#else
+					ret = chip->write_page(mtd, chip, pagebuf, buffer->page_len,
+							NULL, 0, page_in_chip);
+#endif
+				}
+				if (ret) {
+					awrawnand_err("write page@%d fail\n", page);
+					*retlen = 0;
+					goto out;
+				}
+			}
+
+			mdata += buffer->page_len;
+			memcpy(pagebuf, mdata, buffer->page_len);
+
+		}
+
+		page_in_chip = page & chip->simu_chip_pages_mask;
+
+		if ((!page_in_chip) && (page_in_chip != page_in_chip_pre)) {
+			chip->select_chip(mtd, -1);
+			chip->select_chip(mtd, ++chipnr);
+		}
+		/*write last page, use program(0x10)*/
+#if SIMULATE_MULTIPLANE
+		ret = chip->multi_write_page(mtd, chip, pagebuf, buffer->page_len,
+				NULL, 0, page_in_chip);
+#else
+		ret = chip->write_page(mtd, chip, pagebuf, buffer->page_len,
+				NULL, 0, page_in_chip);
+
+#endif
+
+	}
+
+out:
+	buffer->pageno = INVALID_CACHE;
+
+	chip->select_chip(mtd, -1);
+
+	mutex_unlock(&chip->lock);
+
+	if (ret) {
+		awrawnand_err("write page@%d fail\n", page);
+		*retlen = 0;
+	} else {
+		*retlen = wlen;
+	}
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+
+}
+
+static void aw_rawnand_mtd_sync(struct mtd_info *mtd)
+{
+
+}
+
+#ifdef TODO
+static int aw_rawnand_mtd_block_isreserved(struct mtd_info *mtd, loff_t ofs)
+{
+	int ret = 0;
+
+	return ret;
+}
+#endif
+
+/**
+ * aw_rawnand_mtd_block_isbad - check block is badblock or not
+ * @mtd: MTD device structure
+ * @ofs: offset the device start
+ */
+static int aw_rawnand_mtd_block_isbad(struct mtd_info *mtd, loff_t ofs)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int c = 0;
+	int b = 0;
+
+
+	awrawnand_mtd_trace("Enter %s [%llx]\n", __func__, ofs);
+
+	if (ofs < chip->uboot_end) {
+		awrawnand_mtd_trace("Exit %s\n", __func__);
+		return 0;
+	}
+
+	c = (int)(ofs >> chip->simu_chip_shift);
+	b = (int)(ofs >> chip->simu_erase_shift);
+
+
+	mutex_lock(&chip->lock);
+	chip->select_chip(mtd, c);
+
+	ret = chip->block_bad(mtd, b);
+
+	chip->select_chip(mtd, -1);
+	mutex_unlock(&chip->lock);
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+
+/**
+ * aw_rawnand_mtd_block_markbad - mark block at the given offset as bad block
+ * @mtd: MTD device structure
+ * @ofs: offset the device start
+ * */
+static int aw_rawnand_mtd_block_markbad(struct mtd_info *mtd, loff_t ofs)
+{
+
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+	int c = 0;
+	int b = 0;
+
+	awrawnand_mtd_trace("Enter %s [%llx]\n", __func__, ofs);
+
+	c = (int)(ofs >> chip->simu_chip_shift);
+	b = (int)(ofs >> chip->simu_erase_shift);
+
+	mutex_lock(&chip->lock);
+	chip->select_chip(mtd, c);
+
+	ret = chip->block_markbad(mtd, b);
+
+	chip->select_chip(mtd, -1);
+	mutex_unlock(&chip->lock);
+
+	awrawnand_mtd_trace("Exit %s ret@%d\n", __func__, ret);
+	return ret;
+}
+
+#ifndef __UBOOT__
+static int aw_rawnand_mtd_suspend (struct mtd_info *mtd)
+{
+	int ret = 0;
+
+	return ret;
+}
+
+static void aw_rawnand_mtd_resume(struct mtd_info *mtd)
+{
+
+}
+
+static void aw_rawnand_mtd_reboot(struct mtd_info *mtd)
+{
+
+
+}
+#endif
+
+
+static enum rawnand_data_interface_type aw_rawnand_get_itf_type(struct aw_nand_chip *chip)
+{
+	if (RAWNAND_HAS_ITF_SDR(chip))
+		return RAWNAND_SDR_IFACE;
+
+	if (RAWNAND_HAS_ITF_ONFI_DDR(chip) || RAWNAND_HAS_ITF_ONFI_DDR2(chip))
+		return RAWNAND_ONFI_DDR;
+
+	if (RAWNAND_HAS_ITF_TOGGLE_DDR(chip) || RAWNAND_HAS_ITF_TOGGLE_DDR2(chip))
+		return RAWNAND_TOGGLE_DDR;
+
+	return RAWNAND_SDR_IFACE;
+}
+
+static int aw_rawnand_chip_data_init(struct mtd_info *mtd)
+{
+
+	/*initialize chip data that not yet been completed */
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_flash_dev *dev = chip->dev;
+	int i = 0;
+	int ecc_bits = 0;
+	int bbtsize = 0;
+	int uboot_end = 0;
+
+	chip->type = SLC_NAND;
+
+	chip->ecc_mode = aw_rawnand_get_ecc_mode(dev->sparesize, dev->pagesize);
+	if (chip->ecc_mode == BCH_NO) {
+		awrawnand_err("get ecc mode err pls check id table\n");
+		goto out;
+	}
+
+	if (aw_rawnand_ecc_mode_test(dev->sparesize, dev->pagesize, chip->ecc_mode,
+				&chip->ecc_mode)) {
+		awrawnand_err("ecc mode test fail\n");
+		goto out;
+	}
+
+
+	chip->dies = dev->dies_per_chip;
+	for (i = 0; i < chip->dies; i++) {
+		chip->diesize[i] = dev->blks_per_die * dev->pages_per_blk * dev->pagesize;
+		chip->chipsize += chip->diesize[i];
+	}
+
+	chip->simu_chipsize = chip->chipsize;
+
+	chip->chip_shift = ffs(chip->chipsize) - 1;
+	chip->simu_chip_shift = chip->chip_shift;
+	chip->pages_per_blk_shift = ffs(dev->pages_per_blk) -1;
+	chip->simu_pages_per_blk_shift = chip->pages_per_blk_shift;
+	chip->pages_per_blk_mask = ((1 << chip->pages_per_blk_shift) - 1);
+	chip->simu_pages_per_blk_mask = chip->pages_per_blk_mask;
+	chip->real_pagesize = dev->pagesize + dev->sparesize;
+#if SIMULATE_MULTIPLANE
+	chip->simu_pagesize = (dev->pagesize << 1);
+#else
+	chip->simu_pagesize = dev->pagesize;
+#endif
+	chip->simu_pagesize_mask = chip->simu_pagesize - 1;
+	chip->simu_pagesize_shift = ffs(chip->simu_pagesize) - 1;
+	chip->simu_erase_shift = ffs(chip->simu_pagesize << chip->simu_pages_per_blk_shift) - 1;
+	chip->simu_erasesize = 1 << chip->simu_erase_shift;
+	chip->simu_erasesize_mask = (1 << chip->simu_erase_shift) - 1;
+	chip->simu_chip_pages = (1 << (chip->simu_chip_shift - chip->simu_pagesize_shift));
+	chip->simu_chip_pages_mask = ((1 << (chip->simu_chip_shift - chip->simu_pagesize_shift)) - 1);
+
+	chip->pagesize = dev->pagesize;
+	chip->pagesize_mask = chip->pagesize - 1;
+	chip->pagesize_shift = ffs(chip->pagesize) - 1;
+	chip->erase_shift = ffs((chip->pagesize << chip->pages_per_blk_shift)) - 1;
+	chip->erasesize = 1 << chip->erase_shift;
+	chip->erasesize_mask = (1 << chip->erase_shift) - 1;
+	chip->chip_pages = (1 << (chip->chip_shift - chip->pagesize_shift));
+	chip->chip_pages_mask = ((1 << (chip->chip_shift - chip->pagesize_shift)) - 1);
+
+	ecc_bits = aw_rawnand_get_ecc_bits(chip->ecc_mode);
+	if (ecc_bits == -1)
+		goto out;
+
+
+	chip->avalid_sparesize = dev->sparesize - ((B_TO_KB(1 << chip->pagesize_shift) * ecc_bits * 14) / 8);
+	/*4 is a ecc block user data len minimum multiple*/
+	chip->avalid_sparesize = round_down(chip->avalid_sparesize, 4);
+	chip->avalid_sparesize = min(chip->avalid_sparesize, MAX_SPARE_SIZE);
+	chip->options = dev->options;
+	chip->clk_rate = dev->access_freq;
+	chip->data_interface.type = aw_rawnand_get_itf_type(chip);
+
+	chip->random = RAWNAND_NFC_NEED_RANDOM(chip);
+	chip->row_cycles = 3;
+	chip->badblock_mark_pos = dev->badblock_flag_pos;
+
+	bbtsize = ((chip->chips * chip->chipsize) >> chip->erase_shift);
+	bbtsize = round_up(bbtsize, 8);
+	bbtsize = bbtsize >> 3;
+#if SIMULATE_MULTIPLANE
+	bbtsize = bbtsize >> 1;
+#endif
+	chip->bbt = kzalloc(bbtsize, GFP_KERNEL);
+	if (!chip->bbt) {
+		awrawnand_err("kzalloc bbt fail\n");
+		return -ENOMEM;
+	}
+
+	chip->bbtd = kzalloc(bbtsize, GFP_KERNEL);
+	if (!chip->bbtd) {
+		kfree(chip->bbt);
+		awrawnand_err("kzalloc bbtd fail\n");
+		return -ENOMEM;
+	}
+#if SIMULATE_MULTIPLANE
+	chip->buffer.page_len = (chip->pagesize << 1);
+#else
+	chip->buffer.page_len = chip->pagesize;
+
+#endif
+	chip->buffer.sub_page_len = chip->pagesize;
+
+	chip->buffer.pagebuf = kmalloc(chip->buffer.page_len, GFP_KERNEL);
+	if (!chip->buffer.pagebuf) {
+		awrawnand_err("kzalloc pagebuf fail\n");
+		kfree(chip->bbt);
+		kfree(chip->bbtd);
+		return -ENOMEM;
+	}
+	memset(chip->buffer.pagebuf, 0, chip->buffer.page_len);
+
+#if SIMULATE_MULTIPLANE
+	chip->buffer.oob_len = chip->avalid_sparesize << 1;
+#else
+	chip->buffer.oob_len = chip->avalid_sparesize;
+#endif
+	chip->buffer.oobbuf = kmalloc(chip->buffer.oob_len, GFP_KERNEL);
+	if (!chip->buffer.oobbuf) {
+		awrawnand_err("kzalloc oobbuf fail\n");
+		kfree(chip->bbt);
+		kfree(chip->bbtd);
+		kfree(chip->buffer.pagebuf);
+		return -ENOMEM;
+	}
+
+	memset(chip->buffer.oobbuf, 0, chip->buffer.oob_len);
+	chip->buffer.pageno = INVALID_CACHE;
+	chip->buffer.oobno = INVALID_CACHE;
+	chip->buffer.bitflips = 0;
+
+#ifdef __UBOOT__
+	if (chip->type == SLC_NAND) {
+		chip->write_boot0_page = rawslcnand_write_boot0_page;
+		chip->read_boot0_page = rawslcnand_read_boot0_page;
+	}
+#endif
+
+	rawnand_uboot_blknum(NULL, &uboot_end);
+	chip->uboot_end = uboot_end << chip->erase_shift;
+
+	mutex_init(&chip->lock);
+
+	awrawnand_info("chip: chip_shift@%d\n", chip->chip_shift);
+	awrawnand_info("chip: pagesize_mask@%d\n", chip->pagesize_mask);
+	awrawnand_info("chip: chip_pages@%d\n", chip->chip_pages);
+	awrawnand_info("chip: pagesize_shift@%d\n", chip->pagesize_shift);
+	awrawnand_info("chip: erase_shift@%d\n", chip->erase_shift);
+	awrawnand_info("chip: ecc_mode@%d\n", chip->ecc_mode);
+	awrawnand_info("chip: clk_rate@%d(MHz)\n", chip->clk_rate);
+	awrawnand_info("chip: avalid_sparesize@%d\n", chip->avalid_sparesize);
+	awrawnand_info("chip: pages_per_blk_shift_shift@%d\n", chip->pages_per_blk_shift);
+	return 0;
+out:
+	return -EINVAL;
+}
+
+static void aw_rawnand_chip_data_destroy(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+	kfree(chip->bbt);
+	kfree(chip->bbtd);
+	kfree(chip->buffer.pagebuf);
+	kfree(chip->buffer.oobbuf);
+}
+
+static int aw_rawnand_mtd_info_init(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	/*initialize mtd data*/
+	mtd->type = MTD_NANDFLASH; /*SLCNAND*/
+	mtd->flags = MTD_CAP_NANDFLASH;
+	mtd->owner = THIS_MODULE;
+
+	mtd->erasesize = 1 << chip->simu_erase_shift;
+	mtd->erasesize_shift = ffs(mtd->erasesize) - 1;
+	mtd->erasesize_mask = mtd->erasesize - 1;
+
+	mtd->writesize = 1 << chip->simu_pagesize_shift;
+	mtd->writesize_shift = ffs(mtd->writesize) - 1;
+	mtd->writesize_mask = mtd->writesize - 1;
+	mtd->writebufsize = mtd->writesize;
+
+	mtd->oobsize = chip->avalid_sparesize;
+	mtd->oobavail = chip->avalid_sparesize;
+
+
+#if SIMULATE_MULTIPLANE
+	/*In order to don't change user application(e.g. ubi), keep the same with aw spi nand*/
+	mtd->subpage_sft = 1;
+#else
+	mtd->subpage_sft = 0;
+#endif
+
+	/*mtd->subpage_sft = 0;*/
+
+	mtd->size = chip->chips << chip->chip_shift;
+	mtd->bitflip_threshold = ecc_limit_tab[chip->ecc_mode];
+	mtd->name = "nand0";
+	mtd->ecc_strength = ecc_bits_tbl[chip->ecc_mode];
+
+
+	mtd->_erase = aw_rawnand_mtd_erase;
+	mtd->_read = aw_rawnand_mtd_read;
+	mtd->_read_oob = aw_rawnand_mtd_read_oob;
+	mtd->_write = aw_rawnand_mtd_write;
+	mtd->_write_oob = aw_rawnand_mtd_write_oob;
+	mtd->_sync = aw_rawnand_mtd_sync;
+#ifdef TODO
+	/*can use to do boot0&uboot0 img region*/
+	mtd->_block_isreserved = aw_rawnand_mtd_block_isreserved;
+#endif
+	mtd->_block_isbad = aw_rawnand_mtd_block_isbad;
+	mtd->_block_markbad = aw_rawnand_mtd_block_markbad;
+	mtd->_panic_write = aw_rawnand_mtd_panic_write;
+#ifndef __UBOOT__
+	mtd->_suspend = aw_rawnand_mtd_suspend;
+	mtd->_resume = aw_rawnand_mtd_resume;
+	mtd->_reboot = aw_rawnand_mtd_reboot;
+#endif
+
+	awrawnand_info("mtd : type@%s\n", mtd->type == MTD_MLCNANDFLASH ? "MLCNAND" : "SLCNAND");
+	awrawnand_info("mtd : flags@nand flash\n");
+	awrawnand_info("mtd : writesize@%u\n", mtd->writesize);
+	awrawnand_info("mtd : oobsize@%d\n", mtd->oobsize);
+	awrawnand_info("mtd : size@%llu\n", mtd->size);
+	awrawnand_info("mtd : bitflips_threshold@%u\n", mtd->bitflip_threshold);
+	awrawnand_info("mtd : ecc_strength@%u\n", mtd->ecc_strength);
+
+	return 0;
+}
+
+
+static int aw_rawnand_scan_tail(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+
+	ret = aw_rawnand_chip_data_init(mtd);
+	if (ret)
+		goto out_chip_data_fail;
+
+	ret = aw_rawnand_mtd_info_init(mtd);
+	if (ret)
+		goto out_mtd_info_fail;
+
+	ret = aw_host_init_tail(host);
+
+	return ret;
+
+out_mtd_info_fail:
+	awrawnand_err("mtd info init fail\n");
+	return ret;
+
+out_chip_data_fail:
+	awrawnand_err("chip data init fail\n");
+	return ret;
+}
+
+/*return @0: not protected
+ *	@1: protected
+ **/
+static int aw_rawnand_check_wp(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	uint8_t status = 0;
+
+	chip->select_chip(mtd, 0);
+
+	if (chip->dev_ready_wait(mtd)) {
+		status = chip->dev_status(mtd);
+
+	} else
+		awrawnand_err("dev is busy, check wp fail\n");
+
+	chip->select_chip(mtd, -1);
+
+	return (status & RAWNAND_STATUS_WP) ? 0 : 1;
+}
+
+static int aw_rawnand_scan(struct mtd_info *mtd)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+
+	aw_rawnand_chip_init_pre(chip);
+
+	ret = aw_rawnand_scan_device(&chip->mtd);
+	if (ret)
+		awrawnand_err("aw rawnand scan fail@%d\n", ret);
+	else
+		ret = aw_rawnand_scan_tail(&chip->mtd);
+
+	if (aw_rawnand_check_wp(mtd)) {
+		awrawnand_err("device is in wp status,err@%d\n", -EIO);
+		ret = -EIO;
+	}
+
+	return ret;
+}
+
+static struct mtd_partition aw_rawnand_parts[] = {
+	/* .size is set by @aw_rawnand_mtd_update_mtd_parts */
+	{ .name = "boot0", .offset = 0 },
+	{ .name = "uboot", .offset = MTDPART_OFS_APPEND },
+	{ .name = "secure_storage", .offset = MTDPART_OFS_APPEND },
+#if IS_ENABLED(CONFIG_AW_RAWNAND_PSTORE_MTD_PART)
+	{ .name = "pstore", .offset = MTDPART_OFS_APPEND },
+#endif
+	{ .name = "sys", .offset = MTDPART_OFS_APPEND},
+};
+
+static void aw_rawnand_mtd_update_mtd_parts(struct aw_nand_chip *chip,
+		struct mtd_partition *mtdparts)
+{
+	unsigned int uboot_start, uboot_end;
+	int index = 0;
+	int phy_blk_bytes = chip->erasesize;
+
+	rawnand_uboot_blknum(&uboot_start, &uboot_end);
+	chip->uboot_end = uboot_end * phy_blk_bytes;
+
+	/* boot0 */
+	mtdparts[index++].size = uboot_start * phy_blk_bytes;
+	/* uboot */
+	mtdparts[index++].size = (uboot_end - uboot_start) * phy_blk_bytes;
+	/* secure storage */
+	mtdparts[index++].size = PHY_BLKS_FOR_SECURE_STORAGE * phy_blk_bytes;
+#if IS_ENABLED(CONFIG_AW_RAWNAND_PSTORE_MTD_PART)
+	/* pstore */
+	mtdparts[index++].size = PSTORE_SIZE_KB * SZ_1K;
+#endif
+	/* user data */
+	mtdparts[index++].size = MTDPART_SIZ_FULL;
+
+#if IS_ENABLED(CONFIG_AW_RAWNAND_SECURE_STORAGE)
+	struct aw_nand_sec_sto *sec_sto = get_rawnand_sec_sto();
+	sec_sto->chip = chip;
+	sec_sto->startblk = uboot_end;
+	sec_sto->endblk = uboot_end + PHY_BLKS_FOR_SECURE_STORAGE;
+#endif
+}
+
+static int aw_rawnand_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = get_rawnand();
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	struct device *dev = &pdev->dev;
+
+	memset(&awnand_chip, 0, sizeof(struct aw_nand_chip));
+	awnand_chip.priv = &aw_host;
+
+	DISPLAY_VERSION;
+
+	ret = aw_host_init(dev);
+	if (ret) {
+		awrawnand_err("host init fail@%d\n", ret);
+		goto out;
+	}
+
+	ret = aw_rawnand_scan(mtd);
+	if (ret) {
+		awrawnand_err("scan fail\n");
+		goto out;
+	}
+
+	aw_rawnand_mtd_update_mtd_parts(chip, aw_rawnand_parts);
+	ret = mtd_device_register(mtd, aw_rawnand_parts, ARRAY_SIZE(aw_rawnand_parts));
+	if (ret)
+		goto out;
+
+	platform_set_drvdata(pdev, chip);
+
+	return ret;
+
+out:
+	aw_host_exit(awnand_chip.priv);
+	memset(&awnand_chip, 0, sizeof(struct aw_nand_chip));
+	awrawnand_err("init fail\n");
+	return ret;
+}
+
+static int aw_rawnand_remove(struct platform_device *pdev)
+{
+
+	struct aw_nand_chip *chip = platform_get_drvdata(pdev);
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+#ifndef __UBOOT__
+	if (mtd_device_unregister(mtd))
+		return -1;
+#endif
+
+	aw_host_exit(host);
+
+	aw_rawnand_chip_data_destroy(mtd);
+
+	return 0;
+}
+
+static const struct of_device_id of_nand_id[] = {
+    { .compatible = "allwinner,sun50iw9-nand"},
+    { .compatible = "allwinner,sun50iw10-nand"},
+	{ .compatible = "allwinner,sun50iw11-nand"},
+    { .compatible = "allwinner,sun8iw19-nand"},
+    {/* sentinel */},
+};
+
+static struct platform_driver awrawnand_driver = {
+    .probe = aw_rawnand_probe,
+    .remove = aw_rawnand_remove,
+    .driver = {
+	.name = "aw_rawnand",
+	.owner = THIS_MODULE,
+	.of_match_table = of_nand_id,
+    },
+};
+
+static int __init awnand_init(void)
+{
+	return platform_driver_register(&awrawnand_driver);
+}
+
+static void __exit awnand_exit(void)
+{
+	platform_driver_unregister(&awrawnand_driver);
+}
+
+
+module_init(awnand_init);
+module_exit(awnand_exit);
+
+MODULE_ALIAS(DRIVER_NAME);
+MODULE_DESCRIPTION("Allwinner rawnand driver");
+MODULE_AUTHOR("cuizhikui <cuizhikui@allwinnertech.com>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_bbt.c b/drivers/mtd/awnand/rawnand/aw_rawnand_bbt.c
new file mode 100644
index 000000000..a7fb98556
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_bbt.c
@@ -0,0 +1,426 @@
+/**
+ * SPDX-License-Identifier: GPL-2.0+
+ * aw_rawnand_bbt.c
+ *
+ * (C) Copyright 2020 - 2021
+ * Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/aw-rawnand.h>
+#include <linux/slab.h>
+
+
+/**
+ * aw_rawnand_chip_check_badblock_bbt - check bad block from bbt
+ * @mtd: MTD device structure
+ * @block: block offset from device start block
+ * @return: %BBT_B_BAD the block is bad, %BBT_B_GOOD the block is good,
+ *		%BBT_B_INVALID the block in bbt can't to know bad or good
+ * */
+static int aw_rawnand_chip_check_badblock_bbt(struct mtd_info *mtd, int block)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int ret = 0;
+
+	int pos_byte = 0;
+	int pos_bit = 0;
+
+	pos_byte = block >> 3;
+	pos_bit = block & 0x7;
+
+
+	if (chip->bbtd[pos_byte] & (1 << pos_bit)) {
+		if (chip->bbt[pos_byte] & (1 << pos_bit))
+			ret = BBT_B_BAD;
+		else
+			ret = BBT_B_GOOD;
+	} else
+		ret = BBT_B_INVALID;
+
+	return ret;
+}
+
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_check_badblock_bbt);
+
+
+/**
+ * aw_rawnand_chip_updata_bbt - update bbt&bbtd
+ * @mtd : MTD device structure
+ * @block: block offset device start
+ * @flag: %BBT_B_GOOD mark the block is good in bbt,
+ *	%BBT_B_BAD mark the block is bad in bbt
+ * */
+void aw_rawnand_chip_update_bbt(struct mtd_info *mtd, int block, int flag)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	int pos_byte = 0;
+	int pos_bit = 0;
+
+	if (unlikely(!chip->bbt || !chip->bbtd)) {
+		awrawnand_warn("bbt or bbtd is null, nothing to do\n");
+		return;
+	}
+
+	pos_byte = block >> 3;
+	pos_bit = block & 0x7;
+
+	chip->bbtd[pos_byte] |= (1 << pos_bit);
+	if (flag == BBT_B_GOOD)
+		chip->bbt[pos_byte] &= ~(1 << pos_bit);
+	else
+		chip->bbt[pos_byte] |= (1 << pos_bit);
+
+	return;
+}
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_update_bbt);
+/**
+ * aw_rawnand_chip_block_bad - read bad block marker from the chip
+ * @mtd: MTD device structure
+ * @block: block offset from device start block
+ * */
+int aw_rawnand_chip_block_bad(struct mtd_info *mtd, int block)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+	int ret = 0;
+	int page = 0;
+	uint8_t spare[chip->avalid_sparesize];
+	int snd_flag = 0;
+
+	ret = aw_rawnand_chip_check_badblock_bbt(mtd, block);
+	if (ret == BBT_B_GOOD) {
+		awrawnand_dbg("block@%d bbt good\n", block);
+		return 0;
+	} else if (ret == BBT_B_BAD) {
+		awrawnand_err("block@%d bbt bad\n", block);
+		return 1;
+	} else {
+		awrawnand_dbg(
+		"block @%d in bbt is invalid , need to read block mark flag\n", block);
+	}
+
+
+	if (chip->badblock_mark_pos & FIRST_PAGE)
+		page = block << chip->pages_per_blk_shift;
+	else if (chip->badblock_mark_pos & LAST_PAGE) {
+		page = (block << chip->pages_per_blk_shift) +
+			chip->pages_per_blk_mask;
+	} else {
+		awrawnand_err("Unknow the block mark use default mark pos(first page)\n");
+	}
+
+	ret = chip->read_page_spare(mtd, chip, spare, chip->avalid_sparesize, page);
+	if (ret != ECC_ERR) {
+		if (spare[0] != 0xFF) {
+			ret = 1;
+			awrawnand_info("page@%d oob:%02x %02x %02x %02x %02x %02x %02x %02x\n", page, spare[0],
+					spare[1], spare[2], spare[3], spare[4], spare[5], spare[6], spare[7]);
+			aw_rawnand_chip_update_bbt(mtd, block, BBT_B_BAD);
+			goto out;
+		}
+	}
+
+	if ((chip->badblock_mark_pos == FIRST_TWO_PAGES)) {
+		page++;
+		snd_flag = 1;
+	} else if ((chip->badblock_mark_pos == LAST_TWO_PAGES)) {
+		page--;
+		snd_flag = 1;
+	}
+
+	if (snd_flag) {
+		ret = chip->read_page_spare(mtd, chip, spare, chip->avalid_sparesize, page);
+		if (ret != ECC_ERR) {
+			if (spare[0] != 0xFF) {
+				ret = 1;
+				awrawnand_err("page@%d:%02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n", page, spare[0],
+			spare[1], spare[2], spare[3], spare[4], spare[5], spare[6], spare[7], spare[8], spare[9]);
+				aw_rawnand_chip_update_bbt(mtd, block, BBT_B_BAD);
+			}
+		}
+	}
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_block_bad);
+
+/**
+ * aw_rawnand_chip_simu_block_bad - read bad block marker from the chip
+ * @mtd: MTD device structure
+ * @block: simu block offset from device start simu block
+ * */
+int aw_rawnand_chip_simu_block_bad(struct mtd_info *mtd, int block)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+	int ret = 0;
+	int page = 0;
+	uint8_t spare[chip->avalid_sparesize];
+	int snd_flag = 0;
+	int real_blkA = 0, real_blkB = 0, real_blk = 0;
+	int plane = 0;
+
+	ret = aw_rawnand_chip_check_badblock_bbt(mtd, block);
+	if (ret == BBT_B_GOOD) {
+		awrawnand_dbg("block@%d bbt good\n", block);
+		return 0;
+	} else if (ret == BBT_B_BAD) {
+		awrawnand_err("block@%d bbt bad\n", block);
+		return 1;
+	} else {
+		awrawnand_dbg(
+		"block @%d in bbt is invalid , need to read block mark flag\n", block);
+	}
+
+	real_blkA = (block << 1);
+	real_blkB = real_blkA + 1;
+	real_blk = real_blkA;
+
+	/*check planeA & planeB*/
+	for (plane = 0; plane < 2; plane++) {
+		if (chip->badblock_mark_pos & FIRST_PAGE)
+			page = real_blk << chip->simu_pages_per_blk_shift;
+		else if (chip->badblock_mark_pos & LAST_PAGE) {
+			page = (real_blk << chip->simu_pages_per_blk_shift) +
+				chip->simu_pages_per_blk_mask;
+		} else {
+			awrawnand_err("Unknow the block mark use default mark pos(first page)\n");
+		}
+
+		ret = chip->read_page_spare(mtd, chip, spare, chip->avalid_sparesize, page);
+		if (ret != ECC_ERR) {
+			if (spare[0] != 0xFF) {
+				ret = 1;
+				awrawnand_info("page@%d oob:%02x %02x %02x %02x %02x %02x %02x %02x\n", page, spare[0],
+						spare[1], spare[2], spare[3], spare[4], spare[5], spare[6], spare[7]);
+				aw_rawnand_chip_update_bbt(mtd, block, BBT_B_BAD);
+				break;
+			}
+		}
+
+		if ((chip->badblock_mark_pos == FIRST_TWO_PAGES)) {
+			page++;
+			snd_flag = 1;
+		} else if ((chip->badblock_mark_pos == LAST_TWO_PAGES)) {
+			page--;
+			snd_flag = 1;
+		}
+
+		if (snd_flag) {
+			ret = chip->read_page_spare(mtd, chip, spare, chip->avalid_sparesize, page);
+			if (ret != ECC_ERR) {
+				if (spare[0] != 0xFF) {
+					ret = 1;
+					awrawnand_err("page@%d:%02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n", page, spare[0],
+							spare[1], spare[2], spare[3], spare[4], spare[5], spare[6], spare[7], spare[8], spare[9]);
+					aw_rawnand_chip_update_bbt(mtd, block, BBT_B_BAD);
+					break;
+				}
+			}
+		}
+		real_blk = real_blkB;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_simu_block_bad);
+
+/**
+ * aw_rawnand_chip_block_markbad - mark a block bad in mark pos and update bbt&bbtd
+ * @mtd: MTD device structure
+ * @block: block offset from device start
+ * */
+int aw_rawnand_chip_block_markbad(struct mtd_info *mtd, int block)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	uint8_t *mdata = NULL;
+	uint8_t *spare = NULL;
+	int ret = 0;
+	int page = 0;
+	int bound = (chip->chips * chip->chipsize) >> chip->erase_shift;
+	int pagesize = chip->pagesize;
+	int sparesize = chip->avalid_sparesize;
+
+	if (unlikely(block > bound)) {
+		ret = -EINVAL;
+		awrawnand_err("block@%d is exceed boundary@%d\n", block, bound);
+		goto out;
+	}
+
+	page = block << chip->pages_per_blk_shift;
+
+	mdata = kzalloc(pagesize,  GFP_KERNEL);
+	if (!mdata) {
+		awrawnand_err("kzalloc mdata fail\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	spare = kzalloc(sparesize, GFP_KERNEL);
+	if (!spare) {
+		awrawnand_err("kzalloc spare fail\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	ret = chip->erase(mtd, page);
+	if (ret) {
+		awrawnand_err("erase block@%d fail\n", block);
+		ret = -EIO;
+		goto out;
+	}
+
+	if (chip->badblock_mark_pos == FIRST_PAGE)
+		ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+	else if (chip->badblock_mark_pos == LAST_PAGE) {
+		page += chip->pages_per_blk_mask;
+		ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+	} else if (chip->badblock_mark_pos == FIRST_TWO_PAGES) {
+		ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		page++;
+		ret |= chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+	} else if (chip->badblock_mark_pos == LAST_TWO_PAGES) {
+		page += chip->pages_per_blk_mask;
+		ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		page--;
+		ret |= chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+	} else {
+		awrawnand_err("Unknow the block mark use default mark pos(first page)\n");
+		ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+	}
+
+	aw_rawnand_chip_update_bbt(mtd, block, BBT_B_BAD);
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_block_markbad);
+
+/**
+ * aw_rawnand_chip_simu_block_markbad - mark a simu block bad in mark pos and update bbt&bbtd
+ * @mtd: MTD device structure
+ * @block: simu block offset from device start simu block
+ * */
+int aw_rawnand_chip_simu_block_markbad(struct mtd_info *mtd, int block)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+	uint8_t *mdata = NULL;
+	uint8_t *spare = NULL;
+	int ret = 0;
+	int page = 0;
+	int bound = (chip->chips * chip->chipsize) >> chip->simu_erase_shift;
+	int pagesize = chip->pagesize;
+	int sparesize = chip->avalid_sparesize;
+	int real_blkA = 0;
+	int real_blkB = 0;
+	int real_blk = 0;
+	int plane = 0;
+
+	if (unlikely(block > bound)) {
+		ret = -EINVAL;
+		awrawnand_err("block@%d is exceed boundary@%d\n", block, bound);
+		goto out;
+	}
+
+	real_blkA = (block << 1);
+	real_blkB = real_blkA + 1;
+	real_blk = real_blkA;
+
+	/*mark planeA & planeB*/
+
+	mdata = kzalloc(pagesize,  GFP_KERNEL);
+	if (!mdata) {
+		awrawnand_err("kzalloc mdata fail\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	spare = kzalloc(sparesize, GFP_KERNEL);
+	if (!spare) {
+		awrawnand_err("kzalloc spare fail\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	for (plane = 0; plane < 2; plane++) {
+		page = real_blk << chip->simu_pages_per_blk_shift;
+
+		ret = chip->erase(mtd, page);
+		if (ret) {
+			awrawnand_err("erase block@%d fail\n", block);
+			ret = -EIO;
+			goto out;
+		}
+
+		if (chip->badblock_mark_pos == FIRST_PAGE)
+			ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		else if (chip->badblock_mark_pos == LAST_PAGE) {
+			page += chip->pages_per_blk_mask;
+			ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		} else if (chip->badblock_mark_pos == FIRST_TWO_PAGES) {
+			ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+			page++;
+			ret |= chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		} else if (chip->badblock_mark_pos == LAST_TWO_PAGES) {
+			page += chip->pages_per_blk_mask;
+			ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+			page--;
+			ret |= chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		} else {
+			awrawnand_err("Unknow the block mark use default mark pos(first page)\n");
+			ret = chip->write_page(mtd, chip, mdata, pagesize, spare, sparesize, page);
+		}
+		real_blk = real_blkB;
+
+	}
+
+	aw_rawnand_chip_update_bbt(mtd, block, BBT_B_BAD);
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_simu_block_markbad);
+
+int aw_rawnand_chip_scan_bbt(struct mtd_info *mtd)
+{
+	struct aw_nand_chip *chip = awnand_mtd_to_chip(mtd);
+
+#if SIMULATE_MULTIPLANE
+	int total_blocks = chip->chipsize >> chip->erase_shift;
+#else
+	int total_blocks = chip->simu_chipsize >> chip->simu_erase_shift;
+#endif
+
+	int b = 0;
+	int c = 0;
+	int pos_byte = 0;
+	int pos_bit = 0;
+
+	for (c = 0; c < chip->chips; c++) {
+		chip->select_chip(mtd, c);
+		for (b = 0; b < total_blocks; b++) {
+			pos_byte = b >> 3;
+			pos_bit = b & 0x7;
+
+#if SIMULATE_MULTIPLANE
+			if (chip->block_bad(mtd, b)) {
+				chip->block_markbad(mtd, b);
+#else
+			if (chip->simu_block_bad(mtd, b)) {
+				chip->simu_block_markbad(mtd, b);
+#endif
+			} else {
+				chip->bbt[pos_byte] &= ~(1 << pos_bit);
+			}
+			chip->bbtd[pos_byte] |= (1 << pos_bit);
+		}
+		chip->select_chip(mtd, -1);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aw_rawnand_chip_scan_bbt);
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_ids.c b/drivers/mtd/awnand/rawnand/aw_rawnand_ids.c
new file mode 100644
index 000000000..fe88ca074
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_ids.c
@@ -0,0 +1,37 @@
+/**
+ * SPDX-License-Identifier: GPL-2.0+
+ * aw_rawnand_ids.c
+ *
+ * (C) Copyright 2020 - 2021
+ * Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+#include <linux/mtd/aw-rawnand.h>
+#include <linux/sizes.h>
+
+
+
+
+struct aw_nand_flash_dev giga[] = {
+	{
+		.name = "GD9FU2G8F2A",
+		.id = {0xc8, 0xda, 0x90, 0x95, 0x46},
+		.id_len = 5,
+		.dies_per_chip = 1,
+		.pagesize = SZ_2K,
+		.sparesize = 128,
+		.pages_per_blk = 64,
+		.blks_per_die = 2048,
+		.access_freq = 40,
+		.badblock_flag_pos = FIRST_PAGE,
+		.pe_cycles = PE_CYCLES_100K,
+		.options = RAWNAND_ITF_SDR | RAWNAND_NFC_RANDOM | RAWNAND_MULTI_WRITE | RAWNAND_MULTI_ERASE,
+	},
+};
+
+struct rawnand_manufacture aw_manuf_tbl[] = {
+	RAWNAND_MANUFACTURE(RAWNAND_MFR_GIGA, GIGA_NAME, giga),
+};
+
+AW_NAND_MANUFACTURE(aw_nand_manufs, aw_manuf_tbl);
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_nfc.c b/drivers/mtd/awnand/rawnand/aw_rawnand_nfc.c
new file mode 100644
index 000000000..3b86d2e56
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_nfc.c
@@ -0,0 +1,1544 @@
+/**
+ * SPDX-License-Identifier: GPL-2.0+
+ * aw_rawnand_nfc.c
+ *
+ * (C) Copyright 2020 - 2021
+ * Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+#include <linux/clk.h>
+#include <linux/of.h>
+#include <linux/errno.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/aw-rawnand.h>
+#include <linux/kernel.h>
+#include <asm/io.h>
+#include "aw_rawnand_nfc.h"
+#include <linux/dma-mapping.h>
+#include <linux/regulator/consumer.h>
+#include <linux/of_address.h>
+
+struct aw_nand_host aw_host;
+
+#define MBUS_GATE           0x0804
+#define NAND0_CFG           0x0810
+#define NAND1_CFG           0x0814
+#define NAND_GATE           0x082C
+
+#define CCM_NAND_CTRL_M		(0xF << 0)
+#define CCM_NAND_CTRL_CM(x)	((x) - 1)
+#define CCM_NAND_CTRL_N		(0x3 << 8)
+#define CCM_NAND_CTRL_CN(x)	((x) << 8)
+#define CCM_NAND_CTRL_ENABLE	(0x1 << 31)
+#define CCM_NAND_SRC_SELECT	(0x7 << 24)
+#define CCM_NAND_SRC_CSELECT(x)	(((x)& 0x7) << 24)
+
+static struct aw_nand_host *get_host(void)
+{
+	return &aw_host;
+}
+
+void aw_nfc_reg_dump(struct nfc_reg *reg)
+{
+	uint32_t i = 0;
+	for (i = 0; i < 12; i++) {
+		printk("[%p : %08x]\n", reg->ctl + i, readl(reg->ctl + i));
+	}
+
+	for (i = 0; i < 7; i++) {
+		printk("[%p : %08x]\n", reg->ecc_ctl + i, readl(reg->ecc_ctl + i));
+	}
+
+	for (i = 0; i < 8; i++) {
+		printk("[%p : %08x]\n", reg->err_cnt[0] + i, readl(reg->err_cnt[0] + i));
+	}
+
+	for (i = 0; i < 4; i++) {
+		printk("[%p : %08x]\n", reg->user_data_len_base + i,
+				readl(reg->user_data_len_base + i));
+	}
+
+	for (i = 0; i < 16; i++) {
+		printk("[%p : %08x]\n", reg->user_data_base + i,
+				readl(reg->user_data_base + i));
+	}
+
+	printk("[%p : %08x]\n", reg->spare_area, readl(reg->spare_area));
+	printk("[%p : %08x]\n", reg->pat_id, readl(reg->pat_id));
+#if 0
+	volatile unsigned *mdclk = (volatile unsigned *)(0x03001810);
+	volatile unsigned *mcclk = (volatile unsigned *)(0x03001814);
+	volatile unsigned *mbus_gate = (volatile unsigned *)(0x03001804);
+	volatile unsigned *per0 = (volatile unsigned *)(0x03001020);
+	volatile unsigned *mbus  = (volatile unsigned *)(0x03001540);
+	printk("[%p : %08x]\n", mbus_gate, readl(mbus_gate));
+	printk("[%p : %08x]\n", per0, readl(per0));
+	printk("[%p : %08x]\n", mbus, readl(mbus));
+	printk("[%p : %08x]\n", mdclk, readl(mdclk));
+	printk("[%p : %08x]\n", mcclk, readl(mcclk));
+#endif
+}
+
+
+static void aw_nfc_reg_prepare(struct nfc_reg *reg)
+{
+	struct aw_nand_host *host = container_of(reg, struct aw_nand_host, nfc_reg);
+	int i = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s reg@%p\n", __func__, reg);
+
+	AWRAWNAND_TRACE_NFC("host:%p\n", host, host);
+	AWRAWNAND_TRACE_NFC("nfc:%p\n", &host->nfc_reg);
+
+	reg->ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x0000);
+	reg->sta = (volatile uint32_t *)((uint8_t *)host->base + 0x0004);
+	reg->int_ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x0008);
+	reg->timing_ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x000c);
+	reg->timing_cfg = (volatile uint32_t *)((uint8_t *)host->base + 0x0010);
+	reg->addr_low = (volatile uint32_t *)((uint8_t *)host->base + 0x0014);
+	reg->addr_high = (volatile uint32_t *)((uint8_t *)host->base + 0x0018);
+	reg->data_block_mask = (volatile uint32_t *)((uint8_t *)host->base + 0x001c);
+	reg->cnt = (volatile uint32_t *)((uint8_t *)host->base + 0x0020);
+	reg->cmd = (volatile uint32_t *)((uint8_t *)host->base + 0x0024);
+	reg->read_cmd_set = (volatile uint32_t *)((uint8_t *)host->base + 0x0028);
+	reg->write_cmd_set = (volatile uint32_t *)((uint8_t *)host->base + 0x002c);
+	reg->ecc_ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x0034);
+	reg->ecc_sta = (volatile uint32_t *)((uint8_t *)host->base + 0x0038);
+	reg->data_pattern_sta = (volatile uint32_t *)((uint8_t *)host->base + 0x003c);
+	reg->efr = (volatile uint32_t *)((uint8_t *)host->base + 0x0040);
+	reg->rdata_sta_ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x0044);
+	reg->rdata_sta_0 = (volatile uint32_t *)((uint8_t *)host->base + 0x0048);
+	reg->rdata_sta_1 = (volatile uint32_t *)((uint8_t *)host->base + 0x004c);
+	for (i = 0; i < MAX_ERR_CNT; i++)
+		reg->err_cnt[i] = (volatile uint32_t *)((uint8_t *)host->base + 0x0050+(i * 4));
+	reg->user_data_len_base = (volatile uint32_t *)((uint8_t *)host->base + 0x0070);
+	reg->user_data_base = (volatile uint32_t *)((uint8_t *)host->base + 0x0080);
+	reg->efnand_sta = (volatile uint32_t *)((uint8_t *)host->base + 0x0110);
+	reg->spare_area = (volatile uint32_t *)((uint8_t *)host->base + 0x0114);
+	reg->pat_id = (volatile uint32_t *)((uint8_t *)host->base + 0x0118);
+	reg->ddr2_spec_ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x011c);
+	reg->ndma_mode_ctl = (volatile uint32_t *)((uint8_t *)host->base + 0x0120);
+	reg->mbus_dma_dlba = (volatile uint32_t *)((uint8_t *)host->base + 0x0200);
+	reg->mbus_dma_sta = (volatile uint32_t *)((uint8_t *)host->base + 0x0204);
+	reg->mdma_int_mask =  (volatile uint32_t *)((uint8_t *)host->base + 0x0208);
+	reg->mdma_cur_desc_addr = (volatile uint32_t *)((uint8_t *)host->base + 0x020c);
+	reg->mdma_cur_buf_addr = (volatile uint32_t *)((uint8_t *)host->base + 0x0210);
+	reg->dma_cnt = (volatile uint32_t *)((uint8_t *)host->base + 0x0214);
+	reg->ver = (volatile uint32_t *)((uint8_t *)host->base + 0x02f0);
+	reg->ram0_base = (volatile uint32_t *)((uint8_t *)host->base + 0x0400);
+	reg->ram1_base = (volatile uint32_t *)((uint8_t *)host->base + 0x0800);
+
+}
+
+
+static int aw_host_set_pin(struct aw_nand_host *host)
+{
+	struct pinctrl *pinctrl = NULL;
+
+	pinctrl = pinctrl_get_select(host->dev, "default");
+	if (IS_ERR_OR_NULL(pinctrl)) {
+		awrawnand_err("set nand0 pin fail\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static inline void noraml_req_get_addr(struct aw_nfc_normal_req *req, uint32_t *addr_low,
+		uint32_t *addr_high)
+{
+	int i = 0;
+
+	for (i = 0; i < req->op.cmd_with_addr.addr_cycles; i++) {
+		if (i < 4)
+			(*addr_low) |= (req->op.cmd_with_addr.addr[i] << (i * 8));
+		else
+			(*addr_high) |= (req->op.cmd_with_addr.addr[i] << ((i - 4) * 8));
+	}
+}
+
+static int aw_host_nfc_wait_cmd_fifo_empty(struct nfc_reg *nfc)
+{
+	int ret = -ETIMEDOUT;
+
+	unsigned long timeout = jiffies + msecs_to_jiffies(60000);
+
+	do {
+		if (!(readl(nfc->sta) & NFC_CMD_FIFO_STATUS)) {
+			ret = 0;
+			goto out;
+		} else
+			cond_resched();
+
+	} while (time_before(jiffies, timeout));
+
+	if (ret) {
+		awrawnand_err("wait cmd fifo empty timeout 1s status@%p %x\n",
+				nfc->sta, readl(nfc->sta));
+		aw_nfc_reg_dump(nfc);
+	}
+out:
+	return ret;
+}
+
+static int aw_host_nfc_wait_fsm_idle(struct nfc_reg *nfc)
+{
+	int ret = -ETIMEDOUT;
+
+	unsigned long timeout = jiffies +msecs_to_jiffies(60000);
+
+	do {
+		if (!(readl(nfc->sta) & NFC_STA)) {
+			ret = 0;
+			goto out;
+		} else
+			cond_resched();
+
+	} while (time_before(jiffies, timeout));
+
+	if (ret) {
+		awrawnand_err("wait nfc fsm idle timeout 1s status@%p %x\n",
+				nfc->sta, readl(nfc->sta));
+		aw_nfc_reg_dump(nfc);
+	}
+out:
+	return ret;
+}
+
+static int aw_host_nfc_wait_cmd_finish(struct nfc_reg *nfc)
+{
+	int ret = -ETIMEDOUT;
+	/*AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);*/
+
+	unsigned long timeout = jiffies + msecs_to_jiffies(60000);
+
+	do {
+		if ((readl(nfc->sta) & NFC_CMD_INT_FLAG)) {
+			ret = 0;
+			goto out;
+		} else
+			cond_resched();
+
+	} while (time_before(jiffies, timeout));
+
+	if (ret)
+		awrawnand_err("wait nfc wait cmd finish 10s timeout[%p:%x]\n",
+				nfc->sta, readl(nfc->sta));
+out:
+	/*write 1 to clear CMD INT FLAG*/
+	writel(NFC_CMD_INT_FLAG, nfc->sta);
+	/*AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);*/
+	return ret;
+}
+#if 0
+static int aw_host_nfc_wait_B2R_int(struct nfc_reg *nfc)
+{
+	int ret = -ETIMEDOUT;
+	uint32_t cfg = 0;
+
+	unsigned long timeout = jiffies + msecs_to_jiffies(60000);
+
+	do {
+		if ((readl(nfc->sta) & NFC_RB_B2R)) {
+			ret = 0;
+			goto out;
+		} else
+			cond_resched();
+
+	} while (time_before(jiffies, timeout));
+
+	if (ret)
+		awrawnand_err("wait nfc wait B2R 2s timeout [%x: %x]\n",
+				nfc->sta, readl(nfc->sta));
+out:
+	/*write 1 to clear CMD B2R FLAG*/
+	cfg = readl(nfc->sta);
+	cfg |= NFC_RB_B2R;
+	writel(cfg, nfc->sta);
+
+	return ret;
+}
+#endif
+
+static bool aw_host_nfc_wait_rb_ready(struct aw_nand_chip *chip, struct aw_nand_host *host)
+{
+	int ret = 0;
+	uint32_t val = 0;
+	int chip_no = chip->selected_chip.chip_no;
+	unsigned long timeout = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+
+	timeout = jiffies + msecs_to_jiffies(60000);
+
+	do {
+		val = readl(host->nfc_reg.sta);
+		if (chip->selected_chip.chip_no != -1) {
+			ret = ((val & NFC_RB_STATE(chip->selected_chip.ceinfo[chip_no].relate_rb_no)));
+					/*&& (val & NFC_RB_B2R));*/
+			if (ret) {
+				break;
+			} else
+				cond_resched();
+		}
+	} while (time_before(jiffies, timeout));
+
+	AWRAWNAND_TRACE_NFC("Exit %s %s status[%p:%x]\n", __func__, ret ? "ready" : "busy",
+			host->nfc_reg.sta, readl(host->nfc_reg.sta));
+	return ret ? true : false;
+}
+
+static bool aw_host_nfc_rb_ready(struct aw_nand_chip *chip, struct aw_nand_host *host)
+{
+	int ret = 0;
+	uint32_t val = 0;
+	int chip_no = chip->selected_chip.chip_no;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+
+	AWRAWNAND_TRACE_NFC("selected chip_no:%d rb_no:%d\n", chip_no,
+			chip->selected_chip.ceinfo[chip_no].relate_rb_no);
+
+	val = readl(host->nfc_reg.sta);
+	if (chip->selected_chip.chip_no != -1) {
+		ret = ((val & NFC_RB_STATE(chip->selected_chip.ceinfo[chip_no].relate_rb_no))
+				&& (val & NFC_RB_B2R));
+	}
+	AWRAWNAND_TRACE_NFC("Exit %s %s status[%p:%x]\n", __func__, ret ? "ready" : "busy",
+			host->nfc_reg.sta, readl(host->nfc_reg.sta));
+	return ret ? true : false;
+}
+
+
+static int aw_host_wait_dma_ready_flag_timeout(struct aw_nand_host *host, uint32_t timeout_ms)
+{
+	int ret = -ETIMEDOUT;
+
+	unsigned long timeout = jiffies + msecs_to_jiffies(timeout_ms);
+
+	do {
+		if (host->dma_ready_flag) {
+			ret = 0;
+			goto out;
+		} else
+			cond_resched();
+
+	} while (time_before(jiffies, timeout));
+
+	if (ret) {
+		awrawnand_err("wait dma ready int tiemout status[%p:%x]\n",
+				&host->nfc_reg.sta, readl(host->nfc_reg.sta));
+	}
+out:
+	return ret;
+
+}
+static int aw_host_nfc_wait_status_timeout(struct nfc_reg *nfc, uint32_t mark, uint32_t value, uint32_t timeout_ms)
+{
+	int ret = -ETIMEDOUT;
+	unsigned long timeout = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+
+	timeout = jiffies + msecs_to_jiffies(timeout_ms);
+
+
+	do {
+		if ((readl(nfc->sta) & mark) == value) {
+			ret = 0;
+			goto out;
+		} else
+			cond_resched();
+
+	} while (time_before(jiffies, timeout));
+
+	if (ret)
+		awrawnand_err("wait nfc wait status 10s timeout[%x:%x:%p:%x]\n",
+				mark, value, nfc->sta, readl(nfc->sta));
+out:
+	AWRAWNAND_TRACE_NFC("Exit %s ret@%d sta[%x:%x]\n", __func__, ret, nfc->sta, readl(nfc->sta));
+	return ret;
+}
+static int aw_host_nfc_noraml_op_cmd(struct aw_nand_chip *chip, struct aw_nfc_normal_req *req)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	struct nfc_reg *nfc = &host->nfc_reg;
+	uint32_t cmd_cfg = 0;
+
+	int ret = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+
+	if (aw_host_nfc_wait_cmd_fifo_empty(nfc) || aw_host_nfc_wait_fsm_idle(nfc)) {
+		awrawnand_warn("cmd@%02x wait fifo or fsm timeout\n",
+				req->op.cmd.code);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	/*configure first command*/
+	cmd_cfg |= req->op.cmd.code;
+	cmd_cfg |= NFC_SEND_CMD1;
+
+
+	if (req->wait_rb) {
+		cmd_cfg |= NFC_WAIT_FLAG;
+	}
+
+	writel(cmd_cfg, nfc->cmd);
+
+	AWRAWNAND_TRACE_NFC("cmd_cfg@%x reg_cmd@%p %x\n", cmd_cfg, nfc->cmd, readl(nfc->cmd));
+
+	ret = aw_host_nfc_wait_cmd_finish(nfc);
+	if (ret) {
+		awrawnand_err("cmd@%d send fail\n", req->op.cmd.code);
+	}
+
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+out:
+	return ret;
+}
+
+static int aw_host_nfc_noraml_op_cmd_with_addr(struct aw_nand_chip *chip, struct aw_nfc_normal_req *req)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	struct nfc_reg *nfc = &host->nfc_reg;
+	uint32_t low = 0;
+	uint32_t high = 0;
+	uint32_t cmd_cfg = 0;
+
+	int ret = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+
+	if (aw_host_nfc_wait_cmd_fifo_empty(nfc) || aw_host_nfc_wait_fsm_idle(nfc)) {
+		awrawnand_warn("cmd@%02x wait fifo or fsm timeout\n",
+				req->op.cmd.code);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	/*configure first command*/
+	cmd_cfg |= req->op.cmd_with_addr.code;
+	cmd_cfg |= NFC_SEND_CMD1;
+
+
+	if (req->wait_rb) {
+		cmd_cfg |= NFC_WAIT_FLAG;
+	}
+
+	/*configure address*/
+	noraml_req_get_addr(req, &low, &high);
+	writel(low, nfc->addr_low);
+	if (req->op.cmd_with_addr.addr_cycles > 4)
+		writel(high, nfc->addr_high);
+	/*configure send's address number*/
+	cmd_cfg |= NFC_ADR_NUM(req->op.cmd_with_addr.addr_cycles);
+	cmd_cfg |= NFC_SEND_ADR;
+
+	writel(cmd_cfg, nfc->cmd);
+	AWRAWNAND_TRACE_NFC("cmd_cfg@%d cmd[%p:%x] addr_low:[%p:%x] addr_high:[%p:%x]\n", cmd_cfg, nfc->cmd, readl(nfc->cmd), nfc->addr_low, readl(nfc->addr_low), nfc->addr_high, readl(nfc->addr_high));
+
+	ret = aw_host_nfc_wait_cmd_finish(nfc);
+	if (ret) {
+		awrawnand_err("cmd@%d send fail\n", req->op.cmd.code);
+	}
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+out:
+	return ret;
+}
+
+
+static int aw_host_nfc_noraml_op_cmd_with_addr_data(struct aw_nand_chip *chip, struct aw_nfc_normal_req *req)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	struct nfc_reg *nfc = &host->nfc_reg;
+	uint32_t cmd_cfg = 0;
+	uint32_t ctl_cfg = 0;
+	uint32_t low = 0;
+	uint32_t high = 0;
+
+	int ret = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+	if (unlikely(req->op.cmd_with_addr_data.len > SZ_1K))
+		goto out_exceed_fail;
+
+	if (unlikely(req->op.cmd_with_addr_data.len == 0))
+		goto out_do_nothing;
+
+	if (unlikely(req->op.cmd_with_addr_data.in == NULL)
+			&& unlikely(req->op.cmd_with_addr_data.out == NULL))
+		goto out_invalid_paramter;
+
+	if (aw_host_nfc_wait_cmd_fifo_empty(nfc) || aw_host_nfc_wait_fsm_idle(nfc)) {
+		ret = -ETIMEDOUT;
+		goto out_fifo_fsm_fail;
+	}
+
+	/*configure first command*/
+	cmd_cfg |= req->op.cmd_with_addr_data.code;
+	cmd_cfg |= NFC_SEND_CMD1;
+
+
+	if (req->wait_rb) {
+		cmd_cfg |= NFC_WAIT_FLAG;
+	}
+
+	/*configure address*/
+	noraml_req_get_addr(req, &low, &high);
+	low = (req->op.cmd_with_addr_data.addr[0] | (req->op.cmd_with_addr_data.addr[1] << 8)
+			| (req->op.cmd_with_addr_data.addr[2] << 16) |
+			(req->op.cmd_with_addr_data.addr[3] << 24));
+	writel(low, nfc->addr_low);
+	if (req->op.cmd_with_addr.addr_cycles > 4) {
+		high = req->op.cmd_with_addr_data.addr[5];
+		writel(high, nfc->addr_high);
+	}
+
+	/*configure send's address number*/
+	if (req->op.cmd_with_addr_data.addr_cycles) {
+		cmd_cfg |= NFC_ADR_NUM(req->op.cmd_with_addr_data.addr_cycles);
+		cmd_cfg |= NFC_SEND_ADR;
+	}
+
+
+	writel((req->op.cmd_with_addr_data.len & 0x3ff), nfc->cnt);
+
+	ctl_cfg = readl(nfc->ctl);
+	ctl_cfg &= ~NFC_RAM_METHOD_DMA;
+	writel(ctl_cfg, nfc->ctl);
+
+
+	if (req->op.cmd_with_addr_data.direct == FLASH_WRITE) {
+		cmd_cfg |= NFC_ACCESS_DIR;
+		memcpy_toio(nfc->ram0_base, req->op.cmd_with_addr_data.out,
+				req->op.cmd_with_addr_data.len);
+	}
+
+
+	cmd_cfg |= NFC_DATA_TRANS;
+	writel(cmd_cfg, nfc->cmd);
+
+	AWRAWNAND_TRACE_NFC("cmd_cfg@%x cmd[%p:%x] ctl[%p:%x] cnt[%p:%x]\n",
+			cmd_cfg, nfc->cmd, readl(nfc->cmd),
+			nfc->ctl, readl(nfc->ctl), nfc->cnt, readl(nfc->cnt));
+
+	ret = aw_host_nfc_wait_cmd_finish(nfc);
+	if (ret)
+		goto out_read_data_fail;
+
+
+	if ((req->op.cmd_with_addr_data.direct == FLASH_READ) && req->op.cmd_with_addr_data.in) {
+		/*AWRAWNAND_TRACE_NFC("ram:%x %x\n", readl(nfc->ram0_base), readl(nfc->ram0_base + 1));*/
+		memcpy_fromio(req->op.cmd_with_addr_data.in, nfc->ram0_base,
+				req->op.cmd_with_addr_data.len);
+	}
+
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return ret;
+
+out_exceed_fail:
+	awrawnand_err("req len@%d exceed @%d\n", req->op.cmd_with_addr_data.len, SZ_1K);
+	return -EINVAL;
+out_do_nothing:
+	awrawnand_err("req len@0 , do nothing\n");
+	return 0;
+out_invalid_paramter:
+	awrawnand_err("invalid parameter\n");
+	return -EINVAL;
+out_read_data_fail:
+	awrawnand_err("read data fail\n");
+	return ret;
+out_fifo_fsm_fail:
+	awrawnand_err("fifo or fsm is not empty\n");
+	return ret;
+}
+
+static int aw_host_nfc_normal_op(struct aw_nand_chip *chip, struct aw_nfc_normal_req *req)
+{
+	int ret = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s req type@%d\n", __func__, req->type);
+
+	switch (req->type) {
+	case CMD:
+		ret = aw_host_nfc_noraml_op_cmd(chip, req);
+		break;
+	case CMD_WITH_ADDR:
+		ret = aw_host_nfc_noraml_op_cmd_with_addr(chip, req);
+		break;
+	case CMD_WITH_ADDR_DATA:
+		ret = aw_host_nfc_noraml_op_cmd_with_addr_data(chip, req);
+		break;
+	default:
+		awrawnand_err("don't support normal req type@%d\n", req->type);
+		break;
+	}
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+
+	return ret;
+}
+
+
+static int aw_host_nfc_dma_config_start(struct aw_nand_host *host, uint8_t rw, void *addr, unsigned int len)
+{
+
+	struct nfc_reg *nfc = &host->nfc_reg;
+	uint32_t cfg = 0;
+	enum dma_data_direction dir = rw ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL;
+	int ret = 0;
+
+	/*aw_host_flush_dcache(addr, len);*/
+	AWRAWNAND_TRACE_NFC("Enter %s rw@%d addr@%p len@%u\n", __func__, rw, addr, len);
+
+	if (host->dma_type == MBUS_DMA) {
+		host->dma_addr = dma_map_single(host->dev, addr, len, dir);
+		if (dma_mapping_error(host->dev, host->dma_addr)) {
+			awrawnand_err("dma mapping err\n");
+			ret = -EINVAL;
+			goto out;
+		}
+		/*config use mbus dma*/
+		cfg = readl(nfc->ctl);
+		/*use dma*/
+		cfg &= ~NFC_DMA_TYPE;
+		/*use mbus dma*/
+		cfg |= NFC_RAM_METHOD_DMA;
+		writel(cfg, nfc->ctl);
+
+		host->nfc_dma_desc_cpu[0].bcnt = 0;
+		host->nfc_dma_desc_cpu[0].bcnt |= NFC_DESC_BSIZE(len);
+		host->nfc_dma_desc_cpu[0].buff = (unsigned int)host->dma_addr;
+
+		host->nfc_dma_desc_cpu[0].cfg = 0;
+		host->nfc_dma_desc_cpu[0].cfg |= NFC_DESC_FIRST_FLAG;
+		host->nfc_dma_desc_cpu[0].cfg |= NFC_DESC_LAST_FLAG;
+		host->nfc_dma_desc_cpu[0].next = &host->nfc_dma_desc_cpu[0];
+
+		host->desc_addr = dma_map_single(host->dev, (void *)&host->nfc_dma_desc_cpu[0], 1024, DMA_TO_DEVICE);
+		if (dma_mapping_error(host->dev, host->desc_addr)) {
+			awrawnand_err("dma mapping err\n");
+			ret = -EINVAL;
+			goto out;
+		}
+
+		if (host->use_dma_int)
+			aw_host_nfc_dma_int_enable(&host->nfc_reg);
+
+		writel((uint32_t)host->desc_addr, nfc->mbus_dma_dlba);
+
+	} else {
+		; /*to do*/
+	}
+
+out:
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return 0;
+
+}
+
+static void aw_host_rb_wake_up(void)
+{
+	return;
+}
+
+static void aw_host_dma_wake_up(void)
+{
+	return;
+}
+
+void aw_host_nfc_do_nand_interrupt(void)
+{
+	struct aw_nand_host *host = &aw_host;
+	if (aw_host_nfc_rb_b2r_int_occur_check(&host->nfc_reg)) {
+		aw_host_nfc_rb_b2r_intstatus_clear(&host->nfc_reg);
+		aw_host_nfc_rb_b2r_int_disable(&host->nfc_reg);
+		host->rb_ready_flag = 1;
+		aw_host_rb_wake_up();
+	}
+
+	if (host->use_dma_int) {
+		if (aw_host_nfc_dma_int_occur_check(&host->nfc_reg)) {
+			aw_host_nfc_dma_intstatus_clear(&host->nfc_reg);
+			aw_host_nfc_dma_int_disable(&host->nfc_reg);
+			host->dma_ready_flag = 1;
+			aw_host_dma_wake_up();
+		}
+	}
+}
+
+
+static int aw_host_nfc_dma_wait_end(struct aw_nand_host *host, uint8_t rw, void *addr, unsigned int len)
+{
+	int ret = 0;
+	enum dma_data_direction dir = rw ? DMA_TO_DEVICE : DMA_FROM_DEVICE;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+	if (host->use_dma_int) {
+		if (host->dma_ready_flag == 1)
+			goto dma_int_end;
+		if (!aw_host_wait_dma_ready_flag_timeout(host, 10000))
+			goto dma_int_end;
+
+	}
+
+	ret = aw_host_nfc_wait_status_timeout(&host->nfc_reg,
+			NFC_DMA_INT_FLAG, NFC_DMA_INT_FLAG, 60000);
+	if (ret)
+		aw_nfc_reg_dump(&host->nfc_reg);
+
+	if (host->dma_ready_flag == 1)
+		host->dma_ready_flag = 0;
+	else {
+		aw_host_nfc_dma_intstatus_clear(&host->nfc_reg);
+		aw_host_nfc_dma_int_disable(&host->nfc_reg);
+	}
+
+dma_int_end:
+
+	dma_unmap_single(host->dev, host->dma_addr, len, dir);
+	dma_unmap_single(host->dev, host->desc_addr, 1024, DMA_TO_DEVICE);
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return ret;
+}
+
+/*len is align to ecc block size(1KB)*/
+static int aw_host_nfc_check_ecc_status(struct nfc_reg *nfc, uint32_t len)
+{
+
+	struct aw_nand_host *host = awnand_nfc_to_host(nfc);
+
+	uint8_t ecc_limit = ecc_limit_tab[NFC_ECC_GET(readl(nfc->ecc_ctl))];
+	uint32_t ecc_block_cnt = B_TO_KB(len);
+	uint32_t ecc_block_mask = ((1 << ecc_block_cnt) - 1);
+	uint32_t ecc_cnt_w[MAX_ERR_CNT];
+	uint8_t ecc_cnt = 0;
+	int i = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s len@%d\n", __func__, len);
+
+	if (readl(nfc->ecc_sta) & ecc_block_mask) {
+		awrawnand_err("status[%p:%x]\n", nfc->ecc_sta, readl(nfc->ecc_sta));
+		aw_nfc_reg_dump(nfc);
+		return ECC_ERR;
+	}
+
+	/*check ecc limit*/
+	for (i = 0; i < MAX_ERR_CNT; i++) {
+		ecc_cnt_w[i] = readl(nfc->err_cnt[i]);
+	}
+
+	for (i = 0; i < ecc_block_cnt; i++) {
+		ecc_cnt = (uint8_t)(ecc_cnt_w[i >> 2] >> ((i % 4) << 3));
+		if (ecc_cnt > ecc_limit) {
+			AWRAWNAND_TRACE_NFC("Exit %s ret@ECC_LIMIT\n", __func__);
+			host->bitflips = ecc_cnt;
+			awrawnand_info("ecc limit@%d\n", ecc_cnt);
+			return ECC_LIMIT;
+		}
+	}
+
+	host->bitflips = ecc_cnt;
+	AWRAWNAND_TRACE_NFC("Exit %s ret@ECC_GOOG\n", __func__);
+	return ECC_GOOD;
+}
+
+static bool aw_host_nfc_is_blank_page(struct nfc_reg *nfc, uint32_t len)
+{
+	uint32_t ecc_block_cnt = B_TO_KB(len);
+	uint32_t ecc_block_mask = ((1 << ecc_block_cnt) - 1);
+
+	if ((readl(nfc->ecc_ctl) & NFC_ECC_EXCEPTION) &&
+			(!(readl(nfc->ecc_sta) & ecc_block_mask)) &&
+			((readl(nfc->pat_id) & ecc_block_mask) == ecc_block_mask))
+		return true;
+	return false;
+}
+
+static void aw_host_nfc_get_spare_data(struct nfc_reg *nfc, uint8_t *spare, uint8_t len)
+{
+	uint8_t cnt = (len >> 2);
+	uint8_t i = 0;
+	uint32_t val = 0;
+
+	if (likely(spare)) {
+		for (i = 0; i < cnt; i++) {
+			val = readl(nfc->user_data_base + i);
+			spare[i * 4 + 0] = ((val >> 0) & 0xff);
+			spare[i * 4 + 1] = ((val >> 8) & 0xff);
+			spare[i * 4 + 2] = ((val >> 16) & 0xff);
+			spare[i * 4 + 3] = ((val >> 24) & 0xff);
+		}
+	}
+}
+
+static int aw_host_nfc_get_dummy_byte(int real_page_size, int ecc_mode, int ecc_block_cnt, int user_data_len)
+{
+	int ecc_code_size = 0;
+	int valid_size = 0;
+	int dummy_byte = 0;
+	uint8_t ecc_bits_per_ecc_block = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s [%d:%d:%d:%d]\n", __func__, real_page_size, ecc_mode, ecc_block_cnt, user_data_len);
+
+	ecc_bits_per_ecc_block = ecc_bits_tbl[ecc_mode];
+	ecc_code_size = ((14 * ecc_bits_per_ecc_block / 8) * ecc_block_cnt);
+	/*ecc block size : 1024Byte*/
+	valid_size = ecc_code_size + user_data_len + (ecc_block_cnt << 10);
+	dummy_byte = real_page_size - valid_size;
+	AWRAWNAND_TRACE_NFC("Exit %s dummy_byte@%d\n", __func__, dummy_byte);
+
+	return dummy_byte;
+}
+
+#ifdef DEBUG
+static void dump_batch_req(struct aw_nfc_batch_req *req)
+{
+	printk("req type@%d\n", req->type);
+	printk("req layout@%d\n", req->layout);
+	printk("req plane@%d\n", req->plane);
+	printk("req first cmd@%x\n", req->cmd.val.first);
+	printk("req snd cmd@%x\n", req->cmd.val.snd);
+	printk("req rnd1 cmd@%x\n", req->cmd.val.rnd1);
+	printk("req rnd2 cmd@%x\n", req->cmd.val.rnd2);
+	printk("req addr page@%u\n", req->addr.page);
+	printk("req addr cycles@%d\n", req->addr.row_cycles);
+	printk("req data type@%d\n", req->data.type);
+	printk("req data main len@%d\n", req->data.main_len);
+	printk("req data spare len@%d\n", req->data.spare_len);
+	printk("req data main@%p\n", req->data.main);
+	printk("req data spare@%p\n", req->data.spare);
+}
+#endif
+
+
+static int aw_host_nfc_batch_op(struct aw_nand_chip *chip, struct aw_nfc_batch_req *req)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	struct nfc_reg *nfc = &host->nfc_reg;
+	/*int rb_no = aw_host_nfc_get_selected_rb_no(nfc);*/
+	uint32_t page = req->addr.page;
+	uint32_t page_in_block = page & chip->pages_per_blk_mask;
+
+	int ret = 0;
+	uint32_t val = 0;
+	uint8_t col1 = 0, col2 = 0, row1 = 0, row2 = 0, row3 = 0;
+	uint32_t low = 0, high = 0;
+	uint32_t read_set0 = 0;
+	uint32_t cmd = 0;
+	uint32_t ecc_block = 0, ecc_block_bitmap = 0;
+	uint8_t spare[MAX_SPARE_SIZE];
+	int dummy_byte = 0;
+	uint8_t default_spare[chip->avalid_sparesize];
+	int real_pagesize = chip->real_pagesize;
+	int ecc_mode = chip->ecc_mode;
+	int w_ecc_block_cnt = B_TO_KB(chip->pagesize);
+	int w_user_data_len = chip->avalid_sparesize;
+
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+	/*dump_batch_req(req);*/
+
+	if (req->data.type == ONLY_SPARE) {
+		/*data layout interleave mode, ecc and oob store in spare area
+		 * need to read main data and spare data to do ecc,though user only read spare*/
+		ecc_block = B_TO_KB(chip->pagesize);
+		ecc_block_bitmap = ((1 << ecc_block) - 1);
+		/*one ecc_block can attach to 32 bytes*/
+		if (req->data.spare_len < 32)
+			ecc_block_bitmap = 0x1;
+		/*to do : ecc_block_bitmap from req.data.spare_len*/
+	} else {
+		ecc_block = B_TO_KB(req->data.main_len);
+		ecc_block_bitmap = ((1 << ecc_block) - 1);
+	}
+
+	val = readl(nfc->sta);
+	val &= (NFC_RB_B2R | NFC_CMD_INT_FLAG | NFC_DMA_INT_FLAG);
+	val |= readl(nfc->sta);
+	writel(val, nfc->sta);
+
+	/*set ecc mode, randomizer,exception,pipeline*/
+	if (!chip->operate_boot0) {
+		aw_host_nfc_set_ecc_mode(nfc, chip->ecc_mode);
+		aw_host_nfc_ecc_enable(nfc, 1);
+		if (chip->random) {
+			aw_host_nfc_randomize_enable(nfc, page_in_block);
+		}
+	} else {
+		aw_host_nfc_set_ecc_mode(nfc, chip->boot0_ecc_mode);
+		aw_host_nfc_ecc_enable(nfc, 1);
+		aw_host_nfc_randomize_enable(nfc, page);
+	}
+
+	/*configure addr*/
+	row1 = (page & 0xff);
+	row2 = ((page >> 8) & 0xff);
+	row3 = ((page >> 16) & 0xff);
+
+	low = (col1 | (col2 << 8) | (row1 << 16) | (row2 << 24));
+	high |= row3;
+
+	writel(low, nfc->addr_low);
+	writel(high, nfc->addr_high);
+
+	cmd |= NFC_ADR_NUM((req->addr.row_cycles + 2));
+	cmd |= NFC_SEND_ADR;
+
+
+	cmd |= NFC_SEND_CMD2;
+	cmd |= NFC_SEND_CMD1;
+	cmd |= NFC_DATA_TRANS;
+
+	/*default:batch mode use dma*/
+	if (req->data.type != ONLY_SPARE)
+		cmd |= NFC_DATA_SWAP_METHOD;
+
+	if (req->layout == SEQUENCE) {
+		cmd |= NFC_SEQ;
+	}
+
+	cmd |= NFC_BATCH_OP;
+	/*configure read command set*/
+	if (req->type == FLASH_READ) {
+		/*cmd |= req->cmd.r.READ0;*/
+		cmd |= req->cmd.val.first;
+		cmd |= NFC_WAIT_FLAG;
+
+		/*
+		 *read_set0 = ((req->cmd.r.READSTART << 0) | (req->cmd.r.RNOUT << 8) |
+		 *                (req->cmd.r.RNOUTSTART << 16));
+		 */
+
+		read_set0 = ((req->cmd.val.snd << 0) | (req->cmd.val.rnd1 << 8) |
+				(req->cmd.val.rnd2 << 16));
+		writel(read_set0, nfc->read_cmd_set);
+
+		/*configure user data len*/
+		aw_host_nfc_set_user_data_len(nfc, chip->avalid_sparesize);
+		memset(default_spare, 0x99, chip->avalid_sparesize);
+		aw_host_nfc_set_user_data(nfc, default_spare, chip->avalid_sparesize);
+
+	} else {
+
+		/*configure user data len*/
+		if (req->data.spare_len) {
+			memset(spare, 0xff, MAX_SPARE_SIZE);
+			/*data.spare_len should be less than MAX_SPARE_SIZE or equal*/
+			memcpy(spare, req->data.spare, req->data.spare_len);
+			/*avalid_sparesize maximum equal to MAX_SPARE_SIZE*/
+			aw_host_nfc_set_user_data(nfc, spare, chip->avalid_sparesize);
+		} else {
+			aw_host_nfc_set_user_data(nfc, host->spare_default, chip->avalid_sparesize);
+		}
+
+		if (!chip->operate_boot0)
+			aw_host_nfc_set_user_data_len(nfc, chip->avalid_sparesize);
+		else
+			aw_host_nfc_set_boot0_user_data_len(nfc, req->data.spare_len);
+
+		cmd |= NFC_ACCESS_DIR;
+		/*cmd |= req->cmd.w.SEQIN;*/
+		cmd |= req->cmd.val.first;
+
+		/*writel((req->cmd.w.PAGEPROG | (req->cmd.w.RNDIN << 8)), nfc->write_cmd_set);*/
+		writel((req->cmd.val.snd | (req->cmd.val.rnd1 << 8)), nfc->write_cmd_set);
+
+		dummy_byte = aw_host_nfc_get_dummy_byte(real_pagesize, ecc_mode, w_ecc_block_cnt, w_user_data_len);
+		if (dummy_byte > 0)
+			aw_host_nfc_set_dummy_byte(nfc, dummy_byte);
+
+	}
+
+	if (aw_host_nfc_wait_cmd_fifo_empty(nfc) || aw_host_nfc_wait_fsm_idle(nfc)) {
+		ret = -ETIMEDOUT;
+		awrawnand_err("fifo or fsm is not empty\n");
+		goto out_err;
+	}
+
+
+	if (req->data.type != ONLY_SPARE) {
+		aw_host_nfc_dma_config_start(host, req->type, req->data.main, req->data.main_len);
+		/*write command*/
+		writel(ecc_block_bitmap, nfc->data_block_mask);
+		writel(cmd, nfc->cmd);
+		/*aw_nfc_reg_dump(nfc);*/
+		ret = aw_host_nfc_dma_wait_end(host, req->type, req->data.main, req->data.main_len);
+		if (ret) {
+			awrawnand_err("%s wait dma end fail\n", __func__);
+			goto out_err;
+		}
+	} else {
+		writel(ecc_block_bitmap, nfc->data_block_mask);
+		/*write command*/
+		writel(cmd, nfc->cmd);
+	}
+
+	ret = aw_host_nfc_wait_cmd_finish(nfc);
+	if (ret) {
+		awrawnand_err("wait cmd finish fail\n");
+		goto out_err;
+	}
+
+	if (host->use_rb_int) {
+		/*to do*/
+	} else {
+		ret = aw_host_nfc_wait_rb_ready(chip, host);
+		if (!ret) {
+			awrawnand_err("wait rb ready fail\n");
+			aw_nfc_reg_dump(nfc);
+			ret = -ETIMEDOUT;
+			goto out_err;
+		}
+		ret = 0;
+	}
+
+	if (req->type == FLASH_READ) {
+		ret = aw_host_nfc_check_ecc_status(nfc, req->data.main_len);
+		if (ret == ECC_GOOD) {
+			int len = req->data.main_len ? req->data.main_len : req->data.spare_len;
+			len = ((len < 1024) ? 1024 : len);
+			if (aw_host_nfc_is_blank_page(nfc, len)) {
+				AWRAWNAND_TRACE_NFC("%s-%d page%d is blank\n", __func__, __LINE__, page);
+				memset(req->data.main, 0xff, req->data.main_len);
+				memset(req->data.spare, 0xff, req->data.spare_len);
+			} else {
+				aw_host_nfc_get_spare_data(nfc, spare, MAX_SPARE_SIZE);
+				memcpy(req->data.spare, spare, req->data.spare_len);
+			}
+		} else {
+			aw_host_nfc_get_spare_data(nfc, spare, MAX_SPARE_SIZE);
+			memcpy(req->data.spare, spare, req->data.spare_len);
+		}
+
+		chip->bitflips = host->bitflips;
+
+		aw_host_nfc_ecc_disable(nfc);
+		if (chip->random) {
+			aw_host_nfc_randomize_disable(nfc);
+		}
+	} else {
+		aw_host_nfc_ecc_disable(nfc);
+		if (chip->random) {
+			aw_host_nfc_randomize_disable(nfc);
+		}
+		aw_host_nfc_set_dummy_byte(nfc, 0);
+	}
+
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return ret;
+
+out_err:
+	aw_host_nfc_ecc_disable(nfc);
+	if (chip->random) {
+		aw_host_nfc_randomize_disable(nfc);
+	}
+	if (req->type == FLASH_WRITE)
+		aw_host_nfc_set_dummy_byte(nfc, 0);
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return ret;
+}
+
+static int aw_host_set_mdclk(struct aw_nand_host *host, uint32_t mdclk)
+{
+	long rate = 0;
+	int ret = 0;
+
+	rate = clk_round_rate(host->mdclk, mdclk * 1000000);
+
+	ret = clk_set_rate(host->mdclk, mdclk * 1000000);
+	if (ret)
+		awrawnand_err("set nand0 clk@%u MHz fail\n", mdclk);
+
+	return ret;
+}
+
+static int aw_host_set_mcclk(struct aw_nand_host *host, uint32_t mcclk)
+{
+	long rate = 0;
+	int ret = 0;
+
+	rate = clk_round_rate(host->mcclk, mcclk * 1000000);
+
+	ret = clk_set_rate(host->mcclk, mcclk * 1000000);
+	if (ret)
+		awrawnand_err("set nand0 clk@%u MHz fail\n", mcclk);
+
+	return ret;
+
+}
+
+static int aw_host_clk_init(struct aw_nand_host *host)
+{
+	int ret = 0;
+	long rate = 0;
+
+	rate = clk_get_rate(host->pclk);
+	awrawnand_dbg("get pll rate@%ld Hz\n", rate);
+
+
+	ret = clk_set_parent(host->mdclk, host->pclk);
+	if (ret) {
+		awrawnand_err("set nand0 clk parent to pll fail\n");
+		goto out;
+	}
+
+	rate = clk_round_rate(host->mdclk, host->mdclk_val * 1000000);
+	awrawnand_info("rate@%ld\n", rate);
+	ret = clk_set_rate(host->mdclk, rate);
+	if (ret) {
+		awrawnand_err("set nand0 rate@%ld fail\n", rate);
+		goto out;
+	}
+
+	ret = clk_prepare_enable(host->mdclk);
+	if (ret) {
+		awrawnand_err("enable nand0 clk fail\n");
+		goto out;
+	}
+
+	ret = clk_set_parent(host->mcclk, host->pclk);
+	if (ret) {
+		awrawnand_err("set nand0 ecc engine clk parent to pll fail\n");
+		goto out;
+	}
+
+	rate = clk_round_rate(host->mcclk, host->mcclk_val * 1000000);
+	ret = clk_set_rate(host->mdclk, rate);
+	if (ret) {
+		awrawnand_err("set nand0 ecc engine rate@%ld fail\n", rate);
+		goto out;
+	}
+
+	ret = clk_prepare_enable(host->mcclk);
+	if (ret) {
+		awrawnand_err("enable nand0 ecc engin clk fail\n");
+		goto out;
+	}
+
+	host->clk_rate = host->mdclk_val;
+
+out:
+	return ret;
+}
+
+static int aw_host_set_clk(struct aw_nand_host *host, uint32_t mdclk,
+		uint32_t mcclk)
+{
+	int ret = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s [%d:%d]\n", __func__, mdclk, mcclk);
+
+
+	if (host->clk_rate == mdclk)
+		goto out;
+
+	if (!host->init) {
+		ret = aw_host_clk_init(host);
+		if (ret) {
+			awrawnand_err("host clk init fail\n");
+		}
+		goto out;
+	}
+
+	ret = aw_host_set_mdclk(host, mdclk);
+	if (ret) {
+		awrawnand_err("set mdclk fail\n");
+		goto out;
+	}
+
+	ret = aw_host_set_mcclk(host, mcclk);
+	if (ret) {
+		awrawnand_err("set mcclk fail\n");
+		goto out;
+	}
+
+	host->clk_rate = mdclk;
+
+	awrawnand_print("awrawnand(mtd):mdclk:%ld mcclk:%ld\n",
+			clk_get_rate(host->mdclk), clk_get_rate(host->mcclk));
+out:
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return ret;
+}
+
+static int aw_host_nfc_init(struct nfc_reg *nfc)
+{
+	int ret = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+	ret = aw_host_nfc_reset(nfc);
+	if (ret) {
+		awrawnand_err("aw nfc reset fail@%d\n", ret);
+		goto out;
+	}
+	aw_host_nfc_ctl_init(nfc);
+	aw_host_nfc_spare_area_init(nfc);
+	aw_host_nfc_efr_init(nfc);
+
+	aw_host_nfc_randomize_disable(nfc);
+	aw_host_nfc_timing_init(nfc);
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+out:
+	return ret;
+}
+
+static inline int aw_host_resource_init(struct aw_nand_host *host)
+{
+	int ret = 0;
+	struct device *dev = host->dev;
+	const char *vcc_nand = NULL;
+	const char *vcc_io = NULL;
+
+	ret = of_property_read_string(dev->of_node, "nand0_regulator1",
+			&vcc_nand);
+	if (ret) {
+		awrawnand_err("fail to get vcc nand in device tree\n");
+		goto out;
+	}
+
+	host->vcc_nand = regulator_get(NULL, vcc_nand);
+	if (IS_ERR_OR_NULL(host->vcc_nand)) {
+		awrawnand_err("fail to get regulator vcc-nand\n");
+		goto out;
+	}
+
+	ret = of_property_read_string(dev->of_node, "nand0_regulator2",
+			&vcc_io);
+	if (ret) {
+		awrawnand_err("fail to get vcc io in device tree\n");
+		goto out;
+	}
+
+	host->vcc_io = regulator_get(NULL, vcc_io);
+	if (IS_ERR_OR_NULL(host->vcc_io)) {
+		awrawnand_err("fail to get regulator vcc-nand\n");
+		goto out;
+	}
+
+	host->pclk = of_clk_get(dev->of_node, 0);
+	if (IS_ERR_OR_NULL(host->pclk)) {
+		awrawnand_err("fail to get pll clk\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	host->mdclk = of_clk_get(dev->of_node, 1);
+	if (IS_ERR_OR_NULL(host->mdclk)) {
+		awrawnand_err("fail to get nfc clk\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	host->mcclk = of_clk_get(dev->of_node, 2);
+	if (IS_ERR_OR_NULL(host->mcclk)) {
+		awrawnand_err("fail to get nfc ecc engine clk\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	host->base = (void __iomem *)of_iomap(dev->of_node, 0);
+	if (host->base == NULL) {
+		awrawnand_err("fail to get nfc base reg\n");
+		goto out;
+	}
+
+	host->mdclk_val = 10;
+	host->mcclk_val = 20;
+	host->dma_type = MBUS_DMA;
+	host->use_dma_int = 0;
+	host->spare_default = kzalloc(MAX_SPARE_SIZE, GFP_KERNEL);
+	if (!host->spare_default) {
+		awrawnand_err("kzalloc spare default fail\n");
+		goto out;
+	}
+
+	memset(host->spare_default, 0xff, MAX_SPARE_SIZE);
+
+	host->nfc_dma_desc_cpu = kmalloc(
+			sizeof(struct aw_nfc_dma_desc) * NFC_DMA_DESC_MAX_NUM, GFP_KERNEL);
+	if (host->nfc_dma_desc_cpu == NULL) {
+		awrawnand_err("kmalloc for dma desc fail\n");
+		ret = -ENOMEM;
+		kfree(host->spare_default);
+		goto out;
+	}
+
+	host->nfc_dma_desc = host->nfc_dma_desc_cpu;
+
+	host->normal_op = aw_host_nfc_normal_op;
+	host->batch_op = aw_host_nfc_batch_op;
+	host->rb_ready = aw_host_nfc_rb_ready;
+
+	aw_nfc_reg_prepare(&host->nfc_reg);
+
+	AWRAWNAND_TRACE_NFC("bus_gate:%08x\n", host->bus_gate);
+	AWRAWNAND_TRACE_NFC("mbus_gate:%08x\n", host->mbus_gate);
+	AWRAWNAND_TRACE_NFC("mdclk:%08x\n", host->mdclk);
+	AWRAWNAND_TRACE_NFC("mcclk:%08x\n", host->mcclk);
+out:
+	return ret;
+}
+
+static inline void aw_host_resource_destroy(struct aw_nand_host *host)
+{
+	if (host->spare_default)
+		kfree(host->spare_default);
+
+	if (host->nfc_dma_desc_cpu)
+		kfree(host->nfc_dma_desc_cpu);
+
+
+	host->nfc_dma_desc = NULL;
+
+}
+
+static int aw_nfc_pagesize_set(struct nfc_reg *nfc, int pagesize)
+{
+	int ret = 0;
+	uint32_t cfg = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s pagesize@%d\n", __func__, pagesize);
+
+	cfg = readl(nfc->ctl);
+
+	cfg &= ~NFC_PAGE_SHIFT_MSK;
+
+	switch (B_TO_KB(pagesize)) {
+	case 1:
+		cfg |= NFC_PAGE_SIZE_1KB;
+		break;
+	case 2:
+		cfg |= NFC_PAGE_SIZE_2KB;
+		break;
+	case 4:
+		cfg |= NFC_PAGE_SIZE_4KB;
+		break;
+	case 8:
+		cfg |= NFC_PAGE_SIZE_8KB;
+		break;
+	case 16:
+		cfg |= NFC_PAGE_SIZE_16KB;
+		break;
+	case 32:
+		cfg |= NFC_PAGE_SIZE_32KB;
+		break;
+	default:
+		awrawnand_err("Don't support pagesize@%d\n", pagesize);
+		ret = -EINVAL;
+	}
+
+	writel(cfg, nfc->ctl);
+	writel(pagesize, nfc->spare_area);
+
+	AWRAWNAND_TRACE_NFC("Exit %s ctl[%p:%x] spare_area[%p:%x]\n", __func__,
+			nfc->ctl, readl(nfc->ctl), nfc->spare_area, readl(nfc->spare_area));
+	return ret;
+}
+
+static void aw_nfc_ecc_mode_set(struct nfc_reg *nfc, int ecc_mode)
+{
+	uint32_t cfg = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s ecc_mode@%d\n", __func__, ecc_mode);
+
+	cfg = readl(nfc->ecc_ctl);
+	cfg &= ~NFC_ECC_MODE_MSK;
+	cfg |= ecc_mode;
+	writel(cfg, nfc->ecc_ctl);
+	AWRAWNAND_TRACE_NFC("Exit %s ecc ctl[%p:%x]\n", __func__, nfc->ecc_ctl, readl(nfc->ecc_ctl));
+
+}
+
+static void aw_nfc_set_interface(struct nfc_reg *nfc, enum rawnand_data_interface_type type,
+	uint32_t timing_ctl, uint32_t timing_cfg)
+{
+	uint32_t cfg = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s type@%x timing_ctl@%x timing_cfg@%x\n", __func__,
+		type, timing_ctl, timing_cfg);
+
+	/*set ctl interface*/
+	cfg = readl(nfc->ctl);
+	cfg &= ~NFC_DATA_INTERFACE_TYPE_MSK;
+	cfg |= (type & 0x3);
+	writel(cfg, nfc->ctl);
+
+	writel(timing_ctl, nfc->timing_ctl);
+	writel(timing_cfg, nfc->timing_cfg);
+	AWRAWNAND_TRACE_NFC("Exit %s ctl[%p: %x] timing_ctl[%p: %x] timing_cfg[%p: %x]\n",
+		__func__, nfc->ctl, nfc->timing_ctl, nfc->timing_cfg);
+}
+
+static int aw_host_set_clk_itf_sdr(struct aw_nand_host *host)
+{
+	struct aw_nand_chip *chip = awnand_host_to_chip(host);
+	struct nfc_reg *nfc = &host->nfc_reg;
+	uint32_t mdclk = chip->clk_rate;
+	uint32_t mcclk = mdclk * 2;
+	uint32_t cfg = 0;
+	int ret = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s mdclk@%d mcclk@%d\n", __func__, mdclk, mcclk);
+
+	/*set NAND FLASH Type*/
+	cfg = readl(nfc->ctl);
+	cfg &= ~NFC_DATA_INTERFACE_TYPE_MSK;
+	cfg |= NFC_DATA_INTERFACE_TYPE_SDR;
+	writel(cfg, nfc->ctl);
+
+	/*set READ PIPE to EDO*/
+	cfg = readl(nfc->timing_ctl);
+	cfg &= ~NFC_TIMING_CTL_PIPE_MSK;
+	cfg |= NFC_TIMING_SDR_EDO;
+	writel(cfg, nfc->timing_ctl);
+
+	ret = aw_host_set_clk(host, mdclk, mcclk);
+
+	return ret;
+}
+
+static int aw_host_set_interface(struct aw_nand_host *host)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = awnand_host_to_chip(host);
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+
+	int c = 0;
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+
+	if (RAWNAND_HAS_ONLY_TOGGLE(chip)) {
+		/*init toggle ddr interface with classic clock cfg(20MHz)*/
+		if (NAND_DATA_ITF_TYPE_TOGGLE_DDR1_2(chip)) {
+			ret = aw_host_set_clk(host, 20, 20*2);
+			if (ret)
+				goto out;
+			host->nf_type = RAWNAND_TOGGLE_DDR;
+			host->timing_ctl = NFC_TIMING_DDR_PIPE_SEL(0x2);
+			host->timing_ctl |= NFC_TIMING_DC_SEL(0x1f);
+
+			/*1. default value : 0x95
+			 *2. bit16 tCCS=1 for micron l85a, nvddr-100mHZ*/
+			host->timing_cfg = 0x10095;
+
+			 aw_nfc_set_interface(&host->nfc_reg, host->nf_type, host->timing_ctl, host->timing_cfg);
+		}
+	}
+
+	if (RAWNAND_NEED_CHANGE_TO_SDR(chip)) {
+		int feature_addr = TOGGLE_INTERFACE_CHANGE_ADDR;
+		uint8_t p[4] = {1, 0, 0, 0};
+		for (c = 0; c < chip->chips; c++) {
+			chip->select_chip(mtd, c);
+			ret = chip->data_interface.set_feature(chip, feature_addr, p);
+			chip->select_chip(mtd, -1);
+		}
+	}
+
+	if (RAWNAND_HAS_ITF_SDR(chip)) {
+		ret = aw_host_set_clk_itf_sdr(host);
+
+	}
+	if (RAWNAND_HAS_ITF_ONFI_DDR(chip)) {
+		/*to do*/
+	}
+
+	if (RAWNAND_HAS_ITF_TOGGLE_DDR(chip)) {
+		/*to do*/
+	}
+
+out:
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+	return ret;
+}
+
+int aw_host_init_tail(struct aw_nand_host *host)
+{
+
+	int ret = 0;
+	struct aw_nand_chip *chip = awnand_host_to_chip(host);
+	int pagesize = 1 << chip->pagesize_shift;
+
+	/*update pagesize*/
+	aw_nfc_pagesize_set(&host->nfc_reg, pagesize);
+	/*update ecc mode*/
+	aw_nfc_ecc_mode_set(&host->nfc_reg, chip->ecc_mode);
+
+	ret = aw_host_set_interface(host);
+
+	return ret;
+}
+
+static int aw_host_requlator_request(struct aw_nand_host *host)
+{
+	if (regulator_enable(host->vcc_nand)) {
+		awrawnand_err("request vcc nand fail\n");
+		return -EPERM;
+	}
+
+	if (regulator_enable(host->vcc_io)) {
+		awrawnand_err("request vcc io fail\n");
+		return -EPERM;
+	}
+
+	return 0;
+}
+
+int aw_host_init(struct device *dev)
+{
+	struct aw_nand_host *host = get_host();
+	int ret = 0;
+
+	memset(&aw_host, 0, sizeof(struct aw_nand_host));
+	host->priv = &awnand_chip;
+	host->init = false;
+	host->dev = dev;
+
+
+	ret = aw_host_resource_init(host);
+	if (ret)
+		goto out;
+
+	ret = aw_host_requlator_request(host);
+	if (ret)
+		goto out;
+
+	/*set pin*/
+	ret = aw_host_set_pin(host);
+	if (ret) {
+		awrawnand_err("set pin fail err@%d\n", ret);
+		goto out;
+	}
+
+	/*set clock*/
+	ret = aw_host_set_clk(host, host->mdclk_val, host->mcclk_val);
+	if (ret) {
+		awrawnand_err("set clk fail err@%d\n", ret);
+		goto out;
+	}
+
+
+	ret = aw_host_nfc_init(&host->nfc_reg);
+	if (ret) {
+		awrawnand_err("host nfc fail err@%d\n", ret);
+		goto out;
+	}
+
+	host->init = true;
+	return ret;
+
+out:
+	pr_err("%s-%d err@%d fail\n", __func__, __LINE__, ret);
+	memset(&aw_host, 0, sizeof(struct aw_nand_host));
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aw_host_init);
+
+void aw_host_exit(struct aw_nand_host *host)
+{
+	aw_host_resource_destroy(host);
+}
+EXPORT_SYMBOL_GPL(aw_host_exit);
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_nfc.h b/drivers/mtd/awnand/rawnand/aw_rawnand_nfc.h
new file mode 100644
index 000000000..dd34866bb
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_nfc.h
@@ -0,0 +1,571 @@
+/**
+ * SPDX-License-Identifier: GPL-2.0+
+ * aw_rawnand_nfc.h
+ *
+ * (C) Copyright 2020 - 2021
+ * Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+#ifndef __AW_RAWNAND_NFC_H__
+#define __AW_RAWNAND_NFC_H__
+
+#include <linux/sizes.h>
+#include <linux/dma-direction.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <asm/io.h>
+
+#define NFC_DEFAULT_TIMEOUT_MS	1000
+
+#ifndef BIT
+#define BIT(nr)			(1UL << (nr))
+#endif
+
+
+#ifndef SZ_2K
+#define SZ_2K				0x00000800
+#endif
+
+
+
+/* define bit use in NFC_CTL */
+#define NFC_EN			BIT(0)
+#define NFC_RESET		BIT(1)
+#define NFC_BUS_WIDTH_MSK	BIT(2)
+#define NFC_BUS_WIDTH_8		(0 << 2)
+#define NFC_BUS_WIDTH_16	(1 << 2)
+#define NFC_RB_SEL_MSK		(0x3 << 3)
+#define NFC_RB_SEL(x)		((x) << 3)
+#define NFC_CE_SEL_MSK		(0xf << 24)
+#define NFC_CE_SEL(x)		((x) << 24)
+#define NFC_CE_CTL		BIT(6)
+#define NFC_PAGE_SHIFT_MSK	(0xf << 8)
+#define NFC_SAM			BIT(12)
+#define NFC_DMA_TYPE		BIT(15)
+#define NFC_RAM_METHOD_DMA	BIT(14)
+#define NFC_DATA_INTERFACE_TYPE_MSK (0x3 << 18)
+#define NFC_DATA_INTERFACE_TYPE_SDR (0x0 << 18)
+#define NFC_DATA_INTERFACE_TYPE_ONFI_DDR (0x2 << 18)
+#define NFC_DATA_INTERFACE_TYPE_TOGGLE_DDR (0x3 << 18)
+#define NFC_DATA_INTERFACE_TYPE_IS_DDR(reg_val)	(reg_val & (1 << 19))
+#define NFC_DDR_REPEAT_ENABLE	BIT(20)
+#define NFC_NAND_INTERFACE_DDR_TYPE_DDR2 BIT(28)
+#define NFC_DEBUG_CTL		BIT(31)
+
+#define NFC_PAGE_SIZE_1KB	(0 << 8)
+#define NFC_PAGE_SIZE_2KB	(1 << 8)
+#define NFC_PAGE_SIZE_4KB	(2 << 8)
+#define NFC_PAGE_SIZE_8KB	(3 << 8)
+#define NFC_PAGE_SIZE_16KB	(4 << 8)
+#define NFC_PAGE_SIZE_32KB	(5 << 8)
+
+/* define bit use in NFC_STATUS*/
+#define NFC_RB_B2R		BIT(0)
+#define NFC_CMD_INT_FLAG	BIT(1)
+#define NFC_DMA_INT_FLAG	BIT(2)
+#define NFC_CMD_FIFO_STATUS	BIT(3)
+#define NFC_STA			BIT(4)
+#define NFC_RB_STATE(x)		BIT(x + 8)
+
+/* define bit use in NFC_INT */
+#define NFC_B2R_INT_ENABLE	BIT(0)
+#define NFC_CMD_INT_ENABLE	BIT(1)
+#define NFC_DMA_INT_ENABLE	BIT(2)
+#define NFC_INT_MASK		(0x7 << 0)
+
+/* define bit use in NFC_CMD */
+#define NFC_CMD_LOW_BYTE_MSK	0xff
+#define NFC_CMD_HIGH_BYTE_MSK	(0xff << 8)
+#define NFC_CMD(x)		(x)
+#define NFC_ADR_NUM_MSK		(0x7 << 16)
+#define NFC_ADR_NUM(x)		((((x) - 1)&0x7) << 16)
+#define NFC_SEND_ADR		BIT(19)
+#define NFC_ACCESS_DIR		BIT(20)
+#define NFC_DATA_TRANS		BIT(21)
+#define NFC_SEND_CMD1		BIT(22)
+#define NFC_WAIT_FLAG		BIT(23)
+#define NFC_SEND_CMD2		BIT(24)
+#define NFC_SEQ			BIT(25)
+#define NFC_DATA_SWAP_METHOD	BIT(26)
+#define NFC_SEND_RAN_CMD2	BIT(27)
+#define NFC_SEND_CMD3		BIT(28)
+#define NFC_SEND_CMD4		BIT(29)
+#define NFC_CMD_TYPE_MSK	(0x3 << 30)
+#define NFC_NORMAL_OP		(0 << 30)
+#define NFC_ECC_OP		(1 << 30)
+#define NFC_BATCH_OP		(2 << 30)
+
+/* define bit use in NFC_RCMD_SET */
+#define NFC_READ_CMD_MSK	0xff
+#define NFC_RND_READ_CMD0_MSK	(0xff << 8)
+#define NFC_RND_READ_CMD1_MSK	(0xff << 16)
+
+/* define bit use in NFC_WCMD_SET */
+#define NFC_PROGRAM_CMD_MSK	0xff
+#define NFC_RND_WRITE_CMD_MSK	(0xff << 8)
+#define NFC_READ_CMD0_MSK	(0xff << 16)
+#define NFC_READ_CMD1_MSK	(0xff << 24)
+
+/* define bit use in NFC_EFR*/
+#define NFC_ECC_DEBUG		(0x3f << 0)
+#define NFC_WP_CTL		BIT(8)
+#define NFC_DUMMY_BYTE_MSK	(0xff << 16)
+#define NFC_DUMMY_BYTE_SET(x)	(((x)&0xff) << 16)
+#define NFC_DUMMY_BYTE_EN	BIT(24)
+
+
+/* define bit use in NFC_ECC_CTL*/
+#define NFC_ECC_EN		BIT(0)
+#define NFC_ECC_PIPELINE	BIT(3)
+#define NFC_ECC_EXCEPTION	BIT(4)
+#define NFC_RANDOM_EN		BIT(5)
+#define NFC_ECC_MODE_MSK	(0xff << 8)
+#define NFC_ECC_SEL(x)		((x) << 8)
+#define NFC_ECC_GET(x)		(((x) >> 8)&0xff)
+#define NFC_RANDOM_SEED_MSK	(0x7fff << 16)
+#define NFC_RANDOM_SEED_SEL(x)	((x) << 16)
+
+#define NFC_RANDOM_SEED_DEFAULT	(0x4a80 << 16)
+
+/* define bit use in NFC_TIMING_CTL*/
+#define NFC_TIMING_CTL_PIPE_MSK (0xf << 8)
+#define NFC_TIMING_CTL_DC_MSK (0x3f << 0)
+#define NFC_TIMING_SDR_EDO (1 << 8)
+#define NFC_TIMING_SDR_EEDO (2 << 8)
+#define NFC_TIMING_DC_SEL(x)	((x) << 0)
+#define NFC_TIMING_DDR_PIPE_SEL(x)	((x) << 8)
+
+
+/*define bit use in NFC_SPARE_AREA*/
+#define NFC_SPARE_AREA_MSK	(0xffff << 0)
+
+
+
+#define ECC_BLOCKC_SIZE	(1024)
+
+
+static const uint32_t random_seed[128] = {
+    //0        1      2       3        4      5        6       7       8       9
+    0x2b75, 0x0bd0, 0x5ca3, 0x62d1, 0x1c93, 0x07e9, 0x2162, 0x3a72, 0x0d67, 0x67f9,
+    0x1be7, 0x077d, 0x032f, 0x0dac, 0x2716, 0x2436, 0x7922, 0x1510, 0x3860, 0x5287,
+    0x480f, 0x4252, 0x1789, 0x5a2d, 0x2a49, 0x5e10, 0x437f, 0x4b4e, 0x2f45, 0x216e,
+    0x5cb7, 0x7130, 0x2a3f, 0x60e4, 0x4dc9, 0x0ef0, 0x0f52, 0x1bb9, 0x6211, 0x7a56,
+    0x226d, 0x4ea7, 0x6f36, 0x3692, 0x38bf, 0x0c62, 0x05eb, 0x4c55, 0x60f4, 0x728c,
+    0x3b6f, 0x2037, 0x7f69, 0x0936, 0x651a, 0x4ceb, 0x6218, 0x79f3, 0x383f, 0x18d9,
+    0x4f05, 0x5c82, 0x2912, 0x6f17, 0x6856, 0x5938, 0x1007, 0x61ab, 0x3e7f, 0x57c2,
+    0x542f, 0x4f62, 0x7454, 0x2eac, 0x7739, 0x42d4, 0x2f90, 0x435a, 0x2e52, 0x2064,
+    0x637c, 0x66ad, 0x2c90, 0x0bad, 0x759c, 0x0029, 0x0986, 0x7126, 0x1ca7, 0x1605,
+    0x386a, 0x27f5, 0x1380, 0x6d75, 0x24c3, 0x0f8e, 0x2b7a, 0x1418, 0x1fd1, 0x7dc1,
+    0x2d8e, 0x43af, 0x2267, 0x7da3, 0x4e3d, 0x1338, 0x50db, 0x454d, 0x764d, 0x40a3,
+    0x42e6, 0x262b, 0x2d2e, 0x1aea, 0x2e17, 0x173d, 0x3a6e, 0x71bf, 0x25f9, 0x0a5d,
+    0x7c57, 0x0fbe, 0x46ce, 0x4939, 0x6b17, 0x37bb, 0x3e91, 0x76db
+};
+
+
+static uint8_t ecc_bits_tbl[15] = {16, 24, 28, 32, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80};
+static uint8_t ecc_limit_tab[15] = {13, 20, 23, 27, 35, 39, 42, 46, 50, 54, 58, 62, 66, 68, 72};
+#define MAX_ECC_BCH_80	((sizeof(ecc_bits_tbl)/sizeof(ecc_bits_tbl[0])) - 1)
+
+
+static inline int aw_host_nfc_wait_status(volatile uint32_t *reg, uint32_t mark,
+		uint32_t val, uint32_t timeout_ms)
+{
+	unsigned long timeout = 0;
+	int ret = -ETIMEDOUT;
+
+	timeout = jiffies + msecs_to_jiffies(timeout_ms);
+	do {
+		if ((readl(reg) & mark) == val) {
+			ret = 0;
+			break;
+		} else
+			cond_resched();
+	} while (time_before(jiffies, timeout));
+
+	return ret;
+
+}
+
+static inline int aw_host_nfc_reset(struct nfc_reg *nfc)
+{
+	uint32_t val = 0;
+	int ret = -ETIMEDOUT;
+	unsigned long timeout = 0;
+
+	val = readl(nfc->ctl);
+	val |= NFC_RESET;
+	writel(val, nfc->ctl);
+
+	/*ms*/
+	timeout = jiffies + msecs_to_jiffies(30);
+
+	do {
+		if (!(readl(nfc->ctl) & NFC_RESET)) {
+			ret = 0;
+			break;
+		} else
+			cond_resched();
+	} while (time_before(jiffies, timeout));
+
+	awrawnand_info("nfc rest %s\n", ret ? "fail" : "ok");
+	return ret;
+}
+
+static inline void aw_host_nfc_ctl_init(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+
+	cfg |= NFC_EN;
+	cfg |= NFC_BUS_WIDTH_8;
+	cfg &= ~NFC_CE_CTL;
+	cfg |= NFC_PAGE_SIZE_2KB;
+	cfg &= ~NFC_DATA_INTERFACE_TYPE_MSK;
+	cfg |= NFC_DATA_INTERFACE_TYPE_SDR;
+
+	writel(cfg, nfc->ctl);
+}
+static inline void aw_host_nfc_timing_init(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+
+	cfg = readl(nfc->timing_ctl);
+	cfg &= ~NFC_TIMING_CTL_PIPE_MSK;
+	cfg &= ~NFC_TIMING_CTL_DC_MSK;
+	cfg |= NFC_TIMING_SDR_EDO;
+
+	writel(cfg, nfc->timing_ctl);
+
+	/*1.default value 0x95
+	 *2. bit16 tCCS=1 for micron l85a, NVDDR-100mhz*/
+	cfg = 0x10095;
+	writel(cfg, nfc->timing_cfg);
+}
+
+static inline void aw_host_nfc_spare_area_init(struct nfc_reg *nfc)
+{
+	writel((SZ_2K & NFC_SPARE_AREA_MSK), nfc->spare_area);
+}
+
+
+static inline void aw_host_nfc_efr_init(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg |= NFC_WP_CTL;
+	writel(cfg, nfc->efr);
+}
+
+static inline void aw_host_nfc_randomize_disable(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->ecc_ctl);
+	cfg &= ~NFC_RANDOM_EN;
+	cfg &= ~NFC_RANDOM_SEED_MSK;
+	cfg |= NFC_RANDOM_SEED_DEFAULT;
+	writel(cfg, nfc->ecc_ctl);
+
+}
+
+/*enable randomizer and set random seed*/
+static inline void aw_host_nfc_randomize_enable(struct nfc_reg *nfc, uint32_t page)
+{
+	uint32_t cfg = 0, seed = 0;
+
+	seed = random_seed[page % 128];
+
+	cfg = readl(nfc->ecc_ctl);
+
+	cfg &= ~NFC_RANDOM_SEED_MSK;
+	cfg |= NFC_RANDOM_SEED_SEL(seed);
+	cfg |= NFC_RANDOM_EN;
+
+	writel(cfg, nfc->ecc_ctl);
+}
+
+
+static inline void aw_host_nfc_set_ecc_mode(struct nfc_reg *nfc, uint8_t ecc_mode)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->ecc_ctl);
+	cfg &= ~NFC_ECC_MODE_MSK;
+	cfg |= (NFC_ECC_SEL(ecc_mode) & NFC_ECC_MODE_MSK);
+	writel(cfg, nfc->ecc_ctl);
+}
+
+static inline void aw_host_nfc_ecc_enable(struct nfc_reg *nfc, uint8_t pipline)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->ecc_ctl);
+
+	if (pipline)
+		cfg |= NFC_ECC_PIPELINE;
+	else
+		cfg &= ~NFC_ECC_PIPELINE;
+
+	cfg |= NFC_ECC_EXCEPTION;
+	cfg |= NFC_ECC_EN;
+
+	writel(cfg, nfc->ecc_ctl);
+}
+
+static inline void aw_host_nfc_ecc_disable(struct nfc_reg *nfc)
+{
+	writel((readl(nfc->ecc_ctl) & (~NFC_ECC_EN)), nfc->ecc_ctl);
+}
+
+
+static inline void aw_host_nfc_chip_select(struct nfc_reg *nfc, int chip)
+{
+	uint32_t cfg = 0;
+
+	/*ce <==> rb*/
+	cfg = readl(nfc->ctl);
+	cfg &= ~NFC_CE_SEL_MSK;
+	cfg |= NFC_CE_SEL(chip);
+	cfg &= ~NFC_RB_SEL_MSK;
+	if (chip != 0xf)
+		cfg |= NFC_RB_SEL(chip);
+	writel(cfg, nfc->ctl);
+}
+
+static inline int aw_host_nfc_get_selected_rb_no(struct nfc_reg *nfc)
+{
+	return ((readl(nfc->ctl) & NFC_RB_SEL_MSK) >> 3);
+}
+
+static inline void aw_host_nfc_set_addr(struct nfc_reg *nfc, uint8_t *addr, int addr_num)
+{
+	uint32_t low = 0, high = 0;
+	int i = 0;
+
+	for (i = 0; i < addr_num; i++) {
+		if (i < 4)
+			low |= addr[i] << (i * 8);
+		else
+			high |= addr[i] << ((i - 4) * 8);
+	}
+
+	writel(low, nfc->addr_low);
+	writel(high, nfc->addr_high);
+}
+
+static inline void aw_host_nfc_repeat_mode_enable(struct nfc_reg *nfc)
+{
+	uint32_t val = 0;
+
+	val = readl(nfc->ctl);
+
+	if (NFC_DATA_INTERFACE_TYPE_IS_DDR(val)) {
+		val |= NFC_DDR_REPEAT_ENABLE;
+		writel(val, nfc->ctl);
+	}
+}
+
+static inline void aw_host_nfc_repeat_mode_disable(struct nfc_reg *nfc)
+{
+	uint32_t val = 0;
+
+	val = readl(nfc->ctl);
+
+	if (NFC_DATA_INTERFACE_TYPE_IS_DDR(val)) {
+		val &= ~NFC_DDR_REPEAT_ENABLE;
+		writel(val, nfc->ctl);
+	}
+}
+
+static inline void aw_host_nfc_dma_int_enable(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->int_ctl);
+	cfg |= NFC_DMA_INT_ENABLE;
+	writel(cfg, nfc->int_ctl);
+}
+
+static inline void aw_host_nfc_dma_int_disable(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->int_ctl);
+	cfg &= ~NFC_DMA_INT_ENABLE;
+	writel(cfg, nfc->int_ctl);
+}
+
+static inline void aw_host_nfc_dma_intstatus_clear(struct nfc_reg *nfc)
+{
+
+	writel(NFC_DMA_INT_FLAG, nfc->sta);
+}
+
+static inline bool aw_host_nfc_dma_int_occur_check(struct nfc_reg *nfc)
+{
+	return ((readl(nfc->sta) & NFC_DMA_INT_ENABLE) &&
+			(readl(nfc->int_ctl) & NFC_DMA_INT_ENABLE));
+}
+
+static inline void aw_host_nfc_rb_b2r_int_enable(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->int_ctl);
+	cfg |= NFC_B2R_INT_ENABLE;
+	writel(cfg, nfc->int_ctl);
+}
+
+static inline void aw_host_nfc_rb_b2r_int_disable(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->int_ctl);
+	cfg &= ~NFC_B2R_INT_ENABLE;
+	writel(cfg, nfc->int_ctl);
+}
+static inline void aw_host_nfc_rb_b2r_intstatus_clear(struct nfc_reg *nfc)
+{
+	uint32_t cfg = 0;
+
+	cfg = readl(nfc->sta);
+	cfg |= NFC_RB_B2R;
+	writel(cfg, nfc->sta);
+}
+
+static inline bool aw_host_nfc_rb_b2r_int_occur_check(struct nfc_reg *nfc)
+{
+	return ((readl(nfc->sta) & NFC_RB_B2R) && (readl(nfc->int_ctl) & NFC_B2R_INT_ENABLE));
+}
+
+static inline void aw_host_nfc_set_user_data_len(struct nfc_reg *nfc, uint32_t user_data_len)
+{
+	int i = 0;
+	int j = 0;
+	uint32_t cfg = 0;
+	/*In order to ndfc spec, one ecc block can attach user data len*/
+	uint8_t ecc_block_user_len[9] = {0, 4, 8, 12, 16, 20, 24, 28, 32};
+	uint8_t ecc_block_cnt = (user_data_len + 32 - 1) / 32;
+	uint8_t last_ecc_block_user_len = user_data_len % 32;
+	uint8_t last_cfg = 0;
+	uint8_t user_data_len_reg_cnt = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s user_data_len@%d\n", __func__, user_data_len);
+
+	/*find the last_ecc_block_user_len should configure what*/
+	for (i = 0; i < 9; i++) {
+		if (ecc_block_user_len[i] == last_ecc_block_user_len) {
+			last_cfg = i;
+			break;
+		}
+	}
+
+	if (user_data_len == 32)
+		last_cfg = 0x8;
+
+	/*user_data_len register per 4bits indicate
+	 * one ecc block user data len configure,
+	 * one user data len register can indicate 8 ecc block user len setting*/
+	user_data_len_reg_cnt = ecc_block_cnt / 8;
+	if (ecc_block_cnt % 8)
+		user_data_len_reg_cnt++;
+
+
+	for (i = 0; i < user_data_len_reg_cnt; i++) {
+		/*ecc block user data len configure to maximum(32B, 0x8 indicate),
+		 * except the last ecc block*/
+		cfg = 0;
+		if (i == (user_data_len_reg_cnt - 1)) {
+			for (j = 0; j < (ecc_block_cnt % 8); j++) {
+				if (j == ((ecc_block_cnt % 8) - 1)) {
+					cfg |= (last_cfg << (j * 4));
+					break;
+				} else
+					cfg |= 0x8 << (j * 4);
+			}
+		} else {
+			cfg = 0x88888888;
+		}
+		writel(cfg, (nfc->user_data_len_base + i));
+	}
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+}
+
+/*aw_host_nfc_set_boot0_user_data_len - boot0 user len by one ecc block attach 4Byte*/
+static inline void aw_host_nfc_set_boot0_user_data_len(struct nfc_reg *nfc, uint32_t user_data_len)
+{
+	int i = 0;
+	int j = 0;
+	uint32_t cfg = 0;
+	uint8_t user_data_len_reg_cnt = 0;
+	/*In order to ndfc spec, one ecc block can attach user data len*/
+
+/*boot0 user data manage*/
+#define ONE_ECC_BLOCK_ATTACH_4BYTE (4)
+
+	uint8_t ecc_block_cnt = (user_data_len + ONE_ECC_BLOCK_ATTACH_4BYTE - 1) / ONE_ECC_BLOCK_ATTACH_4BYTE;
+
+	AWRAWNAND_TRACE_NFC("Enter %s user_data_len@%d\n", __func__, user_data_len);
+
+	/*user_data_len register per 4bits indicate
+	 * one ecc block user data len configure,
+	 * one user data len register can indicate 8 ecc block user len setting*/
+	user_data_len_reg_cnt = ecc_block_cnt / 8;
+	if (ecc_block_cnt % 8)
+		user_data_len_reg_cnt++;
+
+
+	/*one ecc block attach 4Bytes*/
+	for (i = 0; i < user_data_len_reg_cnt; i++) {
+		cfg = 0;
+		/*one regisetr can configure 8 ecc block*/
+		if (i == (user_data_len_reg_cnt - 1)) {
+			for (j = 0; j < ecc_block_cnt % 8; j++) {
+				/*4bits indicate one ecc block*/
+				cfg |= (1 << (j * 4));
+			}
+		} else {
+			cfg = 0x11111111;
+		}
+		writel(cfg, (nfc->user_data_len_base + i));
+	}
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+}
+
+static inline void aw_host_nfc_set_user_data(struct nfc_reg *nfc, uint8_t *data, int len)
+{
+	int i = 0;
+	uint32_t val = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s data@%p len@%d\n", __func__, data, len);
+	if (!data)
+		return;
+	for (i = 0; i < len; i += 4) {
+		val = (data[i + 3] << 24 | data[i + 2] << 16 | data[i + 1] << 8 | data[i + 0]);
+		writel(val, nfc->user_data_base + (i >> 2));
+	}
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+}
+
+
+static inline void aw_host_nfc_set_dummy_byte(struct nfc_reg *nfc, int dummy_byte)
+{
+	uint32_t cfg = 0;
+
+	AWRAWNAND_TRACE_NFC("Enter %s\n", __func__);
+	cfg = readl(nfc->efr);
+	cfg &= ~NFC_DUMMY_BYTE_MSK;
+	cfg |= NFC_DUMMY_BYTE_SET(dummy_byte);
+	if (dummy_byte != 0)
+		cfg |= NFC_DUMMY_BYTE_EN;
+	writel(cfg, nfc->efr);
+
+	AWRAWNAND_TRACE_NFC("Exit %s\n", __func__);
+}
+
+
+#endif /*AW_RAWNAND_NFC*/
+
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_securestorage.c b/drivers/mtd/awnand/rawnand/aw_rawnand_securestorage.c
new file mode 100644
index 000000000..e811817da
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_securestorage.c
@@ -0,0 +1,30 @@
+/**
+ * SPDX-License-Identifier: GPL-2.0+
+ * aw_rawnand_securestorage.c
+ *
+ * (C) Copyright 2020 - 2021
+ * Allwinner Technology Co., Ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+
+#include <linux/mtd/aw-rawnand.h>
+
+
+struct aw_nand_sec_sto rawnand_sec_sto;
+
+int aw_rawnand_secure_storage_read(struct aw_nand_sec_sto *sec_sto,
+		int item, char *buf, unsigned int len)
+{
+	int ret = 0;
+
+	return ret;
+}
+
+int aw_rawnand_secure_storage_write(struct aw_nand_sec_sto *sec_sto,
+		int item, char *buf, unsigned int len)
+{
+	int ret = 0;
+
+	return ret;
+}
diff --git a/drivers/mtd/awnand/rawnand/aw_rawnand_spl.c b/drivers/mtd/awnand/rawnand/aw_rawnand_spl.c
new file mode 100644
index 000000000..bca92418c
--- /dev/null
+++ b/drivers/mtd/awnand/rawnand/aw_rawnand_spl.c
@@ -0,0 +1,922 @@
+/**
+ * spdx-license-identifier: gpl-2.0+
+ * aw_rawnand_spl.c
+ *
+ * (c) copyright 2020 - 2021
+ * allwinner technology co., ltd. <www.allwinnertech.com>
+ * cuizhikui <cuizhikui@allwinnertech.com>
+ *
+ */
+
+#include <linux/mtd/aw-rawnand.h>
+#include "aw_rawnand_nfc.h"
+
+#define  TOC0_MAGIC             "TOC0.GLH"
+#define  TOC_MAIN_INFO_MAGIC    0x89119800
+#define STAMP_VALUE             0x5F0A6C39
+
+typedef struct sbrom_toc1_head_info {
+	char name[16]	;	//user can modify
+	u32  magic	;	//must equal TOC_U32_MAGIC
+	u32  add_sum	;
+
+	u32  serial_num	;	//user can modify
+	u32  status		;	//user can modify,such as TOC_MAIN_INFO_STATUS_ENCRYP_NOT_USED
+
+	u32  items_nr;	//total entry number
+	u32  valid_len;
+	u32  version_main;	//only one byte
+	u32  version_sub;   //two bytes
+	u32  reserved[3];	//reserved for future
+
+	u32  end;
+} sbrom_toc1_head_info_t;
+
+/*keep it the same with driver/sunxi_flash/nand/nand_bsp.h*/
+typedef struct {
+	__u32	ChannelCnt;
+	__u32	ChipCnt;                            //the count of the total nand flash chips are currently connecting on the CE pin
+	__u32	ChipConnectInfo;                    //chip connect information, bit == 1 means there is a chip connecting on the CE pin
+	__u32	RbCnt;
+	__u32	RbConnectInfo;						//the connect  information of the all rb  chips are connected
+	__u32	RbConnectMode;						//the rb connect  mode
+	__u32	BankCntPerChip;                     //the count of the banks in one nand chip, multiple banks can support Inter-Leave
+	__u32	DieCntPerChip;                      //the count of the dies in one nand chip, block management is based on Die
+	__u32	PlaneCntPerDie;                     //the count of planes in one die, multiple planes can support multi-plane operation
+	__u32	SectorCntPerPage;                   //the count of sectors in one single physic page, one sector is 0.5k
+	__u32	PageCntPerPhyBlk;                   //the count of physic pages in one physic block
+	__u32	BlkCntPerDie;                       //the count of the physic blocks in one die, include valid block and invalid block
+	__u32	OperationOpt;                       //the mask of the operation types which current nand flash can support support
+	__u32	FrequencePar;                       //the parameter of the hardware access clock, based on 'MHz'
+	__u32	EccMode;                            //the Ecc Mode for the nand flash chip, 0: bch-16, 1:bch-28, 2:bch_32
+	__u8	NandChipId[8];                      //the nand chip id of current connecting nand chip
+	__u32	ValidBlkRatio;                      //the ratio of the valid physical blocks, based on 1024
+	__u32	good_block_ratio;					//good block ratio get from hwscan
+	__u32	ReadRetryType;						//the read retry type
+	__u32	DDRType;
+	__u32	uboot_start_block;
+	__u32	uboot_next_block;
+	__u32	logic_start_block;
+	__u32	nand_specialinfo_page;
+	__u32	nand_specialinfo_offset;
+	__u32	physic_block_reserved;
+	/*special nand cmd for some nand in batch cmd, only for write*/
+	__u32	random_cmd2_send_flag;
+	/*random col addr num in batch cmd*/
+	__u32	random_addr_num;
+	/*real physic page size*/
+	__u32	nand_real_page_size;
+	__u32	Reserved[13];
+} boot_nand_para_t;
+
+#define NAND_VERSION_0 0x03
+#define NAND_VERSION_1 0x01
+struct aw_rawnand_boot0 {
+	struct aw_nand_chip *chip;
+/*
+ *#define SLC_NAND	(0)
+ *#define MLC_NAND	(1)
+ */
+	uint8_t nand_type;
+	uint8_t ecc_mode;
+	uint8_t reserve[2];
+#define SLC_MDATA_IO (1024)
+#define MLC_MDATA_IO (4096)
+	/*mdata_len equal to (pagesize - ecc code), which >= MDATA_IO*/
+	int mdata_len;
+	uint8_t *mdata;
+#define BOOT0_OOB_LEN	(8)
+	uint8_t oob[BOOT0_OOB_LEN];
+	int init_flag;
+	/* according to boot0 sram size,
+	 * minimum one page 1K boot0img*/
+#define BOOT0_PAGE_CNT_PER_COPY	(128)
+	int page_cnt_per_copy;
+	int copys_per_blk;
+
+	/*uboot start block*/
+	int boundary;
+	int writen_copy;
+};
+
+struct aw_rawnand_boot0 g_boot0;
+
+static inline struct aw_rawnand_boot0 *get_rawnand_boot0(void)
+{
+	return &g_boot0;
+}
+
+static uint32_t sunxi_generate_checksum(void *buffer, uint32_t length, uint32_t div, uint32_t src_sum)
+{
+	uint32_t *buf;
+	int count;
+	uint32_t sum;
+
+	count = length >> 2;
+	sum   = 0;
+	buf   = (__u32 *)buffer;
+	do {
+		sum += *buf++;
+		sum += *buf++;
+		sum += *buf++;
+		sum += *buf++;
+	} while ((count -= (4*div)) > (4 - 1));
+
+	while (count-- > 0)
+		sum += *buf++;
+
+	sum = sum - src_sum + STAMP_VALUE;
+
+	return sum;
+}
+
+static uint32_t sunxi_sprite_generate_checksum(void *buffer, uint32_t length, uint32_t src_sum)
+{
+	return sunxi_generate_checksum(buffer, length, 1, src_sum);
+}
+
+/*usually small capacity nand is slc nand*/
+static int get_smallnand_uboot_start_block_num(void)
+{
+	return UBOOT_START_BLOCK_SMALLNAND;
+}
+
+/*usually big capacity nand is mlc nand*/
+static int get_bignand_uboot_start_block_num(void)
+{
+	return UBOOT_START_BLOCK_BIGNAND;
+}
+
+void rawnand_uboot_blknum(unsigned int *start, unsigned int *end)
+{
+	uint32_t uboot_block_size = 0;
+	uint32_t uboot_start_block = 0;
+	uint32_t uboot_next_block = 0;
+	uint32_t page_cnt_per_blk = 0;
+
+	struct aw_nand_chip *chip = get_rawnand();
+
+	page_cnt_per_blk = 1 << chip->pages_per_blk_shift;
+	uboot_block_size = chip->erasesize;
+
+	/*small nand:block size < 1MB;  reserve 4M for uboot*/
+	if (uboot_block_size <= 0x20000) { //128K
+		uboot_start_block = get_smallnand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 32;
+	} else if (uboot_block_size <= 0x40000) { //256k
+		uboot_start_block = get_smallnand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 16;
+	} else if (uboot_block_size <= 0x80000) { //512k
+		uboot_start_block = get_smallnand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 8;
+	} else if (uboot_block_size <= 0x100000 && page_cnt_per_blk <= 128) { //1M
+		uboot_start_block = get_smallnand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 4;
+	}
+	/* big nand;  reserve at least 20M for uboot */
+	else if (uboot_block_size <= 0x100000 && page_cnt_per_blk > 128) { //BIGNAND 1M
+		uboot_start_block = get_bignand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 20;
+	} else if (uboot_block_size <= 0x200000) { //BIGNAND 2M
+		uboot_start_block = get_bignand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 10;
+	} else {
+		uboot_start_block = get_bignand_uboot_start_block_num();
+		uboot_next_block = uboot_start_block + 8;
+	}
+
+	if (start) {
+		*start = uboot_start_block;
+		awrawnand_dbg("uboot_start@%u\n", *start);
+	}
+	if (end) {
+		*end = uboot_next_block;
+		awrawnand_dbg("uboot_end@%u\n", *end);
+	}
+
+}
+
+int rawslcnand_write_boot0_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+
+	int ret = 0;
+	uint8_t status = 0;
+	int row_cycles = chip->row_cycles;
+
+	BATCH_REQ_WRITE_SEQ(req, page, row_cycles, mdata, mlen, sdata, slen);
+
+	chip->operate_boot0 = 1;
+	chip->boot0_ecc_mode = MAX_ECC_BCH_80;
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write boot0 page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &req);
+	if (ret == ECC_ERR) {
+		awrawnand_err("%s write boot0 page@%d fail\n", __func__, 0);
+		goto out;
+	}
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy write boot0 page@%d fail\n", page);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+
+	status = chip->dev_status(mtd);
+	if (status & RAWNAND_STATUS_FAIL) {
+		awrawnand_err("write boot0 page@%d fail\n", page);
+		ret = -EIO;
+	}
+
+	chip->operate_boot0 = 0;
+	chip->boot0_ecc_mode = 0;
+
+out:
+	return ret;
+}
+
+int rawslcnand_read_boot0_page(struct mtd_info *mtd, struct aw_nand_chip *chip,
+		uint8_t *mdata, int mlen, uint8_t *sdata, int slen, int page)
+{
+	struct aw_nand_host *host = awnand_chip_to_host(chip);
+	int row_cycles = chip->row_cycles;
+
+	int ret = 0;
+
+
+	BATCH_REQ_READ_SEQ(req, page, row_cycles, mdata, mlen, sdata, slen);
+
+	chip->operate_boot0 = 1;
+	chip->boot0_ecc_mode = MAX_ECC_BCH_80;
+
+	if (!chip->dev_ready_wait(mtd)) {
+		awrawnand_err("dev is busy read page@%d fail\n", page);
+		ret = -EIO;
+		goto out;
+	}
+
+	ret = host->batch_op(chip, &req);
+	if (ret == ECC_ERR)
+		awrawnand_err("read page@%d fail\n", page);
+
+	chip->operate_boot0 = 0;
+	chip->boot0_ecc_mode = 0;
+
+out:
+	return ret;
+}
+
+
+static int rawnand_calc_page_valid_main_data_len(int pagesize, int boot0_ecc_mode, int ecc_mode)
+{
+	int boot0_ecc_bits = 0, normal_ecc_bits = 0;
+	int increase_ecc_size = 0;
+	int cnt = 1;
+	int i = 0;
+	/*ecc block size 1024Bytes*/
+	int ecc_block_cnts = (pagesize >> 10);
+
+	boot0_ecc_bits = ecc_bits_tbl[boot0_ecc_mode];
+	normal_ecc_bits = ecc_bits_tbl[ecc_mode];
+	increase_ecc_size = (14 * (boot0_ecc_bits - normal_ecc_bits) / 8);
+
+	/* data_min_io is ecc block size,
+	 * so the increased overhead(increase_ecc_size)
+	 * is calculate in terms of the data_min_io*/
+	for (i = (ecc_block_cnts - 1); i > 0; i--) {
+		if ((increase_ecc_size * i) < (cnt << 10))
+			break;
+		cnt++;
+	}
+
+	return ((ecc_block_cnts - cnt) << 10);
+}
+
+/**
+ * rawslcnand_write_boot0_one - write one or more copy boot0
+ * @len: boot0 len
+ * @buf: boot0 img
+ * @count: range from 0 to n(rawnand_uboot_blknum(&n, NULL))
+ */
+static int rawslcnand_write_boot0_one(struct aw_rawnand_boot0 *boot0,
+		unsigned int len, void *buf, int count)
+{
+	struct aw_nand_chip *chip = boot0->chip;
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int ret = 0;
+
+	int start = 0;
+	int blks_per_copy = 0;
+	int blk = 0;
+	int page = 0;
+	int pages_per_blk = (1 << chip->pages_per_blk_shift);
+	uint8_t *mdata = buf;
+	int mlen = boot0->mdata_len;
+	uint8_t *sdata = boot0->oob;
+	int slen = BOOT0_OOB_LEN;
+
+	int writen_len = 0;
+
+	blks_per_copy = boot0->page_cnt_per_copy >> chip->pages_per_blk_shift;
+	if (boot0->page_cnt_per_copy & chip->pages_per_blk_mask)
+		blks_per_copy++;
+
+	start = count * blks_per_copy;
+
+	if ((start + blks_per_copy) > boot0->boundary)
+		return 0;
+
+
+	chip->select_chip(mtd, 0);
+	for (blk = start; blk < start + blks_per_copy; blk++) {
+		page = blk << chip->pages_per_blk_shift;
+		ret = chip->erase(mtd, page);
+		if (ret) {
+			awrawnand_err("erase block@%d fail when download boot0 count@%d\n",
+					blk, count);
+			awrawnand_err("skip boot0 count@%d\n", count);
+			break;
+		}
+
+		for (; page < ((blk << chip->pages_per_blk_shift) + pages_per_blk);
+				page++) {
+			ret = chip->write_boot0_page(mtd, chip, mdata, mlen, sdata, slen, page);
+			if (ret) {
+				awrawnand_err("when write boot0 fail in page@%d\n\n", page);
+				/*the info is no mean*/
+				awrawnand_dbg("ecc mode@%d ecc limit@%d\n", ecc_bits_tbl[boot0->ecc_mode],
+						ecc_limit_tab[boot0->ecc_mode]);
+				goto out;
+			}
+			writen_len += SLC_MDATA_IO;
+			if (writen_len == len) {
+				writen_len = 0;
+				awrawnand_print("W boot0 cp@%d suc\n", boot0->writen_copy++);
+			}
+			mdata = buf + writen_len;
+		}
+	}
+	chip->select_chip(mtd, -1);
+
+out:
+	return ret;
+}
+
+/**
+ * rawslcnand_read_boot0_one - read a right boot0 in one boot0 area
+ * @len: boot0 size(buf len)
+ * @buf: boot0 buffer
+ * @count: 0 from n(rawnand_uboot_blknum(&n, NULL))
+ * */
+static int rawslcnand_read_boot0_one(struct aw_rawnand_boot0 *boot0,
+		unsigned int len, void *buf, int count)
+{
+	struct aw_nand_chip *chip = boot0->chip;
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int ret = 0;
+
+	int start = 0;
+	int blks_per_copy = 0;
+	int blk = 0;
+	int page = 0;
+	int pages_per_blk = (1 << chip->pages_per_blk_shift);
+	uint8_t *mdata = buf;
+	int mlen = boot0->mdata_len;
+	uint8_t *sdata = boot0->oob;
+	int slen = BOOT0_OOB_LEN;
+
+	int read_len = 0;
+
+	blks_per_copy = boot0->page_cnt_per_copy >> chip->pages_per_blk_shift;
+	if (boot0->page_cnt_per_copy & chip->pages_per_blk_mask)
+		blks_per_copy++;
+
+	start = count * blks_per_copy;
+
+	if ((start + blks_per_copy) > boot0->boundary)
+		return 0;
+
+
+	chip->select_chip(mtd, 0);
+	for (blk = start; blk < start + blks_per_copy; blk++) {
+		page = blk << chip->pages_per_blk_shift;
+
+		for (; page < ((blk << chip->pages_per_blk_shift) + pages_per_blk);
+				page++) {
+			ret = chip->read_boot0_page(mtd, chip, mdata, mlen, sdata, slen, page);
+			if (ret) {
+				awrawnand_err("when read boot0 fail in page@%d\n\n", page);
+				/*the info is no mean*/
+				awrawnand_dbg("ecc mode@%d ecc limit@%d\n",
+						ecc_bits_tbl[boot0->ecc_mode],
+						ecc_limit_tab[boot0->ecc_mode]);
+				goto out;
+			}
+			read_len += SLC_MDATA_IO;
+			if (read_len == len) {
+				awrawnand_info("R boot0 suc\n");
+				goto out;
+			}
+			mdata = buf + read_len;
+		}
+	}
+	chip->select_chip(mtd, -1);
+
+out:
+	return ret;
+}
+
+/**
+ * rawslcnand_mtd_download_boot0 - download boot0
+ * @boot0:boot0 structure
+ * @len: boot size
+ * @buf: boot0 img
+ * */
+static int rawslcnand_mtd_download_boot0(struct aw_rawnand_boot0 *boot0,
+		unsigned int len, void *buf)
+{
+	int ret = 0;
+	int count = 0;
+	unsigned int start = 0;
+	unsigned int end = 0;
+	rawnand_uboot_blknum(&end, NULL);
+
+	for (count = start; count < end; count++) {
+		ret &= rawslcnand_write_boot0_one(boot0, len, buf, count);
+	}
+
+	if (ret)
+		awrawnand_err("download boot0 fail\n");
+
+	return ret;
+}
+
+/**
+ * rawslcnand_mtd_upload_boot0 - read boot0
+ * @boot0: boot0 structure
+ * @len: boot0 size
+ * @buf: buffer to store boot0 img
+ * */
+static int rawslcnand_mtd_upload_boot0(struct aw_rawnand_boot0 *boot0,
+		unsigned int len, void *buf)
+{
+	int ret = 0;
+	int count = 0;
+	unsigned int start = 0;
+	unsigned int end = 0;
+	rawnand_uboot_blknum(&end, NULL);
+
+	for (count = start; count < end; count++) {
+		ret = rawslcnand_read_boot0_one(boot0, len, buf, count);
+		if (!ret)
+			break;
+	}
+
+	if (count == end)
+		awrawnand_err("upload boot0 fail\n");
+
+	return ret;
+}
+
+/**
+ * rawnand_mtd_download_boot0_init - init boot0 structure parameter
+ * */
+static inline void rawnand_mtd_download_boot0_init(void)
+{
+	struct aw_nand_chip *chip = get_rawnand();
+	unsigned int uboot_start;
+
+	rawnand_uboot_blknum(&uboot_start, NULL);
+
+	if (!g_boot0.init_flag) {
+		g_boot0.chip = chip;
+		g_boot0.nand_type = SLC_NAND;
+		g_boot0.page_cnt_per_copy = BOOT0_PAGE_CNT_PER_COPY;
+		g_boot0.ecc_mode = MAX_ECC_BCH_80;
+		g_boot0.mdata_len = rawnand_calc_page_valid_main_data_len(chip->pagesize,
+				g_boot0.ecc_mode, chip->ecc_mode);
+		memset(g_boot0.oob, 0xff, BOOT0_OOB_LEN);
+		g_boot0.oob[0] = 0xff;
+		g_boot0.oob[1] = 0x00;
+		g_boot0.oob[2] = NAND_VERSION_0;
+		g_boot0.oob[3] = NAND_VERSION_1;
+		g_boot0.init_flag = 1;
+		g_boot0.boundary = uboot_start;
+		g_boot0.writen_copy = 0;
+		awrawnand_info("boot0:nand_type@%s\n", g_boot0.nand_type ? "mlc" : "slc");
+		awrawnand_info("boot0:ecc_mode@%d\n", g_boot0.ecc_mode);
+		awrawnand_info("boot0:mdata_len@%d\n", g_boot0.mdata_len);
+		awrawnand_info("boot0:doundary@%d\n", g_boot0.boundary);
+	}
+}
+
+/**
+ * rawnand_mtd_download_boot0_exit - destory process var
+ * */
+static inline void rawnand_mtd_download_boot0_exit(void)
+{
+	g_boot0.init_flag = 0;
+	g_boot0.writen_copy = 0;
+}
+
+
+/**
+ * fill boot0 header
+ */
+int rawnand_mtd_get_flash_info(void *data, unsigned int len)
+{
+	struct aw_nand_chip *chip = get_rawnand();
+	boot_nand_para_t *boot_info = data;
+	unsigned int uboot_start, uboot_end;
+
+	rawnand_uboot_blknum(&uboot_start, &uboot_end);
+
+	/* nand information */
+
+	boot_info->ChipCnt = chip->chips;
+	boot_info->DieCntPerChip = chip->dies;
+	boot_info->SectorCntPerPage = (1 << (chip->pagesize_shift - 9));
+	boot_info->PageCntPerPhyBlk = (1 << chip->pages_per_blk_shift);
+	/*OperationOpt must be match to boot0 define*/
+	if (chip->options & RAWNAND_NFC_RANDOM)
+		boot_info->OperationOpt |= (1 << 7);
+	if (chip->options & RAWNAND_TOGGLE_DDR_TO_SDR)
+		boot_info->OperationOpt |= (1 << 27);
+	if (chip->type == SLC_NAND)
+		boot_info->OperationOpt &= ~(0xff << 12);
+	else {
+		boot_info->OperationOpt &= ~(0xff << 12);
+		awrawnand_err("don't support nand type, default slc nand\n");
+	}
+	boot_info->FrequencePar = chip->ecc_mode;
+	memcpy(boot_info->NandChipId, chip->id, RAWNAND_MAX_ID_LEN);
+
+
+	/* others */
+	boot_info->uboot_start_block = uboot_start;
+	boot_info->uboot_next_block = uboot_end;
+	boot_info->logic_start_block = uboot_end + AW_RAWNAND_RESERVED_PHY_BLK_FOR_SECURE_STORAGE;
+	boot_info->physic_block_reserved = 0;
+
+	awrawnand_info("flash id@%02x %02x %02x %02x %02x %02x %02x %02x\n",
+			boot_info->NandChipId[0], boot_info->NandChipId[1],
+			boot_info->NandChipId[2], boot_info->NandChipId[3],
+			boot_info->NandChipId[4], boot_info->NandChipId[5],
+			boot_info->NandChipId[6], boot_info->NandChipId[7]);
+	awrawnand_info("uboot_start_block@%u\n", boot_info->uboot_start_block);
+	awrawnand_info("uboot_end_block@%u\n", boot_info->uboot_next_block);
+
+	return 0;
+}
+
+static int rawnand_mtd_download_uboot_fill_remain_pages_in_block(struct aw_nand_chip *chip, int page)
+{
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int ret = 0;
+	int block_end_page = (((page >> chip->pages_per_blk_shift) + 1) << chip->pages_per_blk_shift);
+	int p = 0;
+	uint8_t sdata[chip->avalid_sparesize];
+	uint8_t *mdata = kzalloc(chip->pagesize, GFP_KERNEL);
+	if (mdata == NULL) {
+		awrawnand_err("fill remain page: kzalloc mdata fail\n");
+		return -ENOMEM;
+	}
+	memset(mdata, 0x55, chip->pagesize);
+	memset(sdata, 0xA5, chip->avalid_sparesize);
+
+	for (p = page; p < block_end_page; p++) {
+		ret = chip->write_page(mtd, chip, mdata, chip->pagesize,
+				sdata, chip->avalid_sparesize, p);
+		if (ret) {
+			awrawnand_err("fill remain data in block@%d page@%d fail chip page@%d\n",
+					page >> chip->pages_per_blk_shift, page & chip->pages_per_blk_mask, page);
+		}
+	}
+
+	kfree(mdata);
+	return 0;
+}
+
+static int rawslcnand_mtd_upload_uboot(struct aw_nand_chip *chip,
+		unsigned int len, void *buf)
+{
+	unsigned int start = 0;
+	unsigned int end = 0;
+	int blk = 0;
+	int ret = -1;
+	uint8_t *mdata = buf;
+	uint8_t sdata[chip->avalid_sparesize];
+	int slen = chip->avalid_sparesize;
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int pages_per_blk = (1 << chip->pages_per_blk_shift);
+	int had_read_len = 0;
+	sbrom_toc1_head_info_t *head = buf;
+	unsigned int uboot_len = 0;
+	uint32_t check_sum = 0;
+	int page = 0;
+	uint8_t *mbuf = kzalloc(chip->pagesize, GFP_KERNEL);
+	if (mbuf == NULL) {
+		awrawnand_err("upload uboot: kzalloc fail\n");
+		goto out1;
+	}
+	awrawnand_info("Enter %s\n", __func__);
+
+	rawnand_uboot_blknum(&start, &end);
+
+	chip->select_chip(mtd, 0);
+	for (blk = start; blk < end; blk++) {
+		if (chip->block_bad(mtd, blk)) {
+			awrawnand_err("block@%d is bad,skip it\n", blk);
+			continue;
+		}
+
+		if (had_read_len == 0)
+			mdata = buf;
+
+		page = blk << chip->pages_per_blk_shift;
+		for (; page < ((blk << chip->pages_per_blk_shift) + pages_per_blk);
+				page++) {
+
+
+			ret = chip->read_page(mtd, chip, mdata, chip->pagesize, sdata, slen, page);
+			if (ret) {
+				awrawnand_err("read page@%d fail\n", page);
+				break;
+			}
+
+			had_read_len += chip->pagesize;
+			mdata += chip->pagesize;
+
+			if (head->magic != TOC_MAIN_INFO_MAGIC) {
+				had_read_len = 0;
+				break;
+			}
+
+			uboot_len = head->valid_len;
+			if (len < uboot_len) {
+				awrawnand_err("upload uboot buffer size(len)@%d is less the"
+						"real ubootsize@%d, pls check\n", len,
+						uboot_len);
+				goto out;
+			}
+
+			if (had_read_len >= uboot_len) {
+				memcpy(buf + (had_read_len - chip->pagesize),
+						mbuf, uboot_len - (had_read_len - chip->pagesize));
+			}
+
+			/*last page data align to pagesize read, prevent illegal access buf*/
+			if ((uboot_len - had_read_len) <= chip->pagesize) {
+				mdata = mbuf;
+			}
+
+			if (had_read_len >= uboot_len) {
+				awrawnand_info("to check sum\n");
+				check_sum = sunxi_sprite_generate_checksum(buf,
+						uboot_len, head->add_sum);
+				had_read_len = 0;
+				if (check_sum == head->add_sum) {
+					awrawnand_print("upload uboot success\n");
+					ret = 0;
+					goto out;
+				} else
+					break;
+			}
+
+		} /*for (; ; page++*/
+	} /*for (blk = start; blk < end; blk++)*/
+
+	chip->select_chip(mtd, -1);
+	awrawnand_info("Exit %s\n", __func__);
+
+
+out:
+	kfree(mbuf);
+	return ret;
+out1:
+	return -ENOMEM;
+}
+
+static int rawslcnand_mtd_download_uboot(struct aw_nand_chip *chip,
+		unsigned int len, void *buf)
+{
+	unsigned int start = 0;
+	unsigned int end = 0;
+	int blk = 0;
+	int ret = 0;
+	uint8_t *mdata = buf;
+	uint8_t sdata[chip->avalid_sparesize];
+	int slen = chip->avalid_sparesize;
+	struct mtd_info *mtd = awnand_chip_to_mtd(chip);
+	int pages_per_blk = (1 << chip->pages_per_blk_shift);
+	int writen_len = 0;
+	int copy = 0;
+	int page = 0;
+	int fail_flag = 0;
+	uint8_t *mbuf = kzalloc(chip->pagesize, GFP_KERNEL);
+	if (mbuf == NULL) {
+		awrawnand_err("download uboot: kzalloc fail\n");
+		goto out;
+	}
+
+	/*uboot oob layout*/
+	sdata[0] = 0xff;
+	sdata[1] = 0;
+	sdata[2] = NAND_VERSION_0;
+	sdata[3] = NAND_VERSION_1;
+	memset(sdata + 4, 0xff, chip->avalid_sparesize - 4);
+
+	rawnand_uboot_blknum(&start, &end);
+
+	awrawnand_info("download uboot len@0x%x [%d to%d]\n", len, start, end);
+
+	chip->select_chip(mtd, 0);
+	for (blk = start; blk < end; blk++) {
+
+		if (chip->block_bad(mtd, blk)) {
+			awrawnand_err("block@%d is bad,skip it\n", blk);
+			continue;
+		}
+
+		ret = chip->erase(mtd, (blk << chip->pages_per_blk_shift));
+		if (ret) {
+			awrawnand_err("erase block@%d fail, skip it\n", blk);
+			continue;
+		}
+
+		if (writen_len == 0)
+			mdata = buf;
+
+		page = blk << chip->pages_per_blk_shift;
+		for (; page < ((blk << chip->pages_per_blk_shift) + pages_per_blk);
+				page++) {
+
+
+			ret = chip->write_page(mtd, chip, mdata, chip->pagesize, sdata, slen, page);
+			if (ret) {
+				awrawnand_err("write page@%d fail\n", page);
+				fail_flag = 1;
+			}
+
+			writen_len += chip->pagesize;
+			mdata += chip->pagesize;
+
+			/*prevent len is not align to pagesize*/
+			if (((len - writen_len) < chip->pagesize) &&
+					((len - writen_len) > 0)) {
+				memcpy(mbuf, mdata, (len - writen_len));
+				mdata = mbuf;
+			}
+
+			if (writen_len >= len) {
+				rawnand_mtd_download_uboot_fill_remain_pages_in_block(chip,
+						(page + 1));
+				if (fail_flag == 0)
+					awrawnand_print("W uboot cp@%d ok\n", copy++);
+				writen_len = 0;
+				break;
+			}
+		} /*for (; ; page++*/
+	} /*for (blk = start; blk < end; blk++)*/
+	chip->select_chip(mtd, -1);
+
+
+	/*at least one copy is ok*/
+	kfree(mbuf);
+	return copy ? 0 : -1;
+out:
+	return -ENOMEM;
+}
+
+/**
+ * rawnand_mtd_upload_uboot - upload uboot img interface
+ * @len: uboot size
+ * @buf: uboot img
+ **/
+int rawnand_mtd_upload_uboot(unsigned int len, void *buf)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = get_rawnand();
+
+	if (chip->type == SLC_NAND) {
+		ret = rawslcnand_mtd_upload_uboot(chip, len, buf);
+	} else {
+		awrawnand_err("upload uboot don't support nand type@%s\n",
+				chip->type == SLC_NAND ? "slc" : "mlc");
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rawnand_mtd_upload_uboot);
+
+/**
+ * rawnand_mtd_download_uboot - download uboot img interface
+ * @len: uboot size
+ * @buf: uboot img
+ **/
+int rawnand_mtd_download_uboot(unsigned int len, void *buf)
+{
+	int ret = 0;
+	struct aw_nand_chip *chip = get_rawnand();
+
+#ifdef CONFIG_AW_RAWNAND_BURN_CHECK_UBOOT
+	uint8_t *uboot = kzalloc(len, GFP_KERNEL);
+	if (uboot == NULL) {
+		awrawnand_err("kzalloc buffer for read uboot after write fail\n");
+		goto out;
+	}
+#endif
+
+	if (chip->type == SLC_NAND) {
+		ret = rawslcnand_mtd_download_uboot(chip, len, buf);
+	} else {
+		awrawnand_err("download uboot don't support nand type@%s\n",
+				chip->type == SLC_NAND ? "slc" : "mlc");
+		ret = -EPERM;
+		goto out;
+	}
+
+#ifdef CONFIG_AW_RAWNAND_BURN_CHECK_UBOOT
+	if (!rawnand_mtd_upload_uboot(len, uboot))
+		awrawnand_print("write & read the uboot is same\n");
+	kfree(uboot);
+#endif
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rawnand_mtd_download_uboot);
+
+/**
+ * rawnand_mtd_upload_boot0 - read boot0 interface
+ * @len: boot0 size
+ * @buf: boot0 img
+ **/
+int rawnand_mtd_upload_boot0(unsigned int len, void *buf)
+{
+
+	int ret = 0;
+	struct aw_rawnand_boot0 *boot0 = get_rawnand_boot0();
+
+	rawnand_mtd_download_boot0_init();
+
+	if (g_boot0.nand_type == SLC_NAND) {
+		ret = rawslcnand_mtd_upload_boot0(boot0, len, buf);
+	} else {
+		awrawnand_err("don't support nand to down boot0\n");
+		ret = -EINVAL;
+	}
+
+	rawnand_mtd_download_boot0_exit();
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rawnand_mtd_upload_boot0);
+/**
+ * rawnand_mtd_download_boot0 - download boot0 interface
+ * @len: boot0 size
+ * @buf: boot0 img
+ * */
+int rawnand_mtd_download_boot0(unsigned int len, void *buf)
+{
+
+	int ret = 0;
+	struct aw_rawnand_boot0 *boot0 = get_rawnand_boot0();
+#ifdef CONFIG_AW_RAWNAND_BURN_CHECK_BOOT0
+	uint8_t *rbuf = kzalloc(len, GFP_KERNEL);
+	if (!rbuf) {
+		awrawnand_err("malloc read buf fail\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+#endif
+
+	rawnand_mtd_download_boot0_init();
+
+	if (g_boot0.nand_type == SLC_NAND) {
+		ret = rawslcnand_mtd_download_boot0(boot0, len, buf);
+	} else {
+		awrawnand_err("don't support nand to down boot0\n");
+		ret = -EINVAL;
+		goto out;
+	}
+#ifdef CONFIG_AW_RAWNAND_BURN_CHECK_BOOT0
+	rawnand_mtd_upload_boot0(len, rbuf);
+
+	if (memcmp(rbuf, buf, len))
+		awrawnand_err("read boot0 fail\n");
+	else
+		awrawnand_info("read&write is the same\n");
+	kfree(rbuf);
+#endif
+	rawnand_mtd_download_boot0_exit();
+out:
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rawnand_mtd_download_boot0);
diff --git a/drivers/mtd/awnand/spinand/Kconfig b/drivers/mtd/awnand/spinand/Kconfig
new file mode 100644
index 000000000..8125d4196
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/Kconfig
@@ -0,0 +1,55 @@
+config AW_SPINAND_PHYSICAL_LAYER
+	bool
+	help
+	  Enable support for Allwinner's physical layer for spinand.
+	  It's a command physical layer, used by both AW-nand with nftl and
+	  MTD-nand with ubi.
+
+config AW_SPINAND_SECURE_STORAGE
+	bool
+	help
+	  Enable secure storage for Allwinner's spinand.
+
+	  If unsure, say no.
+
+config AW_SPINAND_PSTORE_MTD_PART
+	bool "create pstore mtd partition for aw ubi spinand"
+	depends on AW_MTD_SPINAND
+	select MTD_PSTORE
+	help
+	  Whether create pstore mtd partition, which is need by pstroe-blk.
+	  If you want linux kernel dump log to spinand when oops/panic, you
+	  should create pstreo mtd partition by this configure.
+
+	  If unsure, say no.
+
+config AW_SPINAND_ENABLE_PHY_CRC16
+	bool "check crc16 for each page on spinand physical layer"
+	depends on AW_SPINAND_PHYSICAL_LAYER
+	help
+	  It is experimental.
+	  To check crc16 for each page on spinand physical layer.
+
+	  If unsure, say no.
+
+config AW_SPINAND_SIMULATE_MULTIPLANE
+	bool "enable simulate multiplane"
+	depends on AW_SPINAND_PHYSICAL_LAYER
+	default y
+	help
+	  spinand do not support multiplane. In order to adapt to aw nand
+	  we simulate multiplane. If set, the common physical layer should
+	  merge two continuous physical block to 'super block' for logical
+	  layer.
+
+	  Merge pages in two adjacent blocks with the same page num to super
+	  page. Merge adjacent blocks to super block.
+
+	  *   phy-block0   phy-block1    = super block 0
+	  * |------------|------------|
+	  * | phy-page 0 | phy-page 0 |  = super page 0 on super block 0
+	  * | phy-page 1 | phy-page 1 |  = super page 1 on super block 0
+	  * |     ...    |     ...    |
+	  * |------------|------------|
+
+	  If unsure, say Y.
diff --git a/drivers/mtd/awnand/spinand/Makefile b/drivers/mtd/awnand/spinand/Makefile
new file mode 100644
index 000000000..9ca4ee034
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/Makefile
@@ -0,0 +1,15 @@
+#
+# Makefile for the SPINAND MTD
+#
+
+obj-$(CONFIG_AW_MTD_SPINAND) += aw-ubi-spinand.o
+
+aw-ubi-spinand-objs += sunxi-core.o sunxi-debug.o
+
+#obj-$(CONFIG_AW_NFTL_SPINAND) += aw-nftl-spinand.o
+
+#aw-nftl-spinand-objs += sunxi-common.o sunxi-nftl-core.o
+
+obj-$(CONFIG_AW_SPINAND_PHYSICAL_LAYER) += physic/
+
+obj-$(CONFIG_AW_SPINAND_SECURE_STORAGE) += secure-storage.o
diff --git a/drivers/mtd/awnand/spinand/physic/Makefile b/drivers/mtd/awnand/spinand/physic/Makefile
new file mode 100644
index 000000000..d63b1e6ad
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/Makefile
@@ -0,0 +1,4 @@
+
+obj-y += spinand-phy.o
+
+spinand-phy-objs += core.o ecc.o id.o ops.o bbt.o cache.o
diff --git a/drivers/mtd/awnand/spinand/physic/bbt.c b/drivers/mtd/awnand/spinand/physic/bbt.c
new file mode 100644
index 000000000..4fbfbd062
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/bbt.c
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* bad block table */
+#define pr_fmt(fmt) "sunxi-spinand-phy: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand.h>
+#include <linux/types.h>
+#include <linux/bitops.h>
+
+#include <linux/delay.h>
+
+#include "physic.h"
+
+static int aw_spinand_bbt_mark_badblock(struct aw_spinand_chip *chip,
+		unsigned int blknum, bool badblk)
+{
+	struct aw_spinand_bbt *bbt = chip->bbt;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+	unsigned int blkcnt = pinfo->DieCntPerChip * pinfo->BlkCntPerDie;
+
+	if (blknum > blkcnt)
+		return -EOVERFLOW;
+
+	if (badblk == true)
+		set_bit(blknum, bbt->bitmap);
+	set_bit(blknum, bbt->en_bitmap);
+	pr_debug("bbt: mark blk %u as %s\n", blknum, badblk ? "bad" : "good");
+	return 0;
+}
+
+static int aw_spinand_bbt_is_badblock(struct aw_spinand_chip *chip,
+		unsigned int blknum)
+{
+	struct aw_spinand_bbt *bbt = chip->bbt;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+	unsigned int blkcnt = pinfo->DieCntPerChip * pinfo->BlkCntPerDie;
+
+	if (blknum > blkcnt)
+		return -EOVERFLOW;
+
+	if (!test_bit(blknum, bbt->en_bitmap))
+		return NOT_MARKED;
+
+	pr_debug("bbt: blk %u is %s\n", blknum,
+			test_bit(blknum, bbt->bitmap) ? "bad" : "good");
+	if (test_bit(blknum, bbt->bitmap))
+		return BADBLOCK;
+	else
+		return NON_BADBLOCK;
+}
+
+struct aw_spinand_bbt aw_spinand_bbt = {
+	.mark_badblock = aw_spinand_bbt_mark_badblock,
+	.is_badblock = aw_spinand_bbt_is_badblock,
+};
+
+int aw_spinand_chip_bbt_init(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+	unsigned int blkcnt = pinfo->DieCntPerChip * pinfo->BlkCntPerDie;
+	unsigned long longcnt = BITS_TO_LONGS(blkcnt);
+	struct aw_spinand_bbt *bbt = &aw_spinand_bbt;
+
+	bbt->bitmap = kzalloc(longcnt * sizeof(long) * 2, GFP_KERNEL);
+	if (!bbt->bitmap)
+		return -ENOMEM;
+	bbt->en_bitmap = bbt->bitmap + longcnt;
+	chip->bbt = bbt;
+	return 0;
+}
+
+void aw_spinand_chip_bbt_exit(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_bbt *bbt = chip->bbt;
+
+	if (bbt && bbt->bitmap)
+		kfree(bbt->bitmap);
+	bbt->bitmap = NULL;
+}
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Commond physic layer for Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/physic/cache.c b/drivers/mtd/awnand/spinand/physic/cache.c
new file mode 100644
index 000000000..80cffa393
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/cache.c
@@ -0,0 +1,477 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand-phy: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand.h>
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+#include <linux/crc16.h>
+#endif
+
+#include "physic.h"
+
+#define INVALID_CACHE ((unsigned int)(-1))
+static void update_cache_info(struct aw_spinand_cache *cache,
+		struct aw_spinand_chip_request *req)
+{
+	if (req && (req->databuf || req->oobbuf)) {
+		cache->block = req->block;
+		cache->page = req->page;
+		cache->area = INVALID_CACHE_ALL_AREA;
+		if (req->databuf)
+			cache->area |= VALID_CACHE_DATA;
+		if (req->oobbuf)
+			cache->area |= VALID_CACHE_OOB;
+		pr_debug("update cache: phy blk %u page %u with%s%s\n",
+				req->block, req->page,
+				req->databuf ? " data" : "",
+				req->oobbuf ? " oob" : "");
+	} else {
+		cache->block = INVALID_CACHE;
+		cache->page = INVALID_CACHE;
+		cache->area = INVALID_CACHE_ALL_AREA;
+		pr_info("clear cache\n");
+	}
+}
+
+/* the request must bases on single physical page/block */
+static int aw_spinand_cache_copy_to_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	unsigned int len;
+	struct aw_spinand_cache *cache = chip->cache;
+
+	BUG_ON(req->pageoff + req->datalen > cache->data_maxlen);
+	BUG_ON(req->ooblen > cache->oob_maxlen);
+
+	memset(cache->databuf, 0xFF, cache->data_maxlen);
+	if (req->databuf && req->datalen) {
+		len = min(req->datalen, cache->data_maxlen - req->pageoff);
+		memcpy(cache->databuf + req->pageoff, req->databuf, len);
+	}
+
+	memset(cache->oobbuf, 0xFF, cache->oob_maxlen);
+	if (req->oobbuf && req->ooblen) {
+		int ret;
+		struct aw_spinand_ecc *ecc = chip->ecc;
+		struct aw_spinand_info *info = chip->info;
+		struct aw_spinand_phy_info *pinfo = info->phy_info;
+
+		len = min3(req->ooblen, cache->oob_maxlen,
+				(unsigned int)AW_OOB_SIZE_PER_PHY_PAGE);
+		ret = ecc->copy_to_oob(pinfo->EccProtectedType,
+				cache->oobbuf, req->oobbuf, len);
+		if (unlikely(ret))
+			return ret;
+	}
+
+	/* we must update cache information when update cache buffer */
+	update_cache_info(cache, req);
+	return 0;
+}
+
+/* the request must bases on single physical page/block */
+static int aw_spinand_cache_copy_from_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	unsigned int len;
+	struct aw_spinand_cache *cache = chip->cache;
+
+	WARN_ON(!cache->match_cache(chip, req));
+
+	if (req->datalen && req->databuf) {
+		len = min(req->datalen, cache->data_maxlen - req->pageoff);
+		memcpy(req->databuf, cache->databuf + req->pageoff, len);
+	}
+
+	if (req->oobbuf && req->ooblen) {
+		int ret;
+		struct aw_spinand_ecc *ecc = chip->ecc;
+		struct aw_spinand_info *info = chip->info;
+		struct aw_spinand_phy_info *pinfo = info->phy_info;
+		unsigned char *oobtmp;
+
+		len = min3(req->ooblen, cache->oob_maxlen,
+				(unsigned int)AW_OOB_SIZE_PER_PHY_PAGE);
+		ret = ecc->copy_from_oob(pinfo->EccProtectedType,
+				req->oobbuf, cache->oobbuf, len);
+		if (unlikely(ret))
+			return ret;
+
+		/*
+		 * the first byte of cache->oobbuf and req->oobbuf is
+		 * not 0xFF means bad block
+		 */
+		oobtmp = req->oobbuf;
+		if (oobtmp[0] != 0xFF || cache->oobbuf[0] != 0xFF)
+			oobtmp[0] = 0x00;
+	}
+
+	return 0;
+}
+
+static bool aw_spinand_cache_match_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_cache *cache = chip->cache;
+
+	if (cache->block == INVALID_CACHE || cache->page == INVALID_CACHE)
+		return false;
+	if (req->block != cache->block || req->page != cache->page)
+		return false;
+	if (req->databuf && !(cache->area & VALID_CACHE_DATA))
+		return false;
+	if (req->oobbuf && !(cache->area & VALID_CACHE_OOB))
+		return false;
+	return true;
+}
+
+static int aw_spinand_cahce_write_to_cache_do(struct aw_spinand_chip *chip,
+		void *buf, unsigned int len, unsigned int column,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_phy_info *pinfo = info->phy_info;
+	unsigned char txbuf[3];
+	struct spi_message msg;
+	struct spi_transfer t[2] = {};
+
+	spi_message_init(&msg);
+
+	if (chip->tx_bit == SPI_NBITS_QUAD)
+		txbuf[0] = column ? SPI_NAND_RANDOM_PP_X4 : SPI_NAND_PP_X4;
+	else
+		txbuf[0] = column ? SPI_NAND_RANDOM_PP : SPI_NAND_PP;
+	txbuf[1] = (column >> 8) & 0xFF;
+	txbuf[2] = column & 0xFF;
+	if ((pinfo->OperationOpt & SPINAND_TWO_PLANE_SELECT) &&
+				(req->block % 2 == 1))
+			txbuf[1] |= SPI_SELECT_ODDNUM_BLACK;
+	t[0].tx_buf = txbuf;
+	t[0].len = 3;
+	spi_message_add_tail(&t[0], &msg);
+
+	t[1].tx_buf = buf;
+	t[1].len = len;
+	t[1].tx_nbits = chip->tx_bit;
+	spi_message_add_tail(&t[1], &msg);
+	return spi_sync(chip->spi, &msg);
+}
+
+static int write_to_cache_half_page_twice(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	int ret;
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_cache *cache = chip->cache;
+	unsigned int column = 0, half_page_size;
+
+	half_page_size = info->phy_page_size(chip) >> 1;
+	/* the first half page */
+	ret = aw_spinand_cahce_write_to_cache_do(chip, cache->databuf,
+			half_page_size, column, req);
+	if (ret)
+		return ret;
+
+	/* the secend half page */
+	column += half_page_size;
+	return aw_spinand_cahce_write_to_cache_do(chip,
+			cache->databuf + half_page_size,
+			half_page_size + info->phy_oob_size(chip),
+			column, req);
+}
+
+static int write_to_cache_whole_page_once(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_cache *cache = chip->cache;
+
+	/* write the whole page */
+	return aw_spinand_cahce_write_to_cache_do(chip, cache->databuf,
+			info->phy_page_size(chip) + info->phy_oob_size(chip),
+			0, req);
+}
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+static int aw_spinand_chip_verify_crc(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_cache *cache = chip->cache;
+	__u16 crc = 0, crc_verify = 0;
+
+	if (!req->databuf || !req->oobbuf)
+		return 0;
+
+	crc_verify = ((unsigned char *)req->oobbuf)[AW_CRC16_OOB_OFFSET] << 8;
+	crc_verify += ((unsigned char *)req->oobbuf)[AW_CRC16_OOB_OFFSET + 1];
+
+	/* we should check cache->databuf rather than req->databuf */
+	crc = crc16(0, cache->databuf, cache->data_maxlen);
+
+	/*
+	 * If databuf are all 0xFF, the CRC16 value will be 0xA041.
+	 * In this case, we should return directly
+	 */
+	if (crc_verify == (__u16)0xFFFF && crc == (__u16)0xA041)
+		return 0;
+
+	/*
+	 * CRC16 is used for debug
+	 * So, we just print warning, do not do anything now
+	 */
+	if (crc != crc_verify) {
+		pr_warn("phy blk %u page %u crc16 check failed: want 0x%04x get 0x%04x\n",
+				req->block, req->page, crc_verify, crc);
+		aw_spinand_hexdump(KERN_WARNING, "oob: ", req->oobbuf,
+				req->ooblen);
+		aw_spinand_hexdump(KERN_WARNING, "data: ", cache->databuf,
+				cache->data_maxlen);
+		return -EINVAL;
+	}
+	pr_debug("phy blk %u page %u crc16 check pass\n", req->block,
+		   req->page);
+	return 0;
+}
+
+static int aw_spinand_chip_update_crc(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_info *info = chip->info;
+	__u16 crc = 0;
+
+	crc = crc16(0, req->databuf, info->phy_page_size(chip));
+
+	((char *)req->oobbuf)[AW_CRC16_OOB_OFFSET] = crc >> 8;
+	((char *)req->oobbuf)[AW_CRC16_OOB_OFFSET + 1] = crc & 0xFF;
+
+	pr_debug("record blk %u page %u crc16 0x%04x\n", req->block, req->page, crc);
+	return 0;
+}
+#endif
+
+/*
+ * 3 step:
+ *  a) copy data from req to cache->databuf/oobbuf
+ *  b) update cache->block/page
+ *  c) send write cache command to spinand
+ */
+static int aw_spinand_cache_write_to_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	int ret;
+	const char *manufacture;
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_cache *cache = chip->cache;
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	unsigned char oob[AW_OOB_SIZE_PER_PHY_PAGE] = {0xFF};
+#endif
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	if (!req->oobbuf) {
+		req->oobbuf = oob;
+		req->ooblen = AW_OOB_SIZE_PER_PHY_PAGE;
+	}
+	aw_spinand_chip_update_crc(chip, req);
+#endif
+
+	ret = cache->copy_to_cache(chip, req);
+	if (ret)
+		goto err;
+
+	manufacture = info->manufacture(chip);
+	if (!strcmp(manufacture, "Winbond"))
+		/*
+		 * Winbond spinand has a feature:
+		 * Data cache in spinand is divide to 2 parts to inprove speed.
+		 * We work with winbond engineers, however, still have no idea
+		 * why the spinand will get wrong data, always 2 bytes 0xFF on
+		 * the head of the 2rd part on cache.
+		 * To fix it, we are recommended to write twice, half of data
+		 * each time
+		 */
+		ret = write_to_cache_half_page_twice(chip, req);
+	else
+		ret = write_to_cache_whole_page_once(chip, req);
+	if (ret)
+		goto err;
+
+	/* we must update cache information when update cache buffer */
+	update_cache_info(cache, req);
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	if (req->oobbuf == oob) {
+		req->oobbuf = NULL;
+		req->ooblen = req->oobleft = 0;
+	}
+#endif
+	return 0;
+err:
+	/*
+	 * the cache buffer is invalid now as we do not know
+	 * what had done to cache buffer
+	 */
+	update_cache_info(cache, NULL);
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	if (req->oobbuf == oob) {
+		req->oobbuf = NULL;
+		req->ooblen = req->oobleft = 0;
+	}
+#endif
+	return ret;
+}
+
+/*
+ * 3 step:
+ *  a) copy data from req to cache->databuf/oobbuf
+ *  b) update cache->block/page
+ *  c) send write cache command to spinand
+ */
+static int aw_spinand_cache_read_from_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	unsigned int rbytes = 0;
+	void *rbuf = NULL;
+	int column = 0, ret;
+	unsigned char txbuf[5];
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_phy_info *pinfo = info->phy_info;
+	struct aw_spinand_cache *cache = chip->cache;
+	struct spi_message msg;
+	struct spi_transfer t[2] = {};
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	unsigned char oob[AW_OOB_SIZE_PER_PHY_PAGE] = {0xFF};
+#endif
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	if (req->databuf && !req->oobbuf) {
+		req->oobbuf = oob;
+		req->ooblen = AW_OOB_SIZE_PER_PHY_PAGE;
+	}
+#endif
+
+	if (req->datalen) {
+		rbuf = cache->databuf;
+		rbytes = cache->data_maxlen;
+		column = 0;
+	}
+
+	if (req->ooblen) {
+		rbytes += cache->oob_maxlen;
+		/* just oob without data */
+		if (!rbuf) {
+			rbuf = cache->oobbuf;
+			column = info->phy_page_size(chip);
+		}
+	}
+
+	spi_message_init(&msg);
+
+	if (chip->rx_bit == SPI_NBITS_QUAD)
+		txbuf[0] = SPI_NAND_READ_X4;
+	else if (chip->rx_bit == SPI_NBITS_DUAL)
+		txbuf[0] = SPI_NAND_READ_X2;
+	else
+		txbuf[0] = SPI_NAND_FAST_READ_X1;
+
+	if (pinfo->OperationOpt & SPINAND_ONEDUMMY_AFTER_RANDOMREAD) {
+		/* 1byte dummy */
+		txbuf[1] = 0x00;
+		txbuf[2] = (column >> 8) & 0xFF;
+		txbuf[3] = column & 0xFF;
+		txbuf[4] = 0x00;
+		t[0].len = 5;
+	} else {
+		txbuf[1] = (column >> 8) & 0xFF;
+		txbuf[2] = column & 0xFF;
+		txbuf[3] = 0x00;
+		t[0].len = 4;
+		if ((pinfo->OperationOpt & SPINAND_TWO_PLANE_SELECT) &&
+				(req->block % 2 == 1))
+			txbuf[1] |= SPI_SELECT_ODDNUM_BLACK;
+	}
+
+	t[0].tx_buf = txbuf;
+	spi_message_add_tail(&t[0], &msg);
+
+	/* to select the signal line count */
+	t[1].rx_nbits = chip->rx_bit;
+	t[1].rx_buf = rbuf;
+	t[1].len = rbytes;
+	spi_message_add_tail(&t[1], &msg);
+
+	ret = spi_sync(chip->spi, &msg);
+	if (ret) {
+		/*
+		 * the cache buffer is invalid now as we do not know
+		 * what spi system had done to cache buffer
+		 */
+		update_cache_info(cache, NULL);
+		return ret;
+	}
+
+	/* we must update cache information when update cache buffer */
+	update_cache_info(cache, req);
+
+	/* no need to update cache information as no change happened */
+	ret = cache->copy_from_cache(chip, req);
+	if (unlikely(ret))
+		return ret;
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_ENABLE_PHY_CRC16)
+	aw_spinand_chip_verify_crc(chip, req);
+	if (req->oobbuf == oob) {
+		req->oobbuf = NULL;
+		req->ooblen = req->oobleft = 0;
+	}
+#endif
+
+	return 0;
+}
+
+/* see what these funcions do on somewhere defined struct aw_spinand_cache */
+struct aw_spinand_cache aw_spinand_cache = {
+	.match_cache = aw_spinand_cache_match_cache,
+	.copy_to_cache = aw_spinand_cache_copy_to_cache,
+	.copy_from_cache = aw_spinand_cache_copy_from_cache,
+	.read_from_cache = aw_spinand_cache_read_from_cache,
+	.write_to_cache = aw_spinand_cache_write_to_cache,
+};
+
+int aw_spinand_chip_cache_init(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_cache *cache = &aw_spinand_cache;
+
+	/*
+	 * this cache is only used for single physical page,
+	 * no need to allocate super page for multiplane.
+	 * If multiplane enabled, the read/write operation will be
+	 * cut into 2 single page.
+	 */
+	cache->data_maxlen = info->phy_page_size(chip);
+	cache->oob_maxlen = info->phy_oob_size(chip);
+	cache->databuf = kzalloc(cache->data_maxlen + cache->oob_maxlen,
+			GFP_KERNEL);
+	if (!cache->databuf)
+		goto err;
+
+	cache->oobbuf = cache->databuf + cache->data_maxlen;
+	chip->cache = cache;
+	return 0;
+err:
+	pr_err("init cache failed\n");
+	return -ENOMEM;
+}
+
+void aw_spinand_chip_cache_exit(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_cache *cache = chip->cache;
+
+	kfree(cache->databuf);
+	cache->databuf = cache->oobbuf = NULL;
+	chip->cache = NULL;
+}
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Commond physic layer for Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/physic/core.c b/drivers/mtd/awnand/spinand/physic/core.c
new file mode 100644
index 000000000..cf57c8a91
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/core.c
@@ -0,0 +1,244 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand-phy: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+
+#include "physic.h"
+#include "../sunxi-spinand.h"
+
+/**
+ * aw_spinand_chip_update_cfg() - Update the configuration register
+ * @chip: spinand chip structure
+ *
+ * Return: 0 on success, a negative error code otherwise.
+ */
+static int aw_spinand_chip_update_cfg(struct aw_spinand_chip *chip)
+{
+	int ret;
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_info *info = chip->info;
+	u8 reg;
+
+	reg = 0;
+	ret = ops->set_block_lock(chip, reg);
+	if (ret)
+		goto err;
+	ret = ops->get_block_lock(chip, &reg);
+	if (ret)
+		goto err;
+	pr_info("block lock register: 0x%02x\n", reg);
+
+	ret = ops->get_otp(chip, &reg);
+	if (ret) {
+		pr_err("get otp register failed: %d\n", ret);
+		goto err;
+	}
+	/* FS35ND01G ECC_EN not on register 0xB0, but on 0x90 */
+	if (!strcmp(info->manufacture(chip), "Foresee")) {
+		ret = ops->write_reg(chip, SPI_NAND_SETSR, FORESEE_REG_ECC_CFG,
+				CFG_ECC_ENABLE);
+		if (ret) {
+			pr_err("enable ecc for foresee failed: %d\n", ret);
+			goto err;
+		}
+	} else {
+		reg |= CFG_ECC_ENABLE;
+	}
+	if (!strcmp(info->manufacture(chip), "Winbond"))
+		reg |= CFG_BUF_MODE;
+	if (info->operation_opt(chip) & SPINAND_QUAD_READ ||
+			info->operation_opt(chip) & SPINAND_QUAD_PROGRAM)
+		reg |= CFG_QUAD_ENABLE;
+	if (info->operation_opt(chip) & SPINAND_QUAD_NO_NEED_ENABLE)
+		reg &= ~CFG_QUAD_ENABLE;
+	ret = ops->set_otp(chip, reg);
+	if (ret) {
+		pr_err("set otp register failed: val %d, ret %d\n", reg, ret);
+		goto err;
+	}
+	ret = ops->get_otp(chip, &reg);
+	if (ret) {
+		pr_err("get updated otp register failed: %d\n", ret);
+		goto err;
+	}
+	pr_info("feature register: 0x%02x\n", reg);
+
+	return 0;
+err:
+	pr_err("update config register failed\n");
+	return ret;
+}
+
+static void aw_spinand_chip_clean(struct aw_spinand_chip *chip)
+{
+	aw_spinand_chip_cache_exit(chip);
+	aw_spinand_chip_bbt_exit(chip);
+}
+
+int aw_spinand_fill_phy_info(struct aw_spinand_chip *chip, void *data)
+{
+	struct aw_spinand *spinand = get_aw_spinand();
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_phy_info *pinfo = info->phy_info;
+	struct device_node *node = chip->spi->dev.of_node;
+	boot_spinand_para_t *boot_info = data;
+	int ret;
+	unsigned int max_hz;
+
+	ret = of_property_read_u32(node, "spi-max-frequency", &max_hz);
+	if (ret < 0)
+		pr_err("get spi-max-frequency from node of spi-nand failed\n");
+
+	ret = of_property_read_u32(node, "sample_mode",
+				&spinand->right_sample_mode);
+	if (ret) {
+		pr_err("Failed to get sample mode\n");
+		spinand->right_sample_mode = AW_SAMP_MODE_DL_DEFAULT;
+	}
+	ret = of_property_read_u32(node, "sample_delay",
+				&spinand->right_sample_delay);
+	if (ret) {
+		pr_err("Failed to get sample delay\n");
+		spinand->right_sample_delay = AW_SAMP_MODE_DL_DEFAULT;
+	}
+
+	/* nand information */
+	boot_info->ChipCnt = 1;
+	boot_info->ConnectMode = 1;
+	boot_info->BankCntPerChip = 1;
+	boot_info->DieCntPerChip = pinfo->DieCntPerChip;
+	boot_info->PlaneCntPerDie = 2;
+	boot_info->SectorCntPerPage = pinfo->SectCntPerPage;
+	boot_info->ChipConnectInfo = 1;
+	boot_info->PageCntPerPhyBlk = pinfo->PageCntPerBlk;
+	boot_info->BlkCntPerDie = pinfo->BlkCntPerDie;
+	boot_info->OperationOpt = pinfo->OperationOpt;
+	boot_info->FrequencePar = max_hz / 1000 / 1000;
+	boot_info->SpiMode = 0;
+	info->nandid(chip, boot_info->NandChipId, 8);
+	boot_info->pagewithbadflag = pinfo->BadBlockFlag;
+	boot_info->MultiPlaneBlockOffset = 1;
+	boot_info->MaxEraseTimes = pinfo->MaxEraseTimes;
+	/* there is no metter what max ecc bits is */
+	boot_info->MaxEccBits = 4;
+	boot_info->EccLimitBits = 4;
+
+	boot_info->sample_mode = spinand->right_sample_mode;
+	boot_info->sample_delay = spinand->right_sample_delay;
+
+	return 0;
+}
+
+static int aw_spinand_chip_init_last(struct aw_spinand_chip *chip)
+{
+	int ret;
+	struct aw_spinand_info *info = chip->info;
+	struct device_node *node = chip->spi->dev.of_node;
+	unsigned int val;
+
+	/* initialize from spinand information */
+	if (info->operation_opt(chip) & SPINAND_QUAD_PROGRAM)
+		chip->tx_bit = SPI_NBITS_QUAD;
+	else
+		chip->tx_bit = SPI_NBITS_SINGLE;
+
+	if (info->operation_opt(chip) & SPINAND_QUAD_READ)
+		chip->rx_bit = SPI_NBITS_QUAD;
+	else if (info->operation_opt(chip) & SPINAND_DUAL_READ)
+		chip->rx_bit = SPI_NBITS_DUAL;
+	else
+		chip->rx_bit = SPI_NBITS_SINGLE;
+
+	/* re-initialize from device tree */
+	ret = of_property_read_u32(node, "spi-rx-bus-width", &val);
+	if (!ret && val < chip->rx_bit) {
+		pr_info("%s reset rx bit width to %u\n",
+				info->model(chip), val);
+		chip->rx_bit = val;
+	}
+
+	ret = of_property_read_u32(node, "spi-tx-bus-width", &val);
+	if (!ret && val < chip->tx_bit) {
+		pr_info("%s reset tx bit width to %u\n",
+				info->model(chip), val);
+		chip->tx_bit = val;
+	}
+
+	/* update spinand register */
+	ret = aw_spinand_chip_update_cfg(chip);
+	if (ret)
+		return ret;
+
+	/* do read/write cache init */
+	ret = aw_spinand_chip_cache_init(chip);
+	if (ret)
+		return ret;
+
+	/* do bad block table init */
+	ret = aw_spinand_chip_bbt_init(chip);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int aw_spinand_chip_preinit(struct spi_device *spi,
+		struct aw_spinand_chip *chip)
+{
+	int ret;
+
+	chip->spi = spi;
+
+	ret = aw_spinand_chip_ecc_init(chip);
+	if (unlikely(ret))
+		return ret;
+
+	ret = aw_spinand_chip_ops_init(chip);
+	if (unlikely(ret))
+		return ret;
+
+	return 0;
+}
+
+
+int aw_spinand_chip_init(struct spi_device *spi, struct aw_spinand_chip *chip)
+{
+	int ret;
+
+	pr_info("AW SPINand Phy Layer Version: %x.%x %x\n",
+			AW_SPINAND_PHY_VER_MAIN, AW_SPINAND_PHY_VER_SUB,
+			AW_SPINAND_PHY_VER_DATE);
+
+	ret = aw_spinand_chip_preinit(spi, chip);
+	if (unlikely(ret))
+		return ret;
+
+	ret = aw_spinand_chip_detect(chip);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_init_last(chip);
+	if (ret)
+		goto err;
+
+	pr_info("sunxi physic nand init end\n");
+	return 0;
+err:
+	aw_spinand_chip_clean(chip);
+	return ret;
+}
+EXPORT_SYMBOL(aw_spinand_chip_init);
+
+void aw_spinand_chip_exit(struct aw_spinand_chip *chip)
+{
+	aw_spinand_chip_clean(chip);
+}
+EXPORT_SYMBOL(aw_spinand_chip_exit);
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Commond physic layer for Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/physic/ecc.c b/drivers/mtd/awnand/spinand/physic/ecc.c
new file mode 100644
index 000000000..59ea9da70
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/ecc.c
@@ -0,0 +1,198 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand-phy: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand.h>
+
+#include "physic.h"
+
+static int __copy_to_oob(unsigned char *to, unsigned char *from,
+		unsigned int cnt, unsigned int spare_size,
+		unsigned int offset, unsigned int datalen)
+{
+	unsigned int i;
+
+	/* bad block mark byte */
+	to[0] = from[0];
+
+	for (i = 0; cnt > 0; i++) {
+		/*
+		 * only the last time, num maybe not datalen, that's why
+		 * memcpy offset add datalen rather than num
+		 */
+		unsigned int num = min(cnt, datalen);
+
+		memcpy(to + offset + spare_size * i, from + datalen * i, num);
+		cnt -= num;
+	}
+	return 0;
+}
+
+static int aw_spinand_ecc_copy_to_oob(enum ecc_oob_protected type,
+		unsigned char *to, unsigned char *from, unsigned int len)
+{
+	switch (type) {
+	case SIZE16_OFF0_LEN16:
+		return __copy_to_oob(to, from, len, 16, 0, 16);
+	case SIZE16_OFF4_LEN12:
+		return __copy_to_oob(to, from, len, 16, 4, 12);
+	case SIZE16_OFF4_LEN4_OFF8:
+		return __copy_to_oob(to, from, len, 16, 4, 4);
+	case SIZE16_OFF4_LEN8_OFF4:
+		return __copy_to_oob(to, from, len, 16, 4, 8);
+	case SIZE16_OFF32_LEN16:
+		return __copy_to_oob(to, from, len, 16, 32, 16);
+	case SIZE16_OFF8_LEN16:
+		return __copy_to_oob(to, from, len, 16, 8, 16);
+	default:
+		return -EINVAL;
+	}
+}
+
+static int __copy_from_oob(unsigned char *to, unsigned char *from,
+		unsigned int cnt, unsigned int spare_size,
+		unsigned int offset, unsigned int datalen)
+{
+	unsigned int i;
+
+	for (i = 0; cnt > 0; i++) {
+		/*
+		 * only the last time, num maybe not datalen, that's why
+		 * memcpy offset add datalen rather than num
+		 */
+		unsigned int num = min(cnt, datalen);
+
+		memcpy(to + datalen * i, from + offset + spare_size * i, num);
+		cnt -= num;
+	}
+	return 0;
+}
+
+static int aw_spinand_ecc_copy_from_oob(enum ecc_oob_protected type,
+		unsigned char *to, unsigned char *from, unsigned int len)
+{
+	switch (type) {
+	case SIZE16_OFF0_LEN16:
+		return __copy_from_oob(to, from, len, 16, 0, 16);
+	case SIZE16_OFF4_LEN12:
+		return __copy_from_oob(to, from, len, 16, 4, 12);
+	case SIZE16_OFF4_LEN4_OFF8:
+		return __copy_from_oob(to, from, len, 16, 4, 4);
+	case SIZE16_OFF4_LEN8_OFF4:
+		return __copy_from_oob(to, from, len, 16, 4, 8);
+	case SIZE16_OFF32_LEN16:
+		return __copy_from_oob(to, from, len, 16, 32, 16);
+	case SIZE16_OFF8_LEN16:
+		return __copy_from_oob(to, from, len, 16, 8, 16);
+	default:
+		return -EINVAL;
+	}
+}
+
+static inline int general_check_ecc(unsigned char ecc,
+		unsigned char limit_from, unsigned char limit_to,
+		unsigned char err_from, unsigned char err_to)
+{
+	if (ecc < limit_from) {
+		return ECC_GOOD;
+	} else if (ecc >= limit_from && ecc <= limit_to) {
+		pr_debug("ecc limit 0x%x\n", ecc);
+		return ECC_LIMIT;
+	} else if (ecc >= err_from && ecc <= err_to) {
+		pr_err("ecc error 0x%x\n", ecc);
+		return ECC_ERR;
+	}
+
+	pr_err("unknown ecc value 0x%x\n", ecc);
+	return ECC_ERR;
+}
+
+static int check_ecc_bit2_limit1_err2_limit3(unsigned char ecc)
+{
+	if (ecc == 0) {
+		return ECC_GOOD;
+	} else if (ecc == 1 || ecc == 3) {
+		pr_debug("ecc limit 0x%x\n", ecc);
+		return ECC_LIMIT;
+	}
+
+	pr_err("ecc error 0x%x\n", ecc);
+	return ECC_ERR;
+}
+
+static int check_ecc_bit3_limit5_err2(unsigned char ecc)
+{
+	if (ecc <= 1) {
+		return ECC_GOOD;
+	} else if (ecc == 3 || ecc == 5) {
+		pr_debug("ecc limit 0x%x\n", ecc);
+		return ECC_LIMIT;
+	}
+
+	pr_err("ecc error 0x%x\n", ecc);
+	return ECC_ERR;
+}
+
+static int check_ecc_bit4_limit5_7_err8_limit12(unsigned char ecc)
+{
+	if (ecc <= 4) {
+		return ECC_GOOD;
+	} else if ((ecc >= 5 && ecc <= 7) || (ecc >= 12)) {
+		pr_debug("ecc limit 0x%x\n", ecc);
+		return ECC_LIMIT;
+	}
+
+	pr_err("ecc err 0x%x\n", ecc);
+	return ECC_ERR;
+}
+
+static int aw_spinand_ecc_check_ecc(enum ecc_limit_err type, u8 status)
+{
+	unsigned char ecc;
+
+	switch (type) {
+	case BIT3_LIMIT2_TO_6_ERR7:
+		ecc = status & 0x07;
+		return general_check_ecc(ecc, 2, 6, 7, 7);
+	case BIT2_LIMIT1_ERR2:
+		ecc = status & 0x03;
+		return general_check_ecc(ecc, 1, 1, 2, 2);
+	case BIT2_LIMIT1_ERR2_LIMIT3:
+		ecc = status & 0x03;
+		return check_ecc_bit2_limit1_err2_limit3(ecc);
+	case BIT4_LIMIT3_TO_4_ERR15:
+		ecc = status & 0x0f;
+		return general_check_ecc(ecc, 3, 4, 15, 15);
+	case BIT3_LIMIT3_TO_4_ERR7:
+		ecc = status & 0x07;
+		return general_check_ecc(ecc, 3, 4, 7, 7);
+	case BIT3_LIMIT5_ERR2:
+		ecc = status & 0x07;
+		return check_ecc_bit3_limit5_err2(ecc);
+	case BIT4_LIMIT5_TO_7_ERR8_LIMIT_12:
+		ecc = status & 0x0f;
+		return check_ecc_bit4_limit5_7_err8_limit12(ecc);
+	case BIT4_LIMIT5_TO_8_ERR9_TO_15:
+		ecc = status & 0x0f;
+		return general_check_ecc(ecc, 5, 8, 9, 15);
+	default:
+		return -EINVAL;
+	}
+}
+
+static struct aw_spinand_ecc aw_spinand_ecc = {
+	.copy_to_oob = aw_spinand_ecc_copy_to_oob,
+	.copy_from_oob = aw_spinand_ecc_copy_from_oob,
+	.check_ecc = aw_spinand_ecc_check_ecc,
+};
+
+int aw_spinand_chip_ecc_init(struct aw_spinand_chip *chip)
+{
+	chip->ecc = &aw_spinand_ecc;
+	return 0;
+}
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Commond physic layer for Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/physic/id.c b/drivers/mtd/awnand/spinand/physic/id.c
new file mode 100644
index 000000000..a3ab4894b
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/id.c
@@ -0,0 +1,998 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand-phy: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand.h>
+#include <linux/of.h>
+
+#include "physic.h"
+
+#define KB (1024)
+#define MB (KB * 1024)
+#define to_kb(size) (size / KB)
+#define to_mb(size) (size / MB)
+
+/* manufacture num */
+#define MICRON_MANUFACTURE	0x2c
+#define GD_MANUFACTURE		0xc8
+#define ATO_MANUFACTURE		0x9b
+#define WINBOND_MANUFACTURE	0xef
+#define MXIC_MANUFACTURE	0xc2
+#define TOSHIBA_MANUFACTURE	0x98
+#define ETRON_MANUFACTURE	0xd5
+#define XTXTECH_MANUFACTURE	0x0b
+#define DSTECH_MANUFACTURE	0xe5
+#define FORESEE_MANUFACTURE	0xcd
+#define ZETTA_MANUFACTURE	0xba
+#define FM_MANUFACTURE		0xa1
+
+struct spinand_manufacture m;
+
+struct aw_spinand_phy_info gigadevice[] = {
+	{
+		.Model		= "GD5F1GQ4UCYIG",
+		.NandID		= {0xc8, 0xb1, 0x48, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ | SPINAND_ONEDUMMY_AFTER_RANDOMREAD,
+		.MaxEraseTimes  = 50000,
+		.EccType	= BIT3_LIMIT2_TO_6_ERR7,
+		.EccProtectedType = SIZE16_OFF0_LEN16,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		.Model		= "GD5F1GQ4UBYIG",
+		.NandID		= {0xc8, 0xd1, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccFlag	= HAS_EXT_ECC_SE01,
+		.EccType	= BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+		.EccProtectedType = SIZE16_OFF4_LEN8_OFF4,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		/* GD5F2GQ4UB9IG did not check yet */
+		.Model		= "GD5F2GQ4UB9IG",
+		.NandID		= {0xc8, 0xd2, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 2048,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccFlag	= HAS_EXT_ECC_SE01,
+		.EccType	= BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+		.EccProtectedType = SIZE16_OFF4_LEN12,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		.Model		= "F50L1G41LB(2M)",
+		.NandID		= {0xc8, 0x01, 0x7f, 0x7f, 0x7f, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ | SPINAND_QUAD_NO_NEED_ENABLE,
+		.MaxEraseTimes  = 65000,
+		.EccType	= BIT2_LIMIT1_ERR2,
+		.EccProtectedType = SIZE16_OFF4_LEN4_OFF8,
+		.BadBlockFlag	= BAD_BLK_FLAG_FIRST_2_PAGE,
+	},
+	{
+		.Model		= "GD5F1GQ5UEYIG",
+		.NandID		= {0xc8, 0x51, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccFlag	= HAS_EXT_ECC_SE01,
+		.EccType	= BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+		.EccProtectedType = SIZE16_OFF4_LEN12,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		.Model		= "GD5F2GQ5UEYIGR",
+		.NandID		= {0xc8, 0x52, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 2048,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccFlag	= HAS_EXT_ECC_SE01,
+		.EccType	= BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+		.EccProtectedType = SIZE16_OFF4_LEN12,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info micron[] = {
+	{
+		.Model		= "MT29F1G01ABAGDWB",
+		.NandID		= {0x2c, 0x14, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ | SPINAND_QUAD_NO_NEED_ENABLE,
+		.MaxEraseTimes  = 65000,
+		.EccType	= BIT3_LIMIT5_ERR2,
+		.EccProtectedType = SIZE16_OFF32_LEN16,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		.Model		= "MT29F2G01ABAGDWB",
+		.NandID		= {0x2c, 0x24, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 2048,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ | SPINAND_QUAD_NO_NEED_ENABLE |
+			SPINAND_TWO_PLANE_SELECT,
+		.MaxEraseTimes  = 65000,
+		.EccType	= BIT3_LIMIT5_ERR2,
+		.EccProtectedType = SIZE16_OFF32_LEN16,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info xtx[] = {
+	{
+		/* XTX26G02A */
+		.Model		= "XTX26G02A",
+		.NandID		= {0x0B, 0xE2, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 2048,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.ecc_status_shift = ECC_STATUS_SHIFT_2,
+		.EccType	= BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+		.EccProtectedType = SIZE16_OFF8_LEN16,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		/* XTX26G02A */
+		.Model		= "XTX26G01A",
+		.NandID		= {0x0B, 0xE1, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.ecc_status_shift = ECC_STATUS_SHIFT_2,
+		.EccType	= BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+		.EccProtectedType = SIZE16_OFF8_LEN16,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		/* XT26G01C */
+		.Model		= "XT26G01C",
+		.NandID		= {0x0B, 0x11, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.ecc_status_shift = ECC_STATUS_SHIFT_4,
+		.EccType	= BIT4_LIMIT5_TO_8_ERR9_TO_15,
+		.EccProtectedType = SIZE16_OFF0_LEN16,
+		.BadBlockFlag	= BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info fm[] = {
+	{
+		/* only rw stress test */
+		.Model		= "FM25S01",
+		.NandID		= {0xa1, 0xa1, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ | SPINAND_QUAD_NO_NEED_ENABLE,
+		.MaxEraseTimes  = 65000,
+		.EccType	= BIT2_LIMIT1_ERR2,
+		.EccProtectedType = SIZE16_OFF0_LEN16,
+		.BadBlockFlag = BAD_BLK_FLAG_FIRST_2_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info etron[] = {
+
+};
+
+struct aw_spinand_phy_info toshiba[] = {
+
+};
+
+struct aw_spinand_phy_info ato[] = {
+
+};
+
+struct aw_spinand_phy_info mxic[] = {
+	{
+		.Model		= "MX35LF1GE4AB",
+		.NandID		= {0xc2, 0x12, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 65000,
+		.EccFlag	= HAS_EXT_ECC_STATUS,
+		.EccType	= BIT4_LIMIT3_TO_4_ERR15,
+		/**
+		 * MX35LF1GE4AB should use SIZE16_OFF4_LEN12, however, in order
+		 * to compatibility with versions already sent to customers,
+		 * which do not use general physical layout, we used
+		 * SIZE16_OFF4_LEN4_OFF8 instead.
+		 */
+		.EccProtectedType = SIZE16_OFF4_LEN4_OFF8,
+		.BadBlockFlag = BAD_BLK_FLAG_FIRST_2_PAGE,
+	},
+	{
+		.Model		= "MX35LF2GE4AD",
+		.NandID		= {0xc2, 0x26, 0x03, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 2048,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 65000,
+		.EccFlag	= HAS_EXT_ECC_STATUS,
+		.EccType	= BIT4_LIMIT5_TO_8_ERR9_TO_15,
+		.EccProtectedType = SIZE16_OFF4_LEN4_OFF8,
+		.BadBlockFlag = BAD_BLK_FLAG_FIRST_2_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info winbond[] = {
+	{
+		.Model		= "W25N01GVZEIG",
+		.NandID		= {0xef, 0xaa, 0x21, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 65000,
+		.EccType	= BIT2_LIMIT1_ERR2,
+		.EccProtectedType = SIZE16_OFF4_LEN4_OFF8,
+		.BadBlockFlag = BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info dosilicon[] = {
+	{
+		.Model		= "DS35X1GAXXX",
+		.NandID		= {0xe5, 0x71, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 65000,
+		.EccType	= BIT2_LIMIT1_ERR2,
+		.EccProtectedType = SIZE16_OFF4_LEN4_OFF8,
+		.BadBlockFlag = BAD_BLK_FLAG_FIRST_2_PAGE,
+	},
+};
+
+struct aw_spinand_phy_info foresee[] = {
+	{
+		.Model		= "FS35ND01G-S1F1QWFI000",
+		.NandID		= {0xcd, 0xb1, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccType	= BIT3_LIMIT3_TO_4_ERR7,
+		.EccProtectedType = SIZE16_OFF0_LEN16,
+		.BadBlockFlag = BAD_BLK_FLAG_FRIST_1_PAGE,
+	},
+	{
+		.Model		= "FS35ND01G-S1Y2QWFI000",
+		.NandID		= {0xcd, 0xea, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccType	= BIT2_LIMIT1_ERR2,
+		.EccProtectedType = SIZE16_OFF0_LEN16,
+		.BadBlockFlag = BAD_BLK_FLAG_FRIST_1_PAGE,
+	}
+};
+
+struct aw_spinand_phy_info zetta[] = {
+	{
+		.Model		= "ZD35Q1GAIB",
+		.NandID		= {0xba, 0x71, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff},
+		.DieCntPerChip  = 1,
+		.SectCntPerPage = 4,
+		.PageCntPerBlk  = 64,
+		.BlkCntPerDie	= 1024,
+		.OobSizePerPage = 64,
+		.OperationOpt	= SPINAND_QUAD_READ | SPINAND_QUAD_PROGRAM |
+			SPINAND_DUAL_READ,
+		.MaxEraseTimes  = 50000,
+		.EccType	= BIT2_LIMIT1_ERR2,
+		.EccProtectedType = SIZE16_OFF4_LEN4_OFF8,
+		.BadBlockFlag = BAD_BLK_FLAG_FIRST_2_PAGE,
+	},
+};
+
+static const char *aw_spinand_info_model(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->Model;
+}
+
+static void aw_spinand_info_nandid(struct aw_spinand_chip *chip,
+		unsigned char *id, int cnt)
+{
+	int i;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	cnt = min(cnt, MAX_ID_LEN);
+	for (i = 0; i < cnt; i++)
+		id[i] = pinfo->NandID[i];
+}
+
+static unsigned int aw_spinand_info_sector_size(struct aw_spinand_chip *chip)
+{
+	return 1 << SECTOR_SHIFT;
+}
+
+static unsigned int aw_spinand_info_phy_page_size(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->SectCntPerPage * aw_spinand_info_sector_size(chip);
+}
+
+static unsigned int aw_spinand_info_page_size(struct aw_spinand_chip *chip)
+{
+#if IS_ENABLED(CONFIG_AW_SPINAND_SIMULATE_MULTIPLANE)
+	return aw_spinand_info_phy_page_size(chip) * 2;
+#else
+	return aw_spinand_info_phy_page_size(chip);
+#endif
+}
+
+static unsigned int aw_spinand_info_phy_block_size(
+		struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->PageCntPerBlk * aw_spinand_info_phy_page_size(chip);
+}
+
+static unsigned int aw_spinand_info_block_size(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->PageCntPerBlk * aw_spinand_info_page_size(chip);
+}
+
+static unsigned int aw_spinand_info_phy_oob_size(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->OobSizePerPage;
+}
+
+static unsigned int aw_spinand_info_oob_size(struct aw_spinand_chip *chip)
+{
+#if IS_ENABLED(CONFIG_AW_SPINAND_SIMULATE_MULTIPLANE)
+	return aw_spinand_info_phy_oob_size(chip) * 2;
+#else
+	return aw_spinand_info_phy_oob_size(chip);
+#endif
+}
+
+static unsigned int aw_spinand_info_die_cnt(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->DieCntPerChip;
+}
+
+static unsigned int aw_spinand_info_total_size(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->DieCntPerChip * pinfo->BlkCntPerDie *
+		aw_spinand_info_phy_block_size(chip);
+}
+
+static int aw_spinand_info_operation_opt(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->OperationOpt;
+}
+
+static int aw_spinand_info_max_erase_times(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return pinfo->MaxEraseTimes;
+}
+
+struct spinand_manufacture {
+	unsigned char id;
+	const char *name;
+	struct aw_spinand_phy_info *info;
+	unsigned int cnt;
+};
+
+#define SPINAND_FACTORY_INFO(_id, _name, _info)			\
+	{							\
+		.id = _id,					\
+		.name = _name,					\
+		.info = _info,					\
+		.cnt = ARRAY_SIZE(_info),			\
+	}
+static struct spinand_manufacture spinand_factory[] = {
+	SPINAND_FACTORY_INFO(MICRON_MANUFACTURE, "Micron", micron),
+	SPINAND_FACTORY_INFO(GD_MANUFACTURE, "GD", gigadevice),
+	SPINAND_FACTORY_INFO(ATO_MANUFACTURE, "ATO", ato),
+	SPINAND_FACTORY_INFO(WINBOND_MANUFACTURE, "Winbond", winbond),
+	SPINAND_FACTORY_INFO(MXIC_MANUFACTURE, "Mxic", mxic),
+	SPINAND_FACTORY_INFO(TOSHIBA_MANUFACTURE, "Toshiba", toshiba),
+	SPINAND_FACTORY_INFO(ETRON_MANUFACTURE, "Etron", etron),
+	SPINAND_FACTORY_INFO(XTXTECH_MANUFACTURE, "XTX", xtx),
+	SPINAND_FACTORY_INFO(DSTECH_MANUFACTURE, "Dosilicon", dosilicon),
+	SPINAND_FACTORY_INFO(FORESEE_MANUFACTURE, "Foresee", foresee),
+	SPINAND_FACTORY_INFO(ZETTA_MANUFACTURE, "Zetta", zetta),
+	SPINAND_FACTORY_INFO(FM_MANUFACTURE, "FM", fm),
+};
+
+
+static int spinand_get_chip_munufacture(struct aw_spinand_chip *chip, const char **m)
+{
+	struct aw_spinand_phy_info *info = chip->info->phy_info;
+
+	switch (info->NandID[0]) {
+	case MICRON_MANUFACTURE:
+		*m = "Micron";
+	break;
+	case GD_MANUFACTURE:
+		*m = "GD";
+	break;
+	case ATO_MANUFACTURE:
+		*m = "ATO";
+	break;
+	case WINBOND_MANUFACTURE:
+		*m = "Winbond";
+	break;
+	case MXIC_MANUFACTURE:
+		*m = "Mxic";
+	break;
+	case TOSHIBA_MANUFACTURE:
+		*m = "Toshiba";
+	break;
+	case ETRON_MANUFACTURE:
+		*m = "Etron";
+	break;
+	case XTXTECH_MANUFACTURE:
+		*m = "XTX";
+	break;
+	case DSTECH_MANUFACTURE:
+		*m = "Dosilicon";
+	break;
+	case FORESEE_MANUFACTURE:
+		*m = "Foresee";
+	break;
+	case ZETTA_MANUFACTURE:
+		*m = "Zetta";
+	break;
+	default:
+		*m = NULL;
+	break;
+	}
+
+	if (*m == NULL)
+		return false;
+	else
+		return true;
+
+}
+static const char *aw_spinand_info_manufacture(struct aw_spinand_chip *chip)
+{
+	int i, j;
+	struct spinand_manufacture *m;
+	struct aw_spinand_phy_info *pinfo;
+	const char *m_name = NULL;
+	int ret = 0;
+
+	for (i = 0; i < ARRAY_SIZE(spinand_factory); i++) {
+		m = &spinand_factory[i];
+		pinfo = chip->info->phy_info;
+		for (j = 0; j < m->cnt; j++)
+			if (pinfo == &m->info[j])
+				return m->name;
+	}
+
+	/*for compatible fdt support spi-nand*/
+	ret = spinand_get_chip_munufacture(chip, &m_name);
+	if (ret < 0)
+		return NULL;
+	else
+		return m_name;
+}
+
+static struct spinand_manufacture *spinand_detect_munufacture(unsigned char id)
+{
+	int index;
+	struct spinand_manufacture *m;
+
+	for (index = 0; index < ARRAY_SIZE(spinand_factory); index++) {
+		m = &spinand_factory[index];
+		if (m->id == id) {
+			pr_info("detect munufacture from id table: %s\n", m->name);
+			return m;
+		}
+	}
+
+	pr_err("not detect any munufacture from id table\n");
+	return NULL;
+}
+
+static struct aw_spinand_phy_info *spinand_match_id(
+		struct spinand_manufacture *m,
+		unsigned char *id)
+{
+	int i, j, match_max = 1, match_index = 0;
+	struct aw_spinand_phy_info *pinfo;
+
+	for (i = 0; i < m->cnt; i++) {
+		int match = 1;
+
+		pinfo = &m->info[i];
+		for (j = 1; j < MAX_ID_LEN; j++) {
+			/* 0xFF matching all ID value */
+			if (pinfo->NandID[j] != id[j] &&
+					pinfo->NandID[j] != 0xFF)
+				break;
+
+			if (pinfo->NandID[j] != 0xFF)
+				match++;
+		}
+
+		if (match > match_max) {
+			match_max = match;
+			match_index = i;
+		}
+	}
+
+	if (match_max > 1)
+		return &m->info[match_index];
+	return NULL;
+}
+
+struct aw_spinand_phy_info *spinand_get_phy_info_from_fdt(struct aw_spinand_chip *chip)
+{
+	static struct aw_spinand_phy_info info;
+	static int had_get;
+	int ret = 0;
+	const char *bad_blk_mark_pos = NULL;
+	const char *quad_read_not_need_enable = NULL;
+	const char *read_seq_need_onedummy = NULL;
+	int len = 0;
+	u32 id = 0xffffffff;
+	struct device_node *node = chip->spi->dev.of_node;
+	u32 rx_bus_width = 0;
+	u32 tx_bus_width = 0;
+
+
+	if (had_get == true)
+		return &info;
+
+#define BAD_BLK_MARK_POS1 "first_1_page"
+#define BAD_BLK_MARK_POS2 "first_2_page"
+#define BAD_BLK_MARK_POS3 "last_1_page"
+#define BAD_BLK_MARK_POS4 "last_2_page"
+	memset(&info, 0, sizeof(struct aw_spinand_phy_info));
+
+	ret = of_property_read_string(node, "model", &(info.Model));
+	if (ret < 0) {
+		pr_err("get spi-nand Model from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "id-0", &id);
+	if (ret < 0) {
+		pr_err("get spi-nand id Low 4 Byte from fdt fail\n");
+		goto err;
+	}
+	len = sizeof(id);
+	memmove(info.NandID, &id, min(MAX_ID_LEN, len));
+
+	id = 0xffffffff;
+	ret = of_property_read_u32(node, "id-1", &id);
+	if (ret < 0) {
+		pr_info("can't get spi-nand id high 4 Byte from fdt, may be not need\n");
+	}
+	memmove(info.NandID + min(MAX_ID_LEN, len), &id, max(MAX_ID_LEN, len) - min(MAX_ID_LEN, len));
+
+	ret = of_property_read_u32(node, "die_cnt_per_chip", &(info.DieCntPerChip));
+	if (ret < 0) {
+		pr_err("get spi-nand DieCntPerChip from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "blk_cnt_per_die", &(info.BlkCntPerDie));
+	if (ret < 0) {
+		pr_err("get spi-nand BlkCntPerDie from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "page_cnt_per_blk", &(info.PageCntPerBlk));
+	if (ret < 0) {
+		pr_err("get spi-nand PageCntPerBlk from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "sect_cnt_per_page", &(info.SectCntPerPage));
+	if (ret < 0) {
+		pr_err("get spi-nand SectCntPerPage from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "oob_size_per_page", &(info.OobSizePerPage));
+	if (ret < 0) {
+		pr_err("get spi-nand OobSizePerPage from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_string(node, "bad_block_mark_pos", &bad_blk_mark_pos);
+	if (ret < 0 || NULL == bad_blk_mark_pos) {
+		pr_err("get spi-nand BadBlockFlag from fdt fail\n");
+		goto err;
+	} else {
+		if (!memcmp(bad_blk_mark_pos, BAD_BLK_MARK_POS1, strlen(BAD_BLK_MARK_POS1)))
+			info.BadBlockFlag = BAD_BLK_FLAG_FRIST_1_PAGE;
+		else if (!memcmp(bad_blk_mark_pos, BAD_BLK_MARK_POS2, strlen(BAD_BLK_MARK_POS2)))
+			info.BadBlockFlag = BAD_BLK_FLAG_FRIST_1_PAGE;
+		else if (!memcmp(bad_blk_mark_pos, BAD_BLK_MARK_POS3, strlen(BAD_BLK_MARK_POS3)))
+			info.BadBlockFlag = BAD_BLK_FLAG_LAST_1_PAGE;
+		else if (!memcmp(bad_blk_mark_pos, BAD_BLK_MARK_POS4, strlen(BAD_BLK_MARK_POS4)))
+			info.BadBlockFlag = BAD_BLK_FLAG_LAST_2_PAGE;
+		else {
+			pr_err("get spi-nand BadBlockFlag pattern is not right\n");
+			goto err;
+		}
+	}
+
+	ret = of_property_read_s32(node, "max_erase_times", &(info.MaxEraseTimes));
+	if (ret < 0) {
+		pr_err("get spi-nand MaxEraseTimes from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "ecc_type", &(info.EccType));
+	if (ret < 0) {
+		pr_err("get spi-nand EccFlag from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "ecc_protected_type", &(info.EccProtectedType));
+	if (ret < 0) {
+		pr_err("get spi-nand ecc_protected_type from fdt fail\n");
+		goto err;
+	}
+
+	ret = of_property_read_u32(node, "spi-rx-bus-width", &rx_bus_width);
+	if (ret < 0) {
+		pr_err("get spi-nand spi-rx-bus-width from fdt fail\n");
+		goto err;
+	} else {
+		switch (rx_bus_width) {
+		case SPI_NBITS_DUAL:
+			info.OperationOpt |= SPINAND_DUAL_READ;
+		break;
+		case SPI_NBITS_QUAD:
+			info.OperationOpt |= SPINAND_QUAD_READ;
+		break;
+		default:
+			info.OperationOpt |= 0;
+		break;
+		}
+	}
+
+	ret = of_property_read_u32(node, "spi-tx-bus-width", &tx_bus_width);
+	if (ret < 0) {
+		pr_err("get spi-nand spi-tx-bus-width from fdt fail\n");
+		goto err;
+	} else {
+		switch (tx_bus_width) {
+		case SPI_NBITS_QUAD:
+			info.OperationOpt |= SPINAND_QUAD_PROGRAM;
+		break;
+		default:
+			info.OperationOpt |= 0;
+		break;
+		}
+	}
+
+	ret = of_property_read_string(node, "read_from_cache_x4_not_need_enable",
+			&quad_read_not_need_enable);
+	pr_info("%d quad_read_not_need_enable:%s\n", __LINE__, quad_read_not_need_enable);
+	if (ret < 0 || NULL == quad_read_not_need_enable) {
+		pr_info("can't get spi-nand read_from_cache_x4_need_enable or it is null,"
+				" maybe not need enable quad read before read from cache x4\n");
+	} else {
+		if (!memcmp(quad_read_not_need_enable, "yes", strlen("yes")))
+			info.OperationOpt |= SPINAND_QUAD_NO_NEED_ENABLE;
+	}
+
+	ret = of_property_read_string(node, "read_from_cache_need_onedummy",
+			&read_seq_need_onedummy);
+	if (ret < 0 || NULL == read_seq_need_onedummy) {
+		pr_info("can't get spi-nand read_from_cache_need_onedummy or it is null,"
+				" maybe read from cache sequence not need one dummy in second Byte\n");
+	} else {
+		if (!memcmp(read_seq_need_onedummy, "yes", strlen("yes")))
+			info.OperationOpt |= SPINAND_ONEDUMMY_AFTER_RANDOMREAD;
+	}
+
+
+	ret = of_property_read_s32(node, "ecc_flag", &(info.EccFlag));
+	if (ret < 0) {
+		pr_err("can't get spi-nand EccFlag from fdt,"
+				" maybe(default) use 0FH + C0H to get feature,wich obtain ecc status\n");
+	}
+
+	ret = of_property_read_u32(node, "ecc_status_shift", &(info.ecc_status_shift));
+	if (ret < 0) {
+		pr_info("can't get spi-nand ecc_status_shift from fdt,"
+				" use default ecc_status_shift_4 to get ecc status in C0H\n");
+	}
+
+	pr_debug("get spinand phy info from fdt\n");
+	pr_debug("Model:%s\n", info.Model);
+	pr_debug("ID:%02x %02x %02x %02x %02x %02x %02x %02x\n",
+			info.NandID[0], info.NandID[1], info.NandID[2], info.NandID[3],
+			info.NandID[4], info.NandID[5], info.NandID[6], info.NandID[7]);
+	pr_debug("DieCntPerChip:%d\n", info.DieCntPerChip);
+	pr_debug("BlkCntPerDie:%d\n", info.BlkCntPerDie);
+	pr_debug("PageCntPerBlk:%d\n", info.PageCntPerBlk);
+	pr_debug("SectCntPerPage:%d\n", info.SectCntPerPage);
+	pr_debug("OobSizePerPage:%d\n", info.OobSizePerPage);
+	pr_debug("BadBlockFlag:%d\n", info.BadBlockFlag);
+	pr_debug("OperationOpt:0x%x\n", info.OperationOpt);
+	pr_debug("MaxEraseTimes:%d\n", info.MaxEraseTimes);
+	pr_debug("EccFlag:%x\n", info.EccFlag);
+	pr_debug("ecc_status_shift:%x\n", info.ecc_status_shift);
+	pr_debug("EccType:%x\n", info.EccType);
+	pr_debug("EccProtectedType:%x\n", info.EccProtectedType);
+
+	had_get = true;
+
+	return &info;
+err:
+	had_get = false;
+	return NULL;
+}
+
+static struct aw_spinand_info aw_spinand_info = {
+	.model = aw_spinand_info_model,
+	.manufacture = aw_spinand_info_manufacture,
+	.nandid = aw_spinand_info_nandid,
+	.die_cnt = aw_spinand_info_die_cnt,
+	.oob_size = aw_spinand_info_oob_size,
+	.page_size = aw_spinand_info_page_size,
+	.block_size = aw_spinand_info_block_size,
+	.phy_oob_size = aw_spinand_info_phy_oob_size,
+	.phy_page_size = aw_spinand_info_phy_page_size,
+	.phy_block_size = aw_spinand_info_phy_block_size,
+	.sector_size = aw_spinand_info_sector_size,
+	.total_size = aw_spinand_info_total_size,
+	.operation_opt = aw_spinand_info_operation_opt,
+	.max_erase_times = aw_spinand_info_max_erase_times,
+};
+
+static struct spinand_manufacture *spinand_detect_munufacture_from_fdt(struct aw_spinand_chip *chip, unsigned char id)
+{
+	struct aw_spinand_phy_info *info = NULL;
+	struct spinand_manufacture *pm = &m;
+	int ret = 0;
+
+	info = spinand_get_phy_info_from_fdt(chip);
+	if (info == NULL) {
+		pr_err("get phy info from fdt fail\n");
+		goto err;
+	}
+
+	if (id == info->NandID[0]) {
+		pm->id = info->NandID[0];
+		pm->info = info;
+		chip->info = &aw_spinand_info;
+		chip->info->phy_info = info;
+		ret = spinand_get_chip_munufacture(chip, &(pm->name));
+		if (ret < 0)
+			goto err;
+		else
+			pr_info("detect munufacture from fdt: %s \n", pm->name);
+	} else {
+		goto err;
+	}
+
+	return pm;
+err:
+	pr_info("not detect munufacture from fdt\n");
+	return NULL;
+}
+
+static struct aw_spinand_phy_info *spinand_match_id_from_fdt(struct aw_spinand_chip *chip,
+		struct spinand_manufacture *m,
+		unsigned char *id)
+{
+	struct aw_spinand_phy_info *info = NULL;
+	int i = 0;
+
+	info = spinand_get_phy_info_from_fdt(chip);
+	if (info == NULL) {
+		pr_err("get phy info from fdt fail\n");
+		goto err;
+	}
+
+	for (i = 0; i < MAX_ID_LEN; i++) {
+		/*0xff match all id value*/
+		if (id[i] != info->NandID[i] && info->NandID[i] != 0xff)
+			goto err;
+	}
+
+	return info;
+
+err:
+	return NULL;
+}
+static int aw_spinand_info_init(struct aw_spinand_chip *chip,
+		struct aw_spinand_phy_info *pinfo)
+{
+	chip->info = &aw_spinand_info;
+	chip->info->phy_info = pinfo;
+
+	pr_info("========== arch info ==========\n");
+	pr_info("Model:               %s\n", pinfo->Model);
+	pr_info("Munufacture:         %s\n", aw_spinand_info_manufacture(chip));
+	pr_info("DieCntPerChip:       %u\n", pinfo->DieCntPerChip);
+	pr_info("BlkCntPerDie:        %u\n", pinfo->BlkCntPerDie);
+	pr_info("PageCntPerBlk:       %u\n", pinfo->PageCntPerBlk);
+	pr_info("SectCntPerPage:      %u\n", pinfo->SectCntPerPage);
+	pr_info("OobSizePerPage:      %u\n", pinfo->OobSizePerPage);
+	pr_info("BadBlockFlag:        0x%x\n", pinfo->BadBlockFlag);
+	pr_info("OperationOpt:        0x%x\n", pinfo->OperationOpt);
+	pr_info("MaxEraseTimes:       %d\n", pinfo->MaxEraseTimes);
+	pr_info("EccFlag:             0x%x\n", pinfo->EccFlag);
+	pr_info("EccType:             %d\n", pinfo->EccType);
+	pr_info("EccProtectedType:    %d\n", pinfo->EccProtectedType);
+	pr_info("========================================\n");
+	pr_info("\n");
+	pr_info("========== physical info ==========\n");
+	pr_info("TotalSize:    %u M\n", to_mb(aw_spinand_info_total_size(chip)));
+	pr_info("SectorSize:   %u B\n", aw_spinand_info_sector_size(chip));
+	pr_info("PageSize:     %u K\n", to_kb(aw_spinand_info_phy_page_size(chip)));
+	pr_info("BlockSize:    %u K\n", to_kb(aw_spinand_info_phy_block_size(chip)));
+	pr_info("OOBSize:      %u B\n", aw_spinand_info_phy_oob_size(chip));
+	pr_info("========================================\n");
+	pr_info("\n");
+	pr_info("========== logical info ==========\n");
+	pr_info("TotalSize:    %u M\n", to_mb(aw_spinand_info_total_size(chip)));
+	pr_info("SectorSize:   %u B\n", aw_spinand_info_sector_size(chip));
+	pr_info("PageSize:     %u K\n", to_kb(aw_spinand_info_page_size(chip)));
+	pr_info("BlockSize:    %u K\n", to_kb(aw_spinand_info_block_size(chip)));
+	pr_info("OOBSize:      %u B\n", aw_spinand_info_oob_size(chip));
+	pr_info("========================================\n");
+
+	return 0;
+}
+
+int aw_spinand_chip_detect(struct aw_spinand_chip *chip)
+{
+	struct aw_spinand_phy_info *pinfo;
+	struct spinand_manufacture *m;
+	unsigned char id[MAX_ID_LEN] = {0xFF};
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	int ret, dummy = 0;
+
+retry:
+	ret = ops->read_id(chip, id, MAX_ID_LEN, dummy);
+	if (ret) {
+		pr_err("read id failed : %d\n", ret);
+		return ret;
+	}
+
+	m = spinand_detect_munufacture(id[0]);
+	if (!m)
+		goto detect_from_fdt;
+
+	pinfo = spinand_match_id(m, id);
+	if (pinfo)
+		goto detect;
+
+detect_from_fdt:
+		m = spinand_detect_munufacture_from_fdt(chip, id[0]);
+		if (!m)
+			goto not_detect;
+
+		pinfo = spinand_match_id_from_fdt(chip, m, id);
+		if (pinfo)
+			goto detect;
+
+not_detect:
+	/* retry with dummy */
+	if (!dummy) {
+		dummy++;
+		goto retry;
+	}
+	pr_info("not match spinand: %x %x\n",
+			*(__u32 *)id,
+			*((__u32 *)id + 1));
+	return -ENODEV;
+detect:
+	pr_info("detect spinand id: %x %x\n",
+			*((__u32 *)pinfo->NandID),
+			*((__u32 *)pinfo->NandID + 1));
+	return aw_spinand_info_init(chip, pinfo);
+}
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Commond physic layer for Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/physic/ops.c b/drivers/mtd/awnand/spinand/physic/ops.c
new file mode 100644
index 000000000..ec5af4b82
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/ops.c
@@ -0,0 +1,832 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand-phy: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand.h>
+#include <linux/types.h>
+#include <linux/delay.h>
+
+#include "physic.h"
+
+static int aw_spinand_chip_wait(struct aw_spinand_chip *chip, u8 *status)
+{
+	unsigned long timeout = jiffies + msecs_to_jiffies(1000);
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	u8 s = 0;
+	int ret;
+
+	do {
+		ret = ops->read_status(chip, &s);
+		if (ret)
+			return ret;
+
+		if (!(s & STATUS_BUSY))
+			goto out;
+	} while (time_before(jiffies, timeout));
+
+	/*
+	 * Extra read, just in case the STATUS_READY bit has changed
+	 * since our last check
+	 */
+	ret = ops->read_status(chip, &s);
+	if (ret)
+		return ret;
+
+out:
+	if (status)
+		*status = s;
+	return s & STATUS_BUSY ? -ETIMEDOUT : 0;
+}
+
+static int aw_spinand_chip_reset(struct aw_spinand_chip *chip)
+{
+	int ret;
+	unsigned char txbuf[1];
+
+	txbuf[0] = SPI_NAND_RESET;
+
+	ret = spi_write(chip->spi, txbuf, 1);
+	if (ret)
+		return ret;
+
+	return aw_spinand_chip_wait(chip, NULL);
+}
+
+static int aw_spinand_chip_read_id(struct aw_spinand_chip *chip, void *id,
+		int len, int dummy)
+{
+	int ret;
+	unsigned char txbuf[2] = {SPI_NAND_RDID, 0x00};
+
+	ret = aw_spinand_chip_reset(chip);
+	if (ret)
+		return ret;
+
+	/* some spinand readid conmand should follow one byte dummy */
+	if (dummy)
+		return spi_write_then_read(chip->spi, txbuf, 2, id, len);
+	else
+		return spi_write_then_read(chip->spi, txbuf, 1, id, len);
+}
+
+static int aw_spinand_chip_read_reg(struct aw_spinand_chip *chip, u8 cmd,
+		u8 reg, u8 *val)
+{
+	unsigned char txbuf[2];
+
+	txbuf[0] = cmd;
+	txbuf[1] = reg;
+
+	return spi_write_then_read(chip->spi, txbuf, 2, val, 1);
+}
+
+static int aw_spinand_chip_write_reg(struct aw_spinand_chip *chip, u8 cmd,
+		u8 reg, u8 val)
+{
+	unsigned char txbuf[3];
+
+	txbuf[0] = cmd;
+	txbuf[1] = reg;
+	txbuf[2] = val;
+
+	return spi_write(chip->spi, txbuf, 3);
+}
+
+static int aw_spinand_chip_read_status(struct aw_spinand_chip *chip,
+		u8 *status)
+{
+	return aw_spinand_chip_read_reg(chip, SPI_NAND_GETSR, REG_STATUS,
+			status);
+}
+
+static int aw_spinand_chip_get_block_lock(struct aw_spinand_chip *chip,
+		u8 *reg_val)
+{
+	return aw_spinand_chip_read_reg(chip, SPI_NAND_GETSR, REG_BLOCK_LOCK,
+			reg_val);
+}
+
+static int aw_spinand_chip_set_block_lock(struct aw_spinand_chip *chip,
+		u8 reg_val)
+{
+	return aw_spinand_chip_write_reg(chip, SPI_NAND_SETSR, REG_BLOCK_LOCK,
+			reg_val);
+}
+
+static int aw_spinand_chip_get_otp(struct aw_spinand_chip *chip, u8 *reg_val)
+{
+	return aw_spinand_chip_read_reg(chip, SPI_NAND_GETSR, REG_CFG, reg_val);
+}
+
+static int aw_spinand_chip_set_otp(struct aw_spinand_chip *chip, u8 reg_val)
+{
+	return aw_spinand_chip_write_reg(chip, SPI_NAND_SETSR, REG_CFG, reg_val);
+}
+
+static int aw_spinand_chip_get_driver_level(struct aw_spinand_chip *chip,
+		u8 *reg_val)
+{
+	return aw_spinand_chip_read_reg(chip, SPI_NAND_GETSR, REG_DRV, reg_val);
+}
+
+static int aw_spinand_chip_set_driver_level(struct aw_spinand_chip *chip,
+		u8 reg_val)
+{
+	return aw_spinand_chip_write_reg(chip, SPI_NAND_SETSR, REG_CFG, reg_val);
+}
+
+static int aw_spinand_chip_write_enable(struct aw_spinand_chip *chip)
+{
+	unsigned char txbuf[1];
+
+	txbuf[0] = SPI_NAND_WREN;
+	return spi_write(chip->spi, txbuf, 1);
+}
+
+/**
+ * addr_to_req: convert data address to request for spinand
+ *
+ * @chip: spinand chip
+ * @req: the request to save
+ * @addr: the address
+ *
+ * Note:
+ * If simulate multiplane, the request converted based on super page/block.
+ */
+int addr_to_req(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req, unsigned int addr)
+{
+	struct aw_spinand_info *info = chip->info;
+	unsigned int addrtmp = addr;
+
+	if (addr > info->total_size(chip)) {
+		pr_err("over size: %u > %u\n", addr, info->total_size(chip));
+		return -EOVERFLOW;
+	}
+	req->block = addrtmp / info->block_size(chip);
+	addrtmp = addrtmp & (info->block_size(chip) - 1);
+	req->page = addrtmp / info->page_size(chip);
+	req->pageoff = addrtmp & (info->page_size(chip) - 1);
+	pr_debug("addr 0x%x to blk %u page %u pageoff %u\n",
+			addr, req->block, req->page, req->pageoff);
+	return 0;
+}
+EXPORT_SYMBOL(addr_to_req);
+
+/**
+ * req_to_paddr: convert request for spinand to page address
+ *
+ * @chip: spinand chip
+ * @req: the request to save
+ * @addr: the address
+ *
+ * Note:
+ * If simulate multiplane, the request converted based on super page/block.
+ * It is only used to get the page address to send to spinand.
+ */
+static unsigned int req_to_paddr(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	return req->block * pinfo->PageCntPerBlk + req->page;
+}
+
+static int aw_spinand_chip_erase_single_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	int ret;
+	unsigned char txbuf[4];
+	unsigned int paddr, pmax;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+	u8 status = 0;
+
+	ret = aw_spinand_chip_write_enable(chip);
+	if (ret)
+		return ret;
+
+	pmax = pinfo->DieCntPerChip * pinfo->BlkCntPerDie * pinfo->PageCntPerBlk;
+	paddr = req_to_paddr(chip, req);
+	if (paddr >= pmax)
+		return -EOVERFLOW;
+
+	txbuf[0] = SPI_NAND_BE;
+	txbuf[1] = (paddr >> 16) & 0xFF;
+	txbuf[2] = (paddr >> 8) & 0xFF;
+	txbuf[3] = paddr & 0xFF;
+
+	ret = spi_write(chip->spi, txbuf, 4);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_wait(chip, &status);
+	if (!ret && (status & STATUS_ERASE_FAILED))
+		ret = -EIO;
+
+	return ret;
+}
+
+/* the request must bases on single physical page/block */
+static inline int aw_spinand_chip_write_to_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	return chip->cache->write_to_cache(chip, req);
+}
+
+static int aw_spinand_chip_program(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	unsigned char txbuf[4];
+	unsigned int paddr, pmax;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	pmax = pinfo->DieCntPerChip * pinfo->BlkCntPerDie * pinfo->PageCntPerBlk;
+	paddr = req_to_paddr(chip, req);
+	if (paddr >= pmax)
+		return -EOVERFLOW;
+
+	txbuf[0] = SPI_NAND_PE;
+	txbuf[1] = (paddr >> 16) & 0xFF;
+	txbuf[2] = (paddr >> 8) & 0xFF;
+	txbuf[3] = paddr & 0xFF;
+
+	return spi_write(chip->spi, txbuf, 4);
+}
+
+static int aw_spinand_chip_write_single_page(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	int ret;
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_phy_info *pinfo = info->phy_info;
+	u8 status = 0;
+
+	aw_spinand_reqdump(pr_debug, "do single write", req);
+	BUG_ON(req->pageoff + req->datalen > chip->info->phy_page_size(chip));
+
+	if (req->page >= pinfo->PageCntPerBlk) {
+		pr_err("page %u over max pages per blk %u\n", req->page,
+				pinfo->PageCntPerBlk);
+		return -EOVERFLOW;
+	}
+
+	if (req->block >= pinfo->BlkCntPerDie) {
+		pr_err("block %u over max blocks per die %u\n", req->block,
+				pinfo->BlkCntPerDie);
+		return -EOVERFLOW;
+	}
+
+	ret = aw_spinand_chip_write_enable(chip);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_write_to_cache(chip, req);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_program(chip, req);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_wait(chip, &status);
+	if (!ret && (status & STATUS_PROG_FAILED))
+		ret = -EIO;
+
+	return ret;
+}
+
+static int aw_spinand_chip_load_page(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	unsigned char txbuf[4];
+	unsigned int paddr, pmax;
+	struct aw_spinand_info *info = chip->info;
+
+	WARN_ON(!req->datalen && !req->ooblen);
+
+	pmax = info->total_size(chip) / info->phy_page_size(chip);
+	paddr = req_to_paddr(chip, req);
+	if (paddr >= pmax)
+		return -EOVERFLOW;
+
+	txbuf[0] = SPI_NAND_PAGE_READ;
+	txbuf[1] = (paddr >> 16) & 0xFF;
+	txbuf[2] = (paddr >> 8) & 0xFF;
+	txbuf[3] = paddr & 0xFF;
+
+	return spi_write(chip->spi, txbuf, 4);
+}
+
+static inline int aw_spinand_chip_read_from_cache(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	return chip->cache->read_from_cache(chip, req);
+}
+
+static int aw_spinand_chip_check_ecc(struct aw_spinand_chip *chip, u8 status)
+{
+	int ret;
+	struct aw_spinand_ecc *ecc = chip->ecc;
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	/* to get real ecc status */
+	if (pinfo->EccFlag & HAS_EXT_ECC_STATUS) {
+		/* extern ecc status should not shift */
+		ret = ops->read_reg(chip, SPI_NAND_READ_INT_ECCSTATUS, 0,
+				&status);
+		if (ret)
+			return ret;
+	} else {
+		if (pinfo->ecc_status_shift)
+			status = status >> pinfo->ecc_status_shift;
+		else
+			status = status >> STATUS_ECC_SHIFT;
+	}
+	if (status && pinfo->EccFlag & HAS_EXT_ECC_SE01) {
+		u8 ext_status;
+
+		ret = ops->read_reg(chip, SPI_NAND_GETSR, GD_REG_EXT_ECC_STATUS,
+				&ext_status);
+		if (pinfo->ecc_status_shift)
+			ext_status = (ext_status >> pinfo->ecc_status_shift) & 0x03;
+		else
+			ext_status = (ext_status >> STATUS_ECC_SHIFT) & 0x03;
+		status = (status << 2) | ext_status;
+	}
+
+	return ecc->check_ecc(pinfo->EccType, status);
+}
+
+static int aw_spinand_chip_read_single_page(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	int ret;
+	u8 status = 0;
+	struct aw_spinand_cache *cache = chip->cache;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+
+	aw_spinand_reqdump(pr_debug, "do single read", req);
+	BUG_ON(req->pageoff + req->datalen > chip->info->phy_page_size(chip));
+
+	if (req->page >= pinfo->PageCntPerBlk) {
+		pr_err("page %u over max pages per blk %u\n", req->page,
+				pinfo->PageCntPerBlk);
+		return -EOVERFLOW;
+	}
+
+	if (req->block >= pinfo->BlkCntPerDie) {
+		pr_err("block %u over max blocks per die %u\n", req->block,
+				pinfo->BlkCntPerDie);
+		return -EOVERFLOW;
+	}
+
+	/* If the cache already has the data before, just copy them to req */
+	if (cache->match_cache(chip, req)) {
+		pr_debug("cache match request blk %u page %u, no need to send to spinand\n",
+				req->block, req->page);
+		return cache->copy_from_cache(chip, req);
+	}
+
+	ret = aw_spinand_chip_load_page(chip, req);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_wait(chip, &status);
+	if (ret)
+		return ret;
+
+	ret = aw_spinand_chip_read_from_cache(chip, req);
+	if (ret)
+		return ret;
+
+	return aw_spinand_chip_check_ecc(chip, status);
+}
+
+static int _aw_spinand_chip_isbad_single_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+	int page_addr[2] = {0, -1};
+	unsigned char oob[AW_OOB_SIZE_PER_PHY_PAGE] = {0xFF};
+	int i;
+
+	switch (pinfo->BadBlockFlag & BAD_BLK_FLAG_MARK) {
+	case BAD_BLK_FLAG_FRIST_1_PAGE:
+		/*
+		 * the bad block flag is in the first page, same as the logical
+		 * information, just read 1 page is ok
+		 */
+		page_addr[0] = 0;
+		break;
+	case BAD_BLK_FLAG_FIRST_2_PAGE:
+		/*
+		 * the bad block flag is in the first page or the second page,
+		 * need read the first page and the second page
+		 */
+		page_addr[1] = 1;
+		break;
+	case BAD_BLK_FLAG_LAST_1_PAGE:
+		/*
+		 * the bad block flag is in the last page, need read the first
+		 * page and the last page
+		 */
+		page_addr[1] = pinfo->PageCntPerBlk - 1;
+		break;
+	case BAD_BLK_FLAG_LAST_2_PAGE:
+		/*
+		 * the bad block flag is in the last 2 page, so, need read the
+		 * first page, the last page and the last-1 page
+		 */
+		page_addr[1] = pinfo->PageCntPerBlk - 2;
+		break;
+	default:
+		break;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(page_addr); i++) {
+		struct aw_spinand_chip_request tmp = {0};
+		int ret;
+
+		if (page_addr[i] == -1)
+			break;
+
+		tmp.block = req->block;
+		tmp.page = (unsigned int)page_addr[i];
+		tmp.ooblen = AW_OOB_SIZE_PER_PHY_PAGE;
+		tmp.oobbuf = oob;
+		ret = aw_spinand_chip_read_single_page(chip, &tmp);
+		if (ret < 0)
+			ret = aw_spinand_chip_read_single_page(chip, &tmp);
+		/* ignore ECC_ERROR and ECC_LIMIT */
+		if (ret < 0 || oob[0] != 0xFF)
+			return true;
+	}
+	return false;
+}
+
+static int aw_spinand_chip_isbad_single_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_bbt *bbt = chip->bbt;
+	int ret;
+
+	ret = bbt->is_badblock(chip, req->block);
+	if (ret == NOT_MARKED) {
+		ret = _aw_spinand_chip_isbad_single_block(chip, req);
+		if (ret < 0)
+			return ret;
+		if (ret == true)
+			bbt->mark_badblock(chip, req->block, true);
+		else
+			bbt->mark_badblock(chip, req->block, false);
+
+		if (ret == true)
+			pr_info("phy blk %d is bad\n", req->block);
+	}
+	return ret;
+}
+
+static int aw_spinand_chip_markbad_single_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	int ret;
+	struct aw_spinand_phy_info *pinfo = chip->info->phy_info;
+	struct aw_spinand_bbt *bbt = chip->bbt;
+	unsigned char oob[AW_OOB_SIZE_PER_PHY_PAGE];
+	struct aw_spinand_chip_request tmp = {0};
+
+	ret = aw_spinand_chip_isbad_single_block(chip, req);
+	if (ret == true)
+		return 0;
+
+	ret = aw_spinand_chip_erase_single_block(chip, req);
+	if (ret)
+		pr_err("erase phy blk %d before markbad failed with %d back\n",
+				req->block, ret);
+
+	bbt->mark_badblock(chip, req->block, true);
+
+	memset(oob, 0, AW_OOB_SIZE_PER_PHY_PAGE);
+	tmp.block = req->block;
+	tmp.oobbuf = oob;
+	tmp.ooblen = AW_OOB_SIZE_PER_PHY_PAGE;
+
+	/* write bad flag on the first page */
+	tmp.page = 0;
+	ret = aw_spinand_chip_write_single_page(chip, &tmp);
+	if (ret) {
+		pr_err("mark phy blk %u page %d as bad failed with %d back\n",
+				tmp.block, tmp.page, ret);
+		return ret;
+	}
+
+	/* write bad flag on the last page */
+	tmp.page = pinfo->PageCntPerBlk - 1;
+	ret = aw_spinand_chip_write_single_page(chip, &tmp);
+	if (ret) {
+		pr_err("mark phy blk %u page %d as bad failed with %d back\n",
+				tmp.block, tmp.page, ret);
+		return ret;
+	}
+	return 0;
+}
+
+static int aw_spinand_chip_copy_single_block(struct aw_spinand_chip *chip,
+		unsigned int from_blk, unsigned int to_blk)
+{
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_phy_info *pinfo = info->phy_info;
+	struct aw_spinand_chip_request req = {0};
+	int i, ret = -ENOMEM;
+
+	req.datalen = info->phy_page_size(chip);
+	req.databuf = kmalloc(req.datalen, GFP_KERNEL);
+	if (!req.databuf)
+		return ret;
+
+	req.ooblen = info->phy_oob_size(chip);
+	req.oobbuf = kmalloc(req.ooblen, GFP_KERNEL);
+	if (!req.oobbuf)
+		goto free_databuf;
+
+	req.block = to_blk;
+	ret = aw_spinand_chip_erase_single_block(chip, &req);
+	if (ret)
+		goto free_oobbuf;
+
+	for (i = 0; i < pinfo->PageCntPerBlk; i++) {
+		req.page = i;
+
+		req.block = from_blk;
+		ret = aw_spinand_chip_read_single_page(chip, &req);
+		if (ret)
+			goto free_oobbuf;
+
+		req.block = to_blk;
+		ret = aw_spinand_chip_write_single_page(chip, &req);
+		if (ret)
+			goto free_oobbuf;
+	}
+
+	ret = 0;
+free_oobbuf:
+	kfree(req.oobbuf);
+free_databuf:
+	kfree(req.databuf);
+	return ret;
+}
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_SIMULATE_MULTIPLANE)
+
+static void super_to_phy(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super,
+		struct aw_spinand_chip_request *phy)
+{
+	struct aw_spinand_info *info = chip->info;
+	unsigned int phy_page_size, phy_oob_size;
+
+	phy_page_size = info->phy_page_size(chip);
+	phy_oob_size = info->phy_oob_size(chip);
+
+	phy->databuf = super->databuf;
+	phy->oobbuf = super->oobbuf;
+	phy->dataleft = super->datalen;
+	phy->oobleft = super->ooblen;
+	phy->pageoff = super->pageoff & (phy_page_size - 1);
+	phy->ooblen = min3(phy_oob_size, super->ooblen,
+				(unsigned int)AW_OOB_SIZE_PER_PHY_PAGE);
+	phy->datalen = min(phy_page_size - phy->pageoff, phy->dataleft);
+	phy->page = super->page;
+	if (super->pageoff >= phy_page_size)
+		phy->block = super->block * 2 + 1;
+	else
+		phy->block = super->block * 2;
+}
+
+void aw_spinand_chip_super_init(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super,
+		struct aw_spinand_chip_request *phy)
+{
+	super_to_phy(chip, super, phy);
+}
+
+int aw_spinand_chip_super_end(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	if (req->dataleft || req->oobleft)
+		return false;
+	return true;
+}
+
+void aw_spinand_chip_super_next(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_info *info = chip->info;
+	unsigned int phy_page_size, phy_oob_size;
+	const char *str;
+
+	if (req->dataleft < req->datalen) {
+		str = "[phy] dataleft < datalen";
+		goto bug;
+	}
+
+	if (req->oobleft < req->ooblen) {
+		str = "[phy] oobleft < ooblen";
+		goto bug;
+	}
+
+	phy_page_size = info->phy_page_size(chip);
+	phy_oob_size = info->phy_oob_size(chip);
+
+	req->databuf += req->datalen;
+	req->dataleft -= req->datalen;
+	req->oobbuf += req->ooblen;
+	req->oobleft -= req->ooblen;
+	req->datalen = min(req->dataleft, phy_page_size);
+	req->ooblen = min(req->oobleft, phy_oob_size);
+	/* next page have no offset */
+	req->pageoff = 0;
+	req->block++;
+
+	/*
+	 * The super page (4K) just cut to 2 phyical page(2K), so,
+	 * aw_spinand_chip_for_each_single maximum can only loop for twice.
+	 */
+	if ((req->dataleft || req->oobleft) && !(req->block % 2)) {
+		str = "[phy] over loop twice";
+		goto bug;
+	}
+
+	return;
+bug:
+	aw_spinand_reqdump(pr_err, str, req);
+	WARN_ON(1);
+	return;
+}
+
+#define aw_spinand_chip_for_each_single(chip, super, phy)		\
+	for (aw_spinand_chip_super_init(chip, super, phy);		\
+		!aw_spinand_chip_super_end(chip, phy);			\
+		aw_spinand_chip_super_next(chip, phy))
+
+static int aw_spinand_chip_isbad_super_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super)
+{
+	struct aw_spinand_chip_request phy = {0};
+	int ret;
+
+	super_to_phy(chip, super, &phy);
+	if (unlikely(phy.block % 2)) {
+		pr_err("unaligned block %d\n", phy.block);
+		return -EINVAL;
+	}
+
+	/* check the 1st block */
+	ret = aw_spinand_chip_isbad_single_block(chip, &phy);
+	if (ret)
+		return ret;
+
+	/* check the 2st block */
+	phy.block++;
+	ret = aw_spinand_chip_isbad_single_block(chip, &phy);
+	if (ret)
+		return ret;
+
+	return false;
+}
+
+static int aw_spinand_chip_markbad_super_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super)
+{
+	struct aw_spinand_chip_request phy = {0};
+	int ret;
+
+	super_to_phy(chip, super, &phy);
+	if (unlikely(phy.block % 2)) {
+		pr_err("unaligned block %d\n", phy.block);
+		return -EINVAL;
+	}
+
+	/* mark the 1st block */
+	ret = aw_spinand_chip_markbad_single_block(chip, &phy);
+
+	/* check the 2st block */
+	phy.block++;
+	ret &= aw_spinand_chip_markbad_single_block(chip, &phy);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int aw_spinand_chip_erase_super_block(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super)
+{
+	struct aw_spinand_chip_request phy = {0};
+	int ret;
+
+	super_to_phy(chip, super, &phy);
+	if (unlikely(phy.block % 2)) {
+		pr_err("unaligned block %d\n", phy.block);
+		return -EINVAL;
+	}
+
+	/* erase the 1st block */
+	ret = aw_spinand_chip_erase_single_block(chip, &phy);
+	if (ret)
+		return ret;
+
+	/* check the 2st block */
+	phy.block++;
+	ret = aw_spinand_chip_erase_single_block(chip, &phy);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int aw_spinand_chip_write_super_page(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super)
+{
+	struct aw_spinand_chip_request phy = {0};
+	int ret;
+
+	aw_spinand_chip_for_each_single(chip, super, &phy) {
+		ret = aw_spinand_chip_write_single_page(chip, &phy);
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+
+static int aw_spinand_chip_read_super_page(struct aw_spinand_chip *chip,
+		struct aw_spinand_chip_request *super)
+{
+	struct aw_spinand_chip_request phy = {0};
+	int ret, limit = 0;
+
+	aw_spinand_chip_for_each_single(chip, super, &phy) {
+		ret = aw_spinand_chip_read_single_page(chip, &phy);
+		if (ret < 0)
+			return ret;
+		if (ret == ECC_LIMIT) {
+			pr_debug("ecc limit: phy block: %u page: %u\n",
+					phy.block, phy.page);
+			limit = ECC_LIMIT;
+			continue;
+		} else if (ret == ECC_ERR) {
+			pr_err("ecc err: phy block: %u page: %u\n",
+					phy.block, phy.page);
+			return ret;
+		}
+		/* else ECC_GOOD */
+	}
+	return limit;
+}
+#endif
+
+static struct aw_spinand_chip_ops spinand_ops = {
+	.get_block_lock = aw_spinand_chip_get_block_lock,
+	.set_block_lock = aw_spinand_chip_set_block_lock,
+	.get_otp = aw_spinand_chip_get_otp,
+	.set_otp = aw_spinand_chip_set_otp,
+	.get_driver_level = aw_spinand_chip_get_driver_level,
+	.set_driver_level = aw_spinand_chip_set_driver_level,
+	.reset = aw_spinand_chip_reset,
+	.read_status = aw_spinand_chip_read_status,
+	.read_id = aw_spinand_chip_read_id,
+	.write_reg = aw_spinand_chip_write_reg,
+	.read_reg = aw_spinand_chip_read_reg,
+#if IS_ENABLED(CONFIG_AW_SPINAND_SIMULATE_MULTIPLANE)
+	.is_bad = aw_spinand_chip_isbad_super_block,
+	.mark_bad = aw_spinand_chip_markbad_super_block,
+	.erase_block = aw_spinand_chip_erase_super_block,
+	.write_page = aw_spinand_chip_write_super_page,
+	.read_page = aw_spinand_chip_read_super_page,
+#else
+	.is_bad = aw_spinand_chip_isbad_single_block,
+	.mark_bad = aw_spinand_chip_markbad_single_block,
+	.erase_block = aw_spinand_chip_erase_single_block,
+	.write_page = aw_spinand_chip_write_single_page,
+	.read_page = aw_spinand_chip_read_single_page,
+#endif
+	.phy_is_bad = aw_spinand_chip_isbad_single_block,
+	.phy_mark_bad = aw_spinand_chip_markbad_single_block,
+	.phy_erase_block = aw_spinand_chip_erase_single_block,
+	.phy_write_page = aw_spinand_chip_write_single_page,
+	.phy_read_page = aw_spinand_chip_read_single_page,
+	.phy_copy_block = aw_spinand_chip_copy_single_block,
+};
+
+int aw_spinand_chip_ops_init(struct aw_spinand_chip *chip)
+{
+	chip->ops = &spinand_ops;
+	return 0;
+}
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Commond physic layer for Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/physic/physic.h b/drivers/mtd/awnand/spinand/physic/physic.h
new file mode 100644
index 000000000..83fbaf07d
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/physic/physic.h
@@ -0,0 +1,202 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#ifndef __SPINAND_PHYSIC_H
+#define __SPINAND_PHYSIC_H
+
+#include <linux/mtd/aw-spinand.h>
+#include <linux/bitops.h>
+#include <linux/printk.h>
+
+#define AW_SPINAND_PHY_VER_MAIN	0x01
+#define AW_SPINAND_PHY_VER_SUB	0x10
+#define AW_SPINAND_PHY_VER_DATE	0x20200306
+
+#define SECTOR_SHIFT 9
+
+/* spinand cmd num */
+#define SPI_NAND_WREN		0x06
+#define SPI_NAND_WRDI		0x04
+#define SPI_NAND_GETSR		0x0f
+#define SPI_NAND_SETSR		0x1f
+#define SPI_NAND_PAGE_READ	0x13
+#define SPI_NAND_FAST_READ_X1	0x0b
+#define SPI_NAND_READ_X1	0x03
+#define SPI_NAND_READ_X2	0x3b
+#define SPI_NAND_READ_X4	0x6b
+#define SPI_NAND_READ_DUAL_IO	0xbb
+#define SPI_NAND_READ_QUAD_IO	0xeb
+#define SPI_NAND_RDID		0x9f
+#define SPI_NAND_PP		0x02
+#define SPI_NAND_PP_X4		0x32
+#define SPI_NAND_RANDOM_PP	0x84
+#define SPI_NAND_RANDOM_PP_X4	0x34
+#define SPI_NAND_PE		0x10
+#define SPI_NAND_BE		0xd8
+#define SPI_NAND_RESET		0xff
+#define SPI_NAND_READ_INT_ECCSTATUS 0x7c
+
+/* status register */
+#define REG_STATUS		0xc0
+#define GD_REG_EXT_ECC_STATUS	0xf0
+#define STATUS_BUSY		BIT(0)
+#define STATUS_ERASE_FAILED	BIT(2)
+#define STATUS_PROG_FAILED	BIT(3)
+#define STATUS_ECC_SHIFT	4
+
+/* feature register */
+#define REG_BLOCK_LOCK		0xa0
+
+/* configuration register */
+#define FORESEE_REG_ECC_CFG	0x90
+#define REG_CFG			0xb0
+#define CFG_OTP_ENABLE		BIT(6)
+#define CFG_BUF_MODE		BIT(3)
+#define CFG_ECC_ENABLE		BIT(4)
+#define CFG_QUAD_ENABLE		BIT(0)
+
+/* driver strength register */
+#define REG_DRV			0xd0
+
+#define SPI_SELECT_ODDNUM_BLACK 0x10
+
+/*differrent manufacture spinand's ecc status location maybe not the same*/
+enum ecc_status_shift {
+	ECC_STATUS_SHIFT_0 = 0,
+	ECC_STATUS_SHIFT_1,
+	ECC_STATUS_SHIFT_2,
+	ECC_STATUS_SHIFT_3,
+	ECC_STATUS_SHIFT_4,
+	ECC_STATUS_SHIFT_5,
+	ECC_STATUS_SHIFT_6,
+	ECC_STATUS_SHIFT_7,
+};
+
+enum ecc_limit_err {
+	ECC_TYPE_ERR = 0,
+	BIT3_LIMIT2_TO_6_ERR7,
+	BIT2_LIMIT1_ERR2,
+	BIT2_LIMIT1_ERR2_LIMIT3,
+	BIT4_LIMIT3_TO_4_ERR15,
+	BIT3_LIMIT3_TO_4_ERR7,
+	BIT3_LIMIT5_ERR2,
+	BIT4_LIMIT5_TO_7_ERR8_LIMIT_12,
+	BIT4_LIMIT5_TO_8_ERR9_TO_15,
+};
+
+enum ecc_oob_protected {
+	ECC_PROTECTED_TYPE = 0,
+	/* all spare data are under ecc protection */
+	SIZE16_OFF0_LEN16,
+	SIZE16_OFF4_LEN12,
+	SIZE16_OFF4_LEN4_OFF8,
+	/*compatible with GD5F1GQ4UBYIG@R6*/
+	SIZE16_OFF4_LEN8_OFF4,
+	SIZE16_OFF32_LEN16,
+	/*compatible with XTX*/
+	SIZE16_OFF8_LEN16,
+};
+
+struct aw_spinand_ecc {
+	int (*copy_to_oob)(enum ecc_oob_protected type, unsigned char *to,
+			unsigned char *from, unsigned int len);
+	int (*copy_from_oob)(enum ecc_oob_protected type, unsigned char *to,
+			unsigned char *from, unsigned int len);
+	int (*check_ecc)(enum ecc_limit_err type, u8 status);
+};
+
+struct aw_spinand_phy_info {
+	const char *Model;
+	unsigned char NandID[MAX_ID_LEN];
+	unsigned int DieCntPerChip;
+	unsigned int BlkCntPerDie;
+	unsigned int PageCntPerBlk;
+	unsigned int SectCntPerPage;
+	unsigned int OobSizePerPage;
+#define BAD_BLK_FLAG_MARK			0x03
+#define BAD_BLK_FLAG_FRIST_1_PAGE		0x00
+#define BAD_BLK_FLAG_FIRST_2_PAGE		0x01
+#define BAD_BLK_FLAG_LAST_1_PAGE		0x02
+#define BAD_BLK_FLAG_LAST_2_PAGE		0x03
+	int BadBlockFlag;
+#define SPINAND_DUAL_READ			BIT(0)
+#define SPINAND_QUAD_READ			BIT(1)
+#define SPINAND_QUAD_PROGRAM			BIT(2)
+#define SPINAND_QUAD_NO_NEED_ENABLE		BIT(3)
+#define	SPINAND_TWO_PLANE_SELECT		BIT(7)
+#define SPINAND_ONEDUMMY_AFTER_RANDOMREAD	BIT(8)
+	int OperationOpt;
+	int MaxEraseTimes;
+#define HAS_EXT_ECC_SE01			BIT(0)
+#define HAS_EXT_ECC_STATUS			BIT(1)
+	enum ecc_status_shift ecc_status_shift;
+	int EccFlag;
+	enum ecc_limit_err EccType;
+	enum ecc_oob_protected EccProtectedType;
+};
+
+/* bbt: bad block table */
+struct aw_spinand_bbt {
+	unsigned long *bitmap;
+	unsigned long *en_bitmap;
+
+	int (*mark_badblock)(struct aw_spinand_chip *chip,
+			unsigned int blknum, bool badblk);
+#define BADBLOCK	(1)
+#define NON_BADBLOCK	(0)
+#define NOT_MARKED	(-1)
+	int (*is_badblock)(struct aw_spinand_chip *chip, unsigned int blknum);
+};
+
+struct aw_spinand_cache {
+	unsigned char *databuf;
+	unsigned char *oobbuf;
+	unsigned int data_maxlen;
+	unsigned int oob_maxlen;
+	unsigned int block;
+	unsigned int page;
+#define INVALID_CACHE_ALL_AREA	0
+#define VALID_CACHE_OOB		BIT(1)
+#define VALID_CACHE_DATA	BIT(2)
+	unsigned int area;
+
+	/*
+	 * If the structure cache already has the data before, just copy
+	 * these data to req.
+	 * @match_cache is helper to check whether the structure cache is
+	 *   what req needed.
+	 * @copy_to_cache is helper to copy req to structure cache.
+	 * @copy_from_cache is helper to copy structure cache to req.
+	 */
+	bool (*match_cache)(struct aw_spinand_chip *chip,
+			struct aw_spinand_chip_request *req);
+	int (*copy_to_cache)(struct aw_spinand_chip *chip,
+			struct aw_spinand_chip_request *req);
+	int (*copy_from_cache)(struct aw_spinand_chip *chip,
+			struct aw_spinand_chip_request *req);
+	/*
+	 * 3 step:
+	 *  a) copy data from req to cache->databuf/oobbuf
+	 *  b) update cache->block/page
+	 *  c) send write cache command to spinand
+	 */
+	int (*write_to_cache)(struct aw_spinand_chip *chip,
+			struct aw_spinand_chip_request *req);
+	/*
+	 * 3 step:
+	 *  a) send read cache command to spinand
+	 *  b) update cache->block/page
+	 *  c) copy data from cache->databuf/oobbuf to req
+	 */
+	int (*read_from_cache)(struct aw_spinand_chip *chip,
+			struct aw_spinand_chip_request *req);
+};
+
+extern int aw_spinand_chip_ecc_init(struct aw_spinand_chip *chip);
+extern int aw_spinand_chip_ops_init(struct aw_spinand_chip *chip);
+extern int aw_spinand_chip_detect(struct aw_spinand_chip *chip);
+extern int aw_spinand_chip_bbt_init(struct aw_spinand_chip *chip);
+extern void aw_spinand_chip_bbt_exit(struct aw_spinand_chip *chip);
+extern int aw_spinand_chip_cache_init(struct aw_spinand_chip *chip);
+extern void aw_spinand_chip_cache_exit(struct aw_spinand_chip *chip);
+
+#endif
diff --git a/drivers/mtd/awnand/spinand/secure-storage.c b/drivers/mtd/awnand/spinand/secure-storage.c
new file mode 100644
index 000000000..0de87f98e
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/secure-storage.c
@@ -0,0 +1,389 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/spi/spi.h>
+#include <linux/mtd/aw-spinand.h>
+#include <linux/mtd/partitions.h>
+#include <linux/mtd/mtd.h>
+
+#define SEC_STO_MAGIC 0x5CAA
+#define SEC_STO_ITEM_BYTES (4096)
+
+struct sec_sto_oob {
+	__u8 badflag;
+	__u16 magic;
+	__u32 checksum;
+} __packed;
+
+static inline bool is_init_end(struct aw_spinand_sec_sto *sec_sto)
+{
+	return !!sec_sto->init_end;
+}
+
+static int is_secure_storage_block(struct aw_spinand_chip *chip,
+		unsigned int blk, unsigned int page)
+{
+	struct aw_spinand_chip_request req = {0};
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct sec_sto_oob oob;
+	int ret;
+
+	req.block = blk;
+	req.page = page;
+	req.oobbuf = &oob;
+	req.ooblen = sizeof(struct sec_sto_oob);
+	ret = ops->phy_read_page(chip, &req);
+	if (ret) {
+		pr_err("check valid secure storage failed with %d back\n", ret);
+		return ret;
+	}
+
+	return le16_to_cpu(oob.magic) == (u16)SEC_STO_MAGIC;
+}
+
+static int get_blocks_for_secure_storage(struct aw_spinand_sec_sto *sec_sto)
+{
+	struct aw_spinand_chip *chip = sec_sto->chip;
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	unsigned int i = 0, blk[2] = {0};
+	unsigned int start, end;
+	int ret;
+
+	start = sec_sto->startblk;
+	end = sec_sto->endblk;
+	/* find the frist two valid blocks */
+	for (; start < end; start++) {
+		req.block = start;
+
+		ret = ops->phy_is_bad(chip, &req);
+		if (ret == true)
+			continue;
+
+		blk[i++] = start;
+		if (i >= 2)
+			break;
+	}
+	if (i < 2) {
+		pr_err("no enough good blk between [%u %u) for secure storage\n",
+				start, end);
+		return -ENOSPC;
+	}
+
+	sec_sto->blk[0] = blk[0];
+	sec_sto->blk[1] = blk[1];
+	return 0;
+}
+
+static unsigned int secure_storage_checksum(char *_buf, unsigned int len)
+{
+	unsigned int sum = 0;
+	unsigned int i;
+	unsigned char *buf = (unsigned char *)_buf;
+
+	for (i = 0; i < len; i++)
+		sum += buf[i];
+	return sum;
+}
+
+static int read_check_secure_storage_page(struct aw_spinand_chip *chip,
+		unsigned int blk, int page, char *mbuf, unsigned int len,
+		struct sec_sto_oob *oob)
+{
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	struct sec_sto_oob _oob;
+	unsigned int checksum;
+	int ret;
+
+	req.block = blk;
+	req.page = page;
+	req.databuf = mbuf;
+	req.datalen = len;
+	req.oobbuf = oob ? oob : &_oob;
+	req.ooblen = sizeof(struct sec_sto_oob);
+
+	ret = ops->phy_read_page(chip, &req);
+	if (ret)
+		return ret;
+
+	checksum = secure_storage_checksum(mbuf, len);
+	if (oob)
+		return (unsigned int)oob->checksum != checksum;
+	else
+		return (unsigned int)_oob.checksum != checksum;
+}
+
+static int write_check_secure_storage_page(struct aw_spinand_chip *chip,
+		unsigned int blk, int page, char *mbuf, unsigned int len,
+		struct sec_sto_oob *oob)
+{
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	int ret;
+
+	req.block = blk;
+	req.page = page;
+	req.databuf = mbuf;
+	req.datalen = len;
+	req.oobbuf = oob;
+	req.ooblen = oob ? sizeof(struct sec_sto_oob) : 0;
+
+	if (req.page == 0) {
+		ret = ops->phy_erase_block(chip, &req);
+		if (ret)
+			return ret;
+	}
+
+	ret = ops->phy_write_page(chip, &req);
+	if (ret)
+		pr_err("write secure storage failed: %d\n", ret);
+
+	return ret;
+}
+
+#define BOTH_SEC_BLKS_GOOD 0
+#define FIRST_SEC_BLK_GOOD 1
+#define SECOND_SEC_BLK_GOOD 2
+static int check_secure_storage(struct aw_spinand_sec_sto *sec_sto)
+{
+	struct aw_spinand_chip *chip = sec_sto->chip;
+	struct aw_spinand_info *info = chip->info;
+	int ret, ret1, ret2, i, blk1, blk2;
+	unsigned int pagecnt;
+	char *buf;
+
+	buf = kmalloc(info->phy_page_size(chip), GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	blk1 = sec_sto->blk[0];
+	blk2 = sec_sto->blk[1];
+	pagecnt = info->phy_block_size(chip) / info->phy_page_size(chip);
+	for (i = 0; i < pagecnt; i++) {
+		ret1 = read_check_secure_storage_page(chip, blk1, i, buf,
+				info->phy_page_size(chip), NULL);
+		ret2 = read_check_secure_storage_page(chip, blk2, i, buf,
+				info->phy_page_size(chip), NULL);
+		if (ret1 != 0 || ret2 != 0)
+			break;
+	}
+
+	/*
+	 * retX negative: read failed
+	 * retX 0: read and check good
+	 * retX 1: read good but check failed
+	 */
+	if (ret1 == 0 && ret2 == 0)
+		ret = BOTH_SEC_BLKS_GOOD;
+	else if (ret1 == 0 && ret2 != 0)
+		ret = FIRST_SEC_BLK_GOOD;
+	else if (ret2 == 0 && ret1 != 0)
+		ret = SECOND_SEC_BLK_GOOD;
+	else if (ret1 == 1 && ret2 == 1)
+		ret = BOTH_SEC_BLKS_GOOD;
+	else
+		ret = -EINVAL;
+
+	if (ret < 0)
+		pr_err("nand secure storage check page %u fail\n", i);
+	kfree(buf);
+	return ret;
+}
+
+static int repair_secure_storage(struct aw_spinand_sec_sto *sec_sto,
+		int good_blk_index)
+{
+	struct aw_spinand_chip *chip = sec_sto->chip;
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	unsigned int from, to;
+	int ret;
+
+	if (good_blk_index == FIRST_SEC_BLK_GOOD) {
+		from = sec_sto->blk[0];
+		to = sec_sto->blk[1];
+	} else {
+		from = sec_sto->blk[1];
+		to = sec_sto->blk[0];
+	}
+
+	ret = ops->phy_copy_block(chip, from, to);
+	if (ret)
+		pr_err("copy phy block from %u to %u failed: %d\n", from, to, ret);
+	return ret;
+}
+
+static int init_secure_storage(struct aw_spinand_sec_sto *sec_sto, int update)
+{
+	int ret;
+
+	ret = get_blocks_for_secure_storage(sec_sto);
+	if (ret)
+		return ret;
+	pr_info("spinand secure storage ok for phy blk %u and %u\n",
+			sec_sto->blk[0], sec_sto->blk[1]);
+
+	/*
+	 * both of blk1 and blk2 has no valid data, it may be no any data had
+	 * been writen to secure storage.
+	 */
+	ret = is_secure_storage_block(sec_sto->chip, sec_sto->blk[0], 0);
+	if (ret == false) {
+		ret = is_secure_storage_block(sec_sto->chip, sec_sto->blk[1], 0);
+		if (ret == false) {
+			pr_info("secure storage blks have never used before\n");
+			goto end;
+		}
+	}
+
+	ret = check_secure_storage(sec_sto);
+	if (ret < 0) {
+		pr_err("check secure storage failed with %d\n", ret);
+		return ret;
+	} else if (ret > 0) {
+		pr_info("try to repair secure storage from block %u\n",
+				sec_sto->blk[ret - 1]);
+		ret = repair_secure_storage(sec_sto, ret);
+		if (ret) {
+			pr_err("repair secure storage failed with %d\n", ret);
+			return ret;
+		}
+	}
+end:
+	pr_debug("init secure storage ok\n");
+	sec_sto->init_end = true;
+	return 0;
+}
+
+int aw_spinand_secure_storage_read(struct aw_spinand_sec_sto *sec_sto,
+		int item, char *buf, unsigned int len)
+{
+	struct aw_spinand_chip *chip = sec_sto->chip;
+	struct aw_spinand_info *info = chip->info;
+	int ret = 0, pages_cnt_per_item, minlen, i;
+	char *pagebuf;
+	unsigned int page, pagesize;
+
+	pr_debug("try to read item %d with len %d\n", item, len);
+	if (!sec_sto || !sec_sto->chip)
+		return -EINVAL;
+
+	if (len % 1024) {
+		pr_err("secure storage need len (%d) align to 1024B\n", len);
+		return -EINVAL;
+	}
+
+	if (!is_init_end(sec_sto)) {
+		ret = init_secure_storage(sec_sto, 1);
+		if (ret)
+			return ret;
+	}
+
+	pagebuf = kzalloc(info->phy_page_size(chip), GFP_KERNEL);
+	if (!pagebuf) {
+		pr_err("no memory for reading secure storage page\n");
+		return -ENOMEM;
+	}
+
+	pagesize = info->phy_page_size(chip);
+	pages_cnt_per_item = DIV_ROUND_UP(len, pagesize);
+	for (i = 0; i < pages_cnt_per_item; i++) {
+		page = item * pages_cnt_per_item + i;
+
+		ret = read_check_secure_storage_page(chip, sec_sto->blk[0],
+				page, pagebuf, pagesize, NULL);
+		if (ret)
+			ret = read_check_secure_storage_page(chip,
+					sec_sto->blk[1], page, pagebuf,
+					pagesize, NULL);
+
+		if (ret < 0) {
+			pr_err("read secure storage page failed with %d back\n", ret);
+			break;
+		} else if (ret == true) {
+			pr_info("secure storage has no valid data on item %d\n", item);
+			break;
+		}
+
+		minlen = min(len, pagesize);
+		memcpy(buf + i * pagesize, pagebuf, minlen);
+		len -= minlen;
+	}
+
+	kfree(pagebuf);
+	return ret;
+}
+EXPORT_SYMBOL(aw_spinand_secure_storage_read);
+
+int aw_spinand_secure_storage_write(struct aw_spinand_sec_sto *sec_sto,
+		int item, char *buf, unsigned int len)
+{
+	struct aw_spinand_chip *chip = sec_sto->chip;
+	struct aw_spinand_info *info = chip->info;
+	int ret = 0, pages_cnt_per_item, minlen;
+	char *pagebuf;
+	struct sec_sto_oob oob;
+	unsigned int pagenum, pagesize, pagecnt;
+
+	pr_debug("try to write item %d with len %d\n", item, len);
+	if (!sec_sto || !sec_sto->chip)
+		return -EINVAL;
+
+	if (len % 1024) {
+		pr_err("secure storage need len (%d) align to 1024B\n", len);
+		return -EINVAL;
+	}
+
+	if (!is_init_end(sec_sto)) {
+		ret = init_secure_storage(sec_sto, 1);
+		if (ret)
+			return ret;
+	}
+
+	pagesize = info->phy_page_size(chip);
+	pagecnt = info->phy_block_size(chip) / info->phy_page_size(chip);
+	pagebuf = kzalloc(pagesize, GFP_KERNEL);
+	if (!pagebuf)
+		return -ENOMEM;
+
+	pages_cnt_per_item = DIV_ROUND_UP(len, pagesize);
+	for (pagenum = 0; pagenum < pagecnt; pagenum++) {
+		if (pagenum / pages_cnt_per_item == item) {
+			unsigned int off;
+
+			memset(pagebuf, 0xFF, pagesize);
+			minlen = min(len, pagesize);
+			off = pagesize * (pagenum % pages_cnt_per_item);
+			len -= minlen;
+
+			memcpy(pagebuf, buf + off, minlen);
+
+			oob.badflag = 0xFF;
+			oob.magic = cpu_to_le16(SEC_STO_MAGIC);
+			oob.checksum = secure_storage_checksum(pagebuf, pagesize);
+		} else {
+			memset(pagebuf, 0, pagesize);
+			ret = read_check_secure_storage_page(chip,
+					sec_sto->blk[0], pagenum,
+					pagebuf, pagesize, &oob);
+			if (ret < 0)
+				goto out;
+		}
+		ret = write_check_secure_storage_page(chip, sec_sto->blk[1],
+				pagenum, pagebuf, pagesize, &oob);
+		if (ret)
+			goto out;
+	}
+
+	ret = repair_secure_storage(sec_sto, SECOND_SEC_BLK_GOOD);
+	if (!ret)
+		pr_info("write secure storage itme %d ok\n", item);
+out:
+	kfree(pagebuf);
+	return ret;
+}
+EXPORT_SYMBOL(aw_spinand_secure_storage_write);
diff --git a/drivers/mtd/awnand/spinand/sunxi-common.c b/drivers/mtd/awnand/spinand/sunxi-common.c
new file mode 100644
index 000000000..d1add6ac3
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/sunxi-common.c
@@ -0,0 +1,73 @@
+/*
+ * sunxi-nftl-core.c for  sunxi spi nand base on nftl.
+ *
+ * Copyright (C) 2019 Allwinner.
+ * SPDX-License-Identifier: GPL-2.0
+ */
+
+#define pr_fmt(fmt) "sunxi-spinand: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/partitions.h>
+#include <linux/of.h>
+#include <linux/spi/spi.h>
+#include <linux/sunxi-sid.h>
+#include <linux/uaccess.h>
+
+#include "sunxi-spinand.h"
+
+static int ubootblks = -1;
+module_param(ubootblks, int, 0400);
+MODULE_PARM_DESC(ubootblks, "block count for uboot");
+
+void aw_spinand_uboot_blknum(struct aw_spinand *spinand, unsigned int *start,
+		unsigned int *end)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	unsigned int blksize = info->phy_block_size(chip);
+	unsigned int pagecnt = blksize / info->phy_page_size(chip);
+	unsigned int _start, _end;
+
+	/* small nand:block size < 1MB;  reserve 4M for uboot */
+	if (blksize <= SZ_128K) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 32;
+	} else if (blksize <= SZ_256K) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 16;
+	} else if (blksize <= SZ_512K) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 8;
+	} else if (blksize <= SZ_1M && pagecnt <= 128) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 4;
+		/* big nand;  reserve at least 20M for uboot */
+	} else if (blksize <= SZ_1M && pagecnt > 128) {
+		_start = UBOOT_START_BLOCK_BIGNAND;
+		_end = _start + 20;
+	} else if (blksize <= SZ_2M) {
+		_start = UBOOT_START_BLOCK_BIGNAND;
+		_end = _start + 10;
+	} else {
+		_start = UBOOT_START_BLOCK_BIGNAND;
+		_end = _start + 8;
+	}
+
+	if (ubootblks > 0)
+		_end = _start + ubootblks;
+	else
+		/* update parameter to /sys */
+		ubootblks = _end - _start;
+
+	if (start)
+		*start = _start;
+	if (end)
+		*end = _end;
+}
+EXPORT_SYMBOL(aw_spinand_uboot_blknum);
+
+MODULE_AUTHOR("cuizhikui <cuizhikui@allwinnertech.com>");
+MODULE_DESCRIPTION("Allwinner's spinand mtd and nftl driver common port");
diff --git a/drivers/mtd/awnand/spinand/sunxi-core.c b/drivers/mtd/awnand/spinand/sunxi-core.c
new file mode 100644
index 000000000..c979384ba
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/sunxi-core.c
@@ -0,0 +1,863 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/spi/spi.h>
+#include <linux/mtd/partitions.h>
+#include <linux/mtd/mtd.h>
+#include <linux/sunxi-sid.h>
+#include <linux/uaccess.h>
+
+#include "sunxi-spinand.h"
+
+struct aw_spinand *g_spinand;
+
+static int ubootblks = -1;
+module_param(ubootblks, int, 0400);
+MODULE_PARM_DESC(ubootblks, "block count for uboot");
+
+struct aw_spinand *get_aw_spinand(void)
+{
+	return g_spinand;
+}
+
+static void aw_spinand_cleanup(struct aw_spinand *spinand)
+{
+	aw_spinand_chip_exit(&spinand->chip);
+}
+
+static int aw_spinand_erase(struct mtd_info *mtd, struct erase_info *einfo)
+{
+	unsigned int len;
+	int ret;
+	struct aw_spinand_chip_request req = {0};
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_info *info = chip->info;
+
+	pr_debug("calling erase\n");
+
+	len = (unsigned int)einfo->len;
+	ret = addr_to_req(chip, &req, einfo->addr);
+	if (ret)
+		return ret;
+
+	while (len) {
+		mutex_lock(&spinand->lock);
+		ret = ops->erase_block(chip, &req);
+		mutex_unlock(&spinand->lock);
+
+		if (ret) {
+			einfo->state = MTD_ERASE_FAILED;
+			einfo->fail_addr = einfo->addr;
+			pr_err("erase block %u in addr 0x%x failed: %d\n",
+					req.block, (unsigned int)einfo->addr,
+					ret);
+			return ret;
+		}
+
+		req.block++;
+		len -= info->block_size(chip);
+	}
+
+	einfo->state = MTD_ERASE_DONE;
+	return 0;
+}
+
+static void aw_spinand_init_mtd_info(struct aw_spinand_chip *chip,
+		struct mtd_info *mtd)
+{
+	struct aw_spinand_info *info = chip->info;
+
+	mtd->type = MTD_NANDFLASH;
+	mtd->flags = MTD_CAP_NANDFLASH;
+	mtd->owner = THIS_MODULE;
+	mtd->erasesize = info->block_size(chip);
+	mtd->writesize = info->page_size(chip);
+	mtd->oobsize = info->oob_size(chip);
+#if IS_ENABLED(CONFIG_AW_SPINAND_SIMULATE_MULTIPLANE)
+	mtd->oobavail = AW_OOB_SIZE_PER_PHY_PAGE * 2;
+	mtd->subpage_sft = 1;
+#else
+	mtd->oobavail = AW_OOB_SIZE_PER_PHY_PAGE;
+	mtd->subpage_sft = 0;
+#endif
+	mtd->writebufsize = info->page_size(chip);
+	mtd->size = info->total_size(chip);
+	mtd->name = "sunxi_mtd_nand";
+	mtd->ecc_strength = ECC_ERR;
+	mtd->bitflip_threshold = ECC_LIMIT;
+}
+
+static inline void aw_spinand_req_init(struct aw_spinand *spinand,
+		loff_t offs, struct mtd_oob_ops *mtd_ops,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	struct mtd_info *mtd = spinand_to_mtd(spinand);
+
+	addr_to_req(chip, req, offs);
+	req->databuf = mtd_ops->datbuf;
+	req->oobbuf = mtd_ops->oobbuf;
+	req->pageoff = offs & (info->page_size(chip) - 1);
+	/* read size once */
+	req->ooblen = min_t(unsigned int, mtd_ops->ooblen, mtd->oobavail);
+	req->datalen = min(info->page_size(chip) - req->pageoff,
+			(unsigned int)(mtd_ops->len));
+	/* the total size to read */
+	req->dataleft = mtd_ops->len;
+	req->oobleft = mtd_ops->ooblen;
+}
+
+static inline void aw_spinand_req_next(struct aw_spinand *spinand,
+		struct aw_spinand_chip_request *req)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	struct mtd_info *mtd = spinand_to_mtd(spinand);
+	unsigned int pages_per_blk;
+	const char *str;
+
+	pages_per_blk = info->block_size(chip) >> spinand->page_shift;
+	req->page++;
+	if (req->page >= pages_per_blk) {
+		req->page = 0;
+		req->block++;
+	}
+
+	if (req->dataleft < req->datalen) {
+		str = "[phy]request: dataleft < datalen";
+		goto bug;
+	}
+
+	if (req->oobleft < req->ooblen) {
+		str = "[phy]request: oobleft < ooblen";
+		goto bug;
+	}
+
+	req->dataleft -= req->datalen;
+	req->databuf += req->datalen;
+	req->oobleft -= req->ooblen;
+	req->oobbuf += req->ooblen;
+	req->pageoff = 0;
+	req->datalen = min(info->page_size(chip), req->dataleft);
+	req->ooblen = min_t(unsigned int, req->oobleft, mtd->oobavail);
+	return;
+
+bug:
+	aw_spinand_reqdump(pr_err, str, req);
+	WARN_ON(1);
+	return;
+}
+
+static inline bool aw_spinand_req_end(struct aw_spinand *spinand,
+		struct aw_spinand_chip_request *req)
+{
+	if (req->dataleft || req->oobleft)
+		return false;
+	return true;
+}
+
+/**
+ * aw_spinand_for_each_req - Iterate over all NAND pages contained in an
+ *				MTD I/O request
+ * @spinand: SPINAND device
+ * @start: start address to read/write from
+ * @mtd_ops: MTD I/O request
+ * @req: sunxi chip request
+ *
+ * Should be used for iterate over pages that are contained in an MTD request
+ */
+#define aw_spinand_for_each_req(spinand, start, mtd_ops, req)	\
+	for (aw_spinand_req_init(spinand, start, mtd_ops, req);	\
+		!aw_spinand_req_end(spinand, req);			\
+		aw_spinand_req_next(spinand, req))
+
+static int aw_spinand_read_oob(struct mtd_info *mtd, loff_t from,
+			    struct mtd_oob_ops *ops)
+{
+	int ret = 0;
+	unsigned int max_bitflips = 0;
+	struct aw_spinand_chip_request req = {0};
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *chip_ops = chip->ops;
+	bool ecc_failed = false;
+
+	if (from < 0 || from >= mtd->size || ops->len > mtd->size - from)
+		return -EINVAL;
+	if (!ops->len)
+		return 0;
+
+	mutex_lock(&spinand->lock);
+
+	pr_debug("calling read with oob: from 0x%llx datalen %lu ooblen %lu\n",
+			from, (unsigned long)ops->len, (unsigned long)ops->ooblen);
+
+	aw_spinand_for_each_req(spinand, from, ops, &req) {
+		aw_spinand_reqdump(pr_debug, "do super read", &req);
+
+		ret = chip_ops->read_page(chip, &req);
+		if (ret < 0) {
+			pr_err("read single page failed: %d\n", ret);
+			break;
+		} else if (ret == ECC_LIMIT) {
+			ret = mtd->bitflip_threshold;
+			mtd->ecc_stats.corrected += ret;
+			max_bitflips = max_t(unsigned int, max_bitflips, ret);
+			pr_debug("ecc limit: block: %u page: %u\n",
+					req.block, req.page);
+		} else if (ret == ECC_ERR) {
+			ecc_failed = true;
+			mtd->ecc_stats.failed++;
+			pr_err("ecc err: block: %u page: %u\n",
+					req.block, req.page);
+		}
+
+		ret = 0;
+		ops->retlen += req.datalen;
+		ops->oobretlen += req.ooblen;
+	}
+	mutex_unlock(&spinand->lock);
+
+	if (ecc_failed && !ret)
+		ret = -EBADMSG;
+
+	pr_debug("exitng read with oob\n");
+	return ret ? ret : max_bitflips;
+}
+
+static int aw_spinand_read(struct mtd_info *mtd,
+	loff_t from, size_t len, size_t *retlen, uint8_t *buf)
+{
+	struct mtd_oob_ops ops = {0};
+	int ret;
+
+	pr_debug("calling read\n");
+
+	ops.len = len;
+	ops.datbuf = buf;
+	ops.oobbuf = NULL;
+	ops.ooblen = ops.ooboffs = ops.oobretlen = 0;
+	ret = aw_spinand_read_oob(mtd, from, &ops);
+	*retlen = ops.retlen;
+	pr_debug("exiting read\n");
+	return ret;
+}
+
+static int aw_spinand_write_oob(struct mtd_info *mtd, loff_t to,
+			     struct mtd_oob_ops *ops)
+{
+	int ret = 0;
+	struct aw_spinand_chip_request req = {0};
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_chip_ops *chip_ops = chip->ops;
+
+	if (to < 0 || to >= mtd->size || ops->len > mtd->size - to)
+		return -EOVERFLOW;
+	if (!ops->len)
+		return 0;
+	/*
+	 * if enable CONFIG_AW_SPINAND_SIMULATE_MULTIPLANE, will eanble
+	 * subpage too. This means that ubi will write header to physic block
+	 * page. So, we should check alignment for physic page rather super page.
+	 */
+	if (ops->len & (info->phy_page_size(chip) - 1))
+		return -EINVAL;
+
+	mutex_lock(&spinand->lock);
+
+	pr_debug("calling write with oob: to 0x%llx datalen %lu ooblen %lu\n",
+			to, (unsigned long)ops->len, (unsigned long)ops->ooblen);
+
+	aw_spinand_for_each_req(spinand, to, ops, &req) {
+		aw_spinand_reqdump(pr_debug, "do super write", &req);
+
+		ret = chip_ops->write_page(chip, &req);
+		if (ret < 0) {
+			pr_err("write single page failed: block %d, page %d, ret %d\n",
+					req.block, req.page, ret);
+			break;
+		}
+
+		ops->retlen += req.datalen;
+		ops->oobretlen += req.ooblen;
+	}
+	mutex_unlock(&spinand->lock);
+
+	pr_debug("exiting write with oob\n");
+	return ret;
+}
+
+static int aw_spinand_write(struct mtd_info *mtd, loff_t to,
+	size_t len, size_t *retlen, const u_char *buf)
+{
+	struct mtd_oob_ops ops = {0};
+	int ret;
+
+	pr_debug("calling write\n");
+
+	ops.len = len;
+	ops.datbuf = (uint8_t *)buf;
+	ops.oobbuf = NULL;
+	ops.ooblen = ops.ooboffs = ops.oobretlen = 0;
+	ret = aw_spinand_write_oob(mtd, to, &ops);
+	*retlen = ops.retlen;
+
+	pr_debug("exiting write\n");
+	return ret;
+}
+
+static int aw_spinand_block_isbad(struct mtd_info *mtd, loff_t offs)
+{
+	int ret;
+	struct aw_spinand_chip_request req = {0};
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *ops = chip->ops;
+
+	pr_debug("calling isbad\n");
+
+	ret = addr_to_req(chip, &req, offs);
+	if (ret)
+		return ret;
+
+	mutex_lock(&spinand->lock);
+	ret = ops->is_bad(chip, &req);
+	mutex_unlock(&spinand->lock);
+	pr_debug("exting isbad: block %u is %s\n", req.block,
+			ret == true ? "bad" : "good");
+	return ret;
+}
+
+static int aw_spinand_block_markbad(struct mtd_info *mtd, loff_t offs)
+{
+	int ret;
+	struct aw_spinand_chip_request req = {0};
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *ops = chip->ops;
+
+	pr_debug("calling markbad\n");
+
+	ret = addr_to_req(chip, &req, offs);
+	if (ret)
+		return ret;
+
+	mutex_lock(&spinand->lock);
+	ret = ops->mark_bad(chip, &req);
+	mutex_unlock(&spinand->lock);
+	pr_info("exting markbad: mark block %d as bad\n", req.block);
+	return ret;
+}
+
+/* write when on panic */
+static int aw_spinand_panic_write(struct mtd_info *mtd, loff_t to,
+		size_t len, size_t *retlen, const u_char *buf)
+{
+	return 0;
+}
+
+static int aw_spinand_mtd_init(struct aw_spinand *spinand)
+{
+	struct mtd_info *mtd = &spinand->mtd;
+	struct aw_spinand_chip *chip = &spinand->chip;
+	struct spi_device *spi = chip->spi;
+
+	spi_set_drvdata(spi, spinand);
+	mtd = spinand_to_mtd(spinand);
+	mtd_set_of_node(mtd, spi->dev.of_node);
+	mtd->dev.parent = &spi->dev;
+
+	aw_spinand_init_mtd_info(chip, mtd);
+
+	mtd->_erase = aw_spinand_erase;
+	mtd->_read = aw_spinand_read;
+	mtd->_read_oob = aw_spinand_read_oob;
+	mtd->_write = aw_spinand_write;
+	mtd->_write_oob = aw_spinand_write_oob;
+	mtd->_block_isbad = aw_spinand_block_isbad;
+	mtd->_block_markbad = aw_spinand_block_markbad;
+	mtd->_panic_write = aw_spinand_panic_write;
+
+	return 0;
+}
+
+void aw_spinand_uboot_blknum(struct aw_spinand *spinand,
+		unsigned int *start, unsigned int *end)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	unsigned int blksize = info->phy_block_size(chip);
+	unsigned int pagecnt = blksize / info->phy_page_size(chip);
+	unsigned int _start, _end;
+
+	/* small nand:block size < 1MB;  reserve 4M for uboot */
+	if (blksize <= SZ_128K) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 32;
+	} else if (blksize <= SZ_256K) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 16;
+	} else if (blksize <= SZ_512K) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 8;
+	} else if (blksize <= SZ_1M && pagecnt <= 128) {
+		_start = UBOOT_START_BLOCK_SMALLNAND;
+		_end = _start + 4;
+	/* big nand;  reserve at least 20M for uboot */
+	} else if (blksize <= SZ_1M && pagecnt > 128) {
+		_start = UBOOT_START_BLOCK_BIGNAND;
+		_end = _start + 20;
+	} else if (blksize <= SZ_2M) {
+		_start = UBOOT_START_BLOCK_BIGNAND;
+		_end = _start + 10;
+	} else {
+		_start = UBOOT_START_BLOCK_BIGNAND;
+		_end = _start + 8;
+	}
+
+	if (ubootblks > 0)
+		_end = _start + ubootblks;
+	else
+		/* update parameter to /sys */
+		ubootblks = _end - _start;
+
+	if (start)
+		*start = _start;
+	if (end)
+		*end = _end;
+}
+
+int aw_spinand_fill_phy_info(struct aw_spinand_chip *chip,
+		void *data);
+
+/*
+ * this function is used to fix for aw boot0
+ * we do not make a new way for ubi on boot0, to fix for the old way, we
+ * have to fill boot_spinand_para_t for boot0 header
+ */
+int aw_spinand_mtd_get_flash_info(struct aw_spinand *spinand,
+		void *data, unsigned int len)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	boot_spinand_para_t *boot_info = data;
+	unsigned int uboot_start, uboot_end;
+
+	aw_spinand_uboot_blknum(spinand, &uboot_start, &uboot_end);
+
+	aw_spinand_fill_phy_info(chip, data);
+
+	boot_info->uboot_start_block = uboot_start;
+	boot_info->uboot_next_block = uboot_end;
+	boot_info->logic_start_block = uboot_end;
+	boot_info->nand_specialinfo_page = uboot_end;
+	boot_info->nand_specialinfo_offset = uboot_end;
+	boot_info->physic_block_reserved = 0;
+
+	return 0;
+}
+
+/**
+ * do download boot data to flash
+ *
+ * @startblk: the block to downlaod
+ * @endblk: the end block to downlaod [start, end)
+ * @pagesize: data size to write to each page, 0 means the whole page
+ * @buf: data buffer
+ * @len: length of buf
+ *
+ * return the blocks count written including the bad blocks.
+ * return negative number if error.
+ */
+static int download_boot(struct mtd_info *mtd,
+		unsigned int startblk, unsigned int endblk,
+		unsigned int pagesize, void *buf, unsigned int len)
+{
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_chip_request req = {0};
+	unsigned int blksize = info->phy_block_size(chip);
+	unsigned int pagecnt = blksize / info->phy_page_size(chip);
+	unsigned int written_blks = 0;
+	int ret, pgindex = 0;
+
+	/* check boundary */
+	pagesize = pagesize ? pagesize : info->phy_page_size(chip);
+	if (pagesize > info->phy_page_size(chip)) {
+		pagesize = info->phy_page_size(chip);
+		pr_warn("reset download boot pagesize to %d\n", pagesize);
+	}
+	/* download */
+	req.block = startblk;
+	req.page = 0;
+	do {
+		if (req.page == 0) {
+			/* must check bad and do erase for new block */
+			mutex_lock(&spinand->lock);
+			ret = ops->phy_is_bad(chip, &req);
+			mutex_unlock(&spinand->lock);
+			if (ret == true) {
+				pr_info("skip bad blk %d for boot, try next blk %d\n",
+						req.block, req.block + 1);
+				req.block++;
+				written_blks++;
+				if (req.block >= endblk) {
+					pr_info("achieve maximum blocks %d\n",
+							endblk);
+					return written_blks;
+				}
+				/* continue to check bad blk before erase */
+				continue;
+			}
+			mutex_lock(&spinand->lock);
+			ret = ops->phy_erase_block(chip, &req);
+			mutex_unlock(&spinand->lock);
+			if (ret) {
+				pr_err("erase blk %d failed\n", req.block);
+				return ret;
+			}
+		}
+
+		req.datalen = min(len, pagesize);
+		/*
+		 * only on the last time, datalen may be less than pagesize,
+		 * to calculate the offset, we should use pagesize * pgindex
+		 */
+		req.databuf = buf + pgindex * pagesize;
+
+		mutex_lock(&spinand->lock);
+		ret = ops->phy_write_page(chip, &req);
+		mutex_unlock(&spinand->lock);
+		if (ret) {
+			pr_err("write boot to blk %d page %d failed\n",
+					req.block, req.page);
+			return ret;
+		}
+		if (req.page == 0)
+			written_blks++;
+
+		pgindex++;
+		len -= req.datalen;
+		req.page++;
+		if (req.page >= pagecnt) {
+			req.page = 0;
+			req.block++;
+		}
+	} while (len > 0);
+
+	return written_blks;
+}
+
+static int gen_check_sum(void *boot_buf)
+{
+	standard_boot_file_head_t *head_p;
+	unsigned int length;
+	unsigned int *buf;
+	unsigned int loop;
+	unsigned int i;
+	unsigned int sum;
+	unsigned int *p;
+	toc0_private_head_t *toc0_head;
+
+	if (sunxi_soc_is_secure()) {
+		/* secure */
+		toc0_head = (toc0_private_head_t *) boot_buf;
+		length = toc0_head->length;
+		p = &(toc0_head->check_sum);
+	} else {
+		head_p = (standard_boot_file_head_t *) boot_buf;
+		length = head_p->length;
+		p = &(head_p->check_sum);
+	}
+
+	if ((length & 0x3) != 0)	/* must 4-byte-aligned */
+		return -1;
+
+	buf = (unsigned int *)boot_buf;
+	*p = STAMP_VALUE;	/* fill stamp */
+	loop = length >> 2;
+
+	for (i = 0, sum = 0; i < loop; i++)
+		sum += buf[i];
+
+	*p = sum;
+	return 0;
+}
+
+static int get_nand_para(struct aw_spinand *spinand, void *boot_buf)
+{
+	boot0_file_head_t *boot0_buf;
+	char *data_buf;
+	void *nand_para;
+	sbrom_toc0_config_t *secure_toc0_buf;
+
+	if (sunxi_soc_is_secure()) {
+		/* secure */
+		secure_toc0_buf =
+		    (sbrom_toc0_config_t *) (boot_buf + SBROM_TOC0_HEAD_SPACE);
+		data_buf = secure_toc0_buf->storage_data;
+		nand_para = (void *)data_buf;
+	} else {
+		/* nonsecure */
+		boot0_buf = (boot0_file_head_t *) boot_buf;
+		data_buf = boot0_buf->prvt_head.storage_data;
+		nand_para = (void *)data_buf;
+	}
+
+	aw_spinand_mtd_get_flash_info(spinand, nand_para, STORAGE_BUFFER_SIZE);
+
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+int aw_spinand_mtd_read_secure_storage(struct mtd_info *mtd,
+		int item, void *buf, unsigned int len)
+{
+	int ret;
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+
+	mutex_lock(&spinand->lock);
+	ret = aw_spinand_secure_storage_read(&spinand->sec_sto,
+		item, buf, len);
+	mutex_unlock(&spinand->lock);
+
+	return ret;
+}
+
+int aw_spinand_mtd_write_secure_storage(struct mtd_info *mtd,
+		int item, void *buf, unsigned int len)
+{
+	int ret;
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+
+	mutex_lock(&spinand->lock);
+	ret = aw_spinand_secure_storage_write(&spinand->sec_sto,
+		item, buf, len);
+	mutex_unlock(&spinand->lock);
+
+	return ret;
+}
+#endif
+
+int aw_spinand_mtd_download_boot0(struct mtd_info *mtd,
+		unsigned int len, void *buf)
+{
+	unsigned int start, end;
+	int ret = 0;
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+
+	get_nand_para(spinand, buf);
+	gen_check_sum(buf);
+
+	start = NAND_BOOT0_BLK_START;
+	/* start addr of uboot is the end addr of boot0 */
+	aw_spinand_uboot_blknum(spinand, &end, NULL);
+
+	/* In general, size of boot0 is less than a block */
+	while (start < end) {
+		pr_info("download boot0 to block %d len %dK\n",
+				start, len / SZ_1K);
+		ret = download_boot(mtd, start, end,
+				VALID_PAGESIZE_FOR_BOOT0,
+				buf, len);
+		if (ret <= 0) {
+			pr_err("download boot0 to blk %d failed\n",
+					start);
+			break;
+		} else if (ret > 0) {
+			/* means had written @ret blks */
+			start += ret;
+		} else {
+			start++;
+		}
+	}
+
+	return 0;
+}
+
+int aw_spinand_mtd_download_uboot(struct mtd_info *mtd,
+		unsigned int len, void *buf)
+{
+	struct aw_spinand *spinand = mtd_to_spinand(mtd);
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	unsigned int phy_blk_size;
+	unsigned int start, end, blks_per_uboot;
+	int blks_written = 0;
+
+	/* [start, end) */
+	aw_spinand_uboot_blknum(spinand, &start, &end);
+	phy_blk_size = info->phy_block_size(chip);
+	blks_per_uboot = (len + phy_blk_size - 1) / phy_blk_size;
+
+	if (end - start < blks_per_uboot) {
+		pr_err("no enough space for at least one uboot\n");
+		pr_err("block per uboot is %u, however the uboot blks range is only [%u,%u)\n",
+				blks_per_uboot, start, end);
+		return -ENOSPC;
+	}
+
+	pr_info("uboot blk range [%u-%u)\n", start, end);
+	if (len % info->page_size(chip)) {
+		pr_err("len (%u) of uboot must align to pagesize %u\n",
+				len, info->page_size(chip));
+		return -EINVAL;
+	}
+
+	while (start + blks_per_uboot <= end) {
+		pr_info("download uboot to block %u (%u blocks) len %uK\n",
+				start, blks_per_uboot, len / SZ_1K);
+		blks_written = download_boot(mtd, start, end, 0, buf, len);
+		if (blks_written <= 0) {
+			pr_err("download uboot to blk %u failed\n", start);
+			return blks_written;
+		}
+		if (blks_written < blks_per_uboot) {
+			pr_err("something error, written %u blks but wanted %u blks\n",
+					blks_written, blks_per_uboot);
+			return -EINVAL;
+		}
+		start += blks_written;
+	}
+
+	return 0;
+}
+
+static struct mtd_partition aw_spinand_parts[] = {
+	/* .size is set by @aw_spinand_mtd_update_mtd_parts */
+	{ .name = "boot0", .offset = 0 },
+	{ .name = "uboot", .offset = MTDPART_OFS_APPEND },
+	{ .name = "secure_storage", .offset = MTDPART_OFS_APPEND },
+#if IS_ENABLED(CONFIG_AW_SPINAND_PSTORE_MTD_PART)
+	{ .name = "pstore", .offset = MTDPART_OFS_APPEND },
+#endif
+	{ .name = "sys", .offset = MTDPART_OFS_APPEND},
+};
+
+static void aw_spinand_mtd_update_mtd_parts(struct aw_spinand *spinand,
+		struct mtd_partition *mtdparts)
+{
+	struct aw_spinand_chip *chip = &spinand->chip;
+	struct aw_spinand_info *info = chip->info;
+	unsigned int blk_bytes = info->phy_block_size(chip);
+	unsigned int uboot_start, uboot_end;
+	int index = 0;
+
+	aw_spinand_uboot_blknum(spinand, &uboot_start, &uboot_end);
+	/* boot0 */
+	mtdparts[index++].size = uboot_start * blk_bytes;
+	/* uboot */
+	mtdparts[index++].size = (uboot_end - uboot_start) * blk_bytes;
+	/* secure storage */
+	mtdparts[index++].size = PHY_BLKS_FOR_SECURE_STORAGE * blk_bytes;
+#if IS_ENABLED(CONFIG_AW_SPINAND_PSTORE_MTD_PART)
+	/* pstore */
+	mtdparts[index++].size = PSTORE_SIZE_KB * SZ_1K;
+#endif
+	/* user data */
+	mtdparts[index++].size = MTDPART_SIZ_FULL;
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+	spinand->sec_sto.chip = spinand_to_chip(spinand);
+	spinand->sec_sto.startblk = uboot_end;
+	spinand->sec_sto.endblk = uboot_end + PHY_BLKS_FOR_SECURE_STORAGE;
+#endif
+}
+
+static int aw_spinand_probe(struct spi_device *spi)
+{
+	struct aw_spinand *spinand;
+	struct aw_spinand_chip *chip;
+	int ret;
+
+	if (g_spinand) {
+		pr_info("AW Spinand already initialized\n");
+		return -EBUSY;
+	}
+
+	pr_info("AW SPINand MTD Layer Version: %x.%x %x\n",
+			AW_MTD_SPINAND_VER_MAIN, AW_MTD_SPINAND_VER_SUB,
+			AW_MTD_SPINAND_VER_DATE);
+
+	spinand = devm_kzalloc(&spi->dev, sizeof(*spinand), GFP_KERNEL);
+	if (!spinand)
+		return -ENOMEM;
+	chip = spinand_to_chip(spinand);
+	mutex_init(&spinand->lock);
+
+	ret = aw_spinand_chip_init(spi, chip);
+	if (ret)
+		return ret;
+
+	spinand->sector_shift = ffs(chip->info->sector_size(chip)) - 1;
+	spinand->page_shift = ffs(chip->info->page_size(chip)) - 1;
+	spinand->block_shift = ffs(chip->info->block_size(chip)) - 1;
+
+	ret = aw_spinand_mtd_init(spinand);
+	if (ret)
+		goto err_spinand_cleanup;
+
+	aw_spinand_mtd_update_mtd_parts(spinand, aw_spinand_parts);
+	ret = mtd_device_register(&spinand->mtd, aw_spinand_parts,
+			ARRAY_SIZE(aw_spinand_parts));
+	if (ret)
+		goto err_spinand_cleanup;
+
+	g_spinand = spinand;
+	return 0;
+
+err_spinand_cleanup:
+	aw_spinand_cleanup(spinand);
+	return ret;
+}
+
+static int aw_spinand_remove(struct spi_device *spi)
+{
+	int ret;
+	struct aw_spinand *spinand;
+	struct mtd_info *mtd;
+
+	spinand = spi_to_spinand(spi);
+	mtd = spinand_to_mtd(spinand);
+
+	ret = mtd_device_unregister(mtd);
+	if (ret)
+		return ret;
+
+	aw_spinand_cleanup(spinand);
+	return 0;
+}
+
+static const struct spi_device_id aw_spinand_ids[] = {
+	{ .name = "spi-nand" },
+	{ /* sentinel */ },
+};
+
+static const struct of_device_id aw_spinand_of_ids[] = {
+	{ .compatible = "spi-nand" },
+	{ /* sentinel */ },
+};
+
+static struct spi_driver aw_spinand_drv = {
+	.driver = {
+		.name = "spi-nand",
+		.of_match_table = of_match_ptr(aw_spinand_of_ids),
+	},
+	.id_table = aw_spinand_ids,
+	.probe = aw_spinand_probe,
+	.remove = aw_spinand_remove,
+};
+module_spi_driver(aw_spinand_drv);
+
+MODULE_AUTHOR("liaoweixiong <liaoweixiong@allwinnertech.com>");
+MODULE_DESCRIPTION("Allwinner's spinand driver");
diff --git a/drivers/mtd/awnand/spinand/sunxi-debug.c b/drivers/mtd/awnand/spinand/sunxi-debug.c
new file mode 100644
index 000000000..6594116d0
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/sunxi-debug.c
@@ -0,0 +1,284 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#define pr_fmt(fmt) "sunxi-spinand: " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sysfs.h>
+#include <linux/mtd/aw-spinand.h>
+#include <linux/mtd/partitions.h>
+
+#include "sunxi-spinand.h"
+
+static ssize_t aw_spinand_sysfs_show(struct kobject *kobj,
+		struct attribute *attr, char *buf);
+static ssize_t aw_spinand_sysfs_store(struct kobject *kobj,
+		struct attribute *attr, const char *buf, size_t count);
+static ssize_t aw_spinand_show_arch(struct aw_spinand *spinand, char *buf);
+static ssize_t aw_spinand_show_nanddbg(struct aw_spinand *spinand, char *buf);
+static ssize_t aw_spinand_show_version(struct aw_spinand *spinand, char *buf);
+static ssize_t aw_spinand_show_badblk(struct aw_spinand *spinand, char *buf);
+
+static struct attribute attr_debug = {
+	.name = "nand_debug",
+	.mode = S_IRUGO,
+};
+
+static struct attribute attr_arch = {
+	.name = "arch",
+	.mode = S_IRUGO,
+};
+
+static struct attribute attr_badblk = {
+	.name = "badblock",
+	.mode = S_IRUGO,
+};
+
+static struct attribute attr_version = {
+	.name = "version",
+	.mode = S_IRUGO,
+};
+
+static struct attribute *sysfs_attrs[] = {
+	&attr_debug,
+	&attr_arch,
+	&attr_badblk,
+	&attr_version,
+	NULL,
+};
+
+struct aw_spinand_attr {
+	struct attribute *attr;
+	ssize_t (*show)(struct aw_spinand *spinand, char *buf);
+	ssize_t (*store)(struct aw_spinand *spinand, const char *buf,
+			size_t cnt);
+};
+
+static struct aw_spinand_attr attr_ops_array[] = {
+	{
+		.attr = &attr_debug,
+		.show = aw_spinand_show_nanddbg,
+	},
+	{
+		.attr = &attr_arch,
+		.show = aw_spinand_show_arch,
+	},
+	{
+		.attr = &attr_version,
+		.show = aw_spinand_show_version,
+	},
+	{
+		.attr = &attr_badblk,
+		.show = aw_spinand_show_badblk,
+	},
+};
+
+static const struct sysfs_ops sysfs_ops = {
+	.show = aw_spinand_sysfs_show,
+	.store = aw_spinand_sysfs_store,
+};
+
+static struct kobj_type sysfs_type = {
+	.sysfs_ops = &sysfs_ops,
+	.default_attrs = sysfs_attrs,
+};
+
+static struct kobject aw_spinand_debug_kobj;
+
+static int __init aw_spinand_debug_init(void)
+{
+	int ret;
+	struct aw_spinand *spinand = get_aw_spinand();
+
+	if (!spinand)
+		return -EBUSY;
+
+	ret = kobject_init_and_add(&aw_spinand_debug_kobj, &sysfs_type, NULL,
+				"nand_driver0");
+	if (ret) {
+		pr_err("init nand sysfs fail!\n");
+		return ret;
+	}
+	return 0;
+}
+module_init(aw_spinand_debug_init);
+
+static void __exit aw_spinand_debug_exit(void)
+{
+	struct aw_spinand *spinand = get_aw_spinand();
+
+	if (spinand)
+		kobject_del(&aw_spinand_debug_kobj);
+}
+module_exit(aw_spinand_debug_exit);
+
+static ssize_t aw_spinand_sysfs_show(struct kobject *kobj, struct attribute *attr,
+		char *buf)
+{
+	int index;
+	struct aw_spinand_attr *spinand_attr;
+	struct aw_spinand *spinand = get_aw_spinand();
+
+	BUG_ON(!spinand);
+
+	for (index = 0; index < ARRAY_SIZE(attr_ops_array); index++) {
+		spinand_attr = &attr_ops_array[index];
+		if (attr == spinand_attr->attr)
+			break;
+	}
+
+	if (unlikely(index == ARRAY_SIZE(attr_ops_array))) {
+		pr_err("not found attr_ops for %s\n", attr->name);
+		return -EINVAL;
+	}
+
+	if (spinand_attr->show)
+		return spinand_attr->show(spinand, buf);
+	return -EINVAL;
+}
+
+static ssize_t aw_spinand_sysfs_store(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count)
+{
+	int index;
+	struct aw_spinand_attr *spinand_attr;
+	struct aw_spinand *spinand = get_aw_spinand();
+
+	BUG_ON(!spinand);
+
+	for (index = 0; index < ARRAY_SIZE(attr_ops_array); index++) {
+		spinand_attr = &attr_ops_array[index];
+		if (attr == spinand_attr->attr)
+			break;
+	}
+
+	if (unlikely(index == ARRAY_SIZE(attr_ops_array))) {
+		pr_err("not found attr_ops for %s\n", attr->name);
+		return -EINVAL;
+	}
+
+	if (spinand_attr->store)
+		return spinand_attr->store(spinand, buf, count);
+	return -EINVAL;
+}
+
+static ssize_t aw_spinand_show_arch(struct aw_spinand *spinand, char *buf)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	unsigned char ids[MAX_ID_LEN];
+	unsigned int id, pagesize;
+	ssize_t ret = 0;
+
+	pagesize = 1 << PAGE_SHIFT;
+	info->nandid(chip, ids, MAX_ID_LEN);
+	id = ids[0] << 0 | ids[1] << 8 | ids[2] << 16 | ids[3] << 24;
+
+	ret += snprintf(buf + ret, pagesize - ret, "Model: %s\n",
+			info->model(chip));
+	ret += snprintf(buf + ret, pagesize - ret, "NandID: 0x%x\n", id);
+	ret += snprintf(buf + ret, pagesize - ret, "Size: %dM\n",
+			info->total_size(chip) / SZ_1M);
+	ret += snprintf(buf + ret, pagesize - ret, "DieCntPerChip: %d\n",
+			info->die_cnt(chip));
+	ret += snprintf(buf + ret, pagesize - ret, "SectCntPerPage: %d\n",
+			info->phy_page_size(chip) >> SECTOR_SHIFT);
+	ret += snprintf(buf + ret, pagesize - ret, "PageCntPerBlk: %d\n",
+			info->phy_block_size(chip) / info->phy_page_size(chip));
+	ret += snprintf(buf + ret, pagesize - ret, "BlkCntPerDie: %d\n",
+			info->total_size(chip) / info->phy_block_size(chip));
+	ret += snprintf(buf + ret, pagesize - ret, "OperationOpt: 0x%x\n",
+			info->operation_opt(chip));
+	ret += snprintf(buf + ret, pagesize - ret, "MaxEraseTimes: %d\n",
+			info->max_erase_times(chip));
+	ret += snprintf(buf + ret, pagesize - ret, "Manufacture: %s\n",
+			info->manufacture(chip));
+	return ret;
+}
+
+static ssize_t aw_spinand_show_version(struct aw_spinand *spinand, char *buf)
+{
+	return snprintf(buf, 1 << PAGE_SHIFT, "%x.%x %x\n",
+			AW_MTD_SPINAND_VER_MAIN, AW_MTD_SPINAND_VER_SUB,
+			AW_MTD_SPINAND_VER_DATE);
+}
+
+/*
+ * It is ok to loop for much times to check bad block, because spinand has bad
+ * block table on ddr. It will not take so long.
+ */
+static ssize_t aw_spinand_show_badblk(struct aw_spinand *spinand, char *buf)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	unsigned int cnt = 0, blk, blkcnt, pagesize;
+	ssize_t ret = 0;
+
+	pagesize = 1 << PAGE_SHIFT;
+	blkcnt = info->total_size(chip) / info->block_size(chip);
+
+	/* logic bad block count */
+	for (blk = 0; blk < blkcnt; blk++) {
+		req.block = blk;
+		if (ops->is_bad(chip, &req) == true)
+			cnt++;
+	}
+	ret += snprintf(buf + ret, pagesize - ret, "cnt: %u\n", cnt);
+
+	/* logic bad block num */
+	ret += snprintf(buf + ret, pagesize - ret, "blk:");
+	for (blk = 0; blk < blkcnt; blk++) {
+		req.block = blk;
+		if (ops->is_bad(chip, &req) == true)
+			ret += snprintf(buf + ret, pagesize - ret, " %u", blk);
+	}
+	ret += snprintf(buf + ret, pagesize - ret, "\n");
+
+	/* physic bad block count */
+	blkcnt = info->total_size(chip) / info->phy_block_size(chip);
+	for (cnt = 0, blk = 0; blk < blkcnt; blk++) {
+		req.block = blk;
+		if (ops->phy_is_bad(chip, &req) == true)
+			cnt++;
+	}
+	ret += snprintf(buf + ret, pagesize - ret, "phy cnt: %u\n", cnt);
+
+	/* logic bad block num */
+	ret += snprintf(buf + ret, pagesize - ret, "phy blk:");
+	for (blk = 0; blk < blkcnt; blk++) {
+		req.block = blk;
+		if (ops->phy_is_bad(chip, &req) == true)
+			ret += snprintf(buf + ret, pagesize - ret, " %u", blk);
+	}
+	ret += snprintf(buf + ret, pagesize - ret, "\n");
+
+	return ret;
+}
+
+static ssize_t aw_spinand_show_nanddbg(struct aw_spinand *spinand, char *buf)
+{
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_info *info = chip->info;
+	struct aw_spinand_chip_ops *ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	unsigned int pagesize = 1 << PAGE_SHIFT, blkcnt, cnt = 0, blk;
+	ssize_t ret = 0;
+
+	/* arch */
+	ret += aw_spinand_show_arch(spinand, buf);
+	/* bad block */
+	blkcnt = info->total_size(chip) / info->block_size(chip);
+	for (blk = 0; blk < blkcnt; blk++) {
+		req.block = blk;
+		if (ops->is_bad(chip, &req) == true)
+			cnt++;
+	}
+	ret += snprintf(buf + ret, pagesize - ret, "BadBlkCnt: %u\n", cnt);
+	/* version */
+	ret += snprintf(buf + ret, pagesize - ret, "NandVersion: %x.%x %x\n",
+			AW_MTD_SPINAND_VER_MAIN, AW_MTD_SPINAND_VER_SUB,
+			AW_MTD_SPINAND_VER_DATE);
+	return ret;
+}
diff --git a/drivers/mtd/awnand/spinand/sunxi-nftl-core.c b/drivers/mtd/awnand/spinand/sunxi-nftl-core.c
new file mode 100644
index 000000000..81b599fad
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/sunxi-nftl-core.c
@@ -0,0 +1,516 @@
+/*
+ * sunxi-nftl-core.c for  sunxi spi nand base on nftl.
+ *
+ * Copyright (C) 2019 Allwinner.
+ * SPDX-License-Identifier: GPL-2.0
+ */
+
+#define pr_fmt(fmt) "sunxi-spinand: " fmt
+
+#include "sunxi-spinand.h"
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mtd/aw-spinand-nftl.h>
+#include <linux/of.h>
+#include <linux/spi/spi.h>
+#include <linux/sunxi-sid.h>
+#include <linux/uaccess.h>
+
+struct aw_spinand *g_spinand;
+
+#define OP_FLASH_MEMORY_LEGALITY_CHECK(dienum, blocknum, pagenum)              \
+	spinand_nftl_ops_flash_memory_legality_check(g_spinand, dienum,        \
+			blocknum, pagenum)
+
+static int spinand_nftl_ops_flash_memory_legality_check(
+		struct aw_spinand *spinand, unsigned short dienum,
+		unsigned short blocknum, unsigned short pagenum)
+{
+	unsigned int die_cnt = 0, block_cnt = 0, page_cnt = 0;
+
+	if (!unlikely(spinand) || !unlikely(spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	die_cnt = spinand->chip.info->die_cnt(&spinand->chip);
+	block_cnt = spinand->chip.info->total_size(&spinand->chip) >>
+		spinand->block_shift;
+	page_cnt = spinand->chip.info->block_size(&spinand->chip) >>
+		spinand->page_shift;
+
+	if (unlikely(dienum >= die_cnt)) {
+		pr_err("nftl ops input dieno.%d/%d err\n", dienum, die_cnt);
+		return -EOVERFLOW;
+	}
+
+	if (unlikely(blocknum >= block_cnt)) {
+		pr_err("nftl ops input blockno.%d/%d err\n", blocknum,
+				block_cnt);
+		return -EOVERFLOW;
+	}
+
+	if (unlikely(pagenum >= page_cnt)) {
+		pr_err("nftl ops input pageno.%d/%d err\n", pagenum, page_cnt);
+		return -EOVERFLOW;
+	}
+
+	return 0;
+}
+
+#define OP_BUF_LEGALITY_CHECK(mbuf, spare)                                     \
+	spinand_nftl_ops_buf_legality_check(mbuf, spare)
+static int spinand_nftl_ops_buf_legality_check(void *mbuf, void *spare)
+{
+	if (!unlikely(mbuf) && !unlikely(spare)) {
+		pr_err("nftl ops input mbuf and spare is null\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#define NFTL_OPS_REQUEST(name, blocknum, pagenum, len, oobsize, mbuf, spare)   \
+{                                                                      \
+	(name).block = blocknum;                                       \
+	(name).page = pagenum;                                         \
+	(name).pageoff = 0;                                            \
+	(name).ooblen = oobsize;                                       \
+	(name).datalen = len;                                          \
+	(name).databuf = mbuf;                                         \
+	(name).oobbuf = spare;                                         \
+}
+
+/*
+ * calc_valid_bits - calculate valid bit
+ * @secbitmap: record valid bits
+ *
+ * Returns: valid bit count;
+ */
+
+unsigned int calc_valid_bits(unsigned int secbitmap)
+{
+	unsigned int validbit = 0;
+
+	while (secbitmap) {
+		if (secbitmap & 0x1)
+			validbit++;
+		secbitmap >>= 1;
+	}
+
+	return validbit;
+}
+
+/*
+ * spinand_nftl_read_page - nftl layer to read data
+ * @dienum: a chip maybe have multiple die,which mean you want to operate that
+ *	die,0 means the first die
+ * @blocknum: which mean you want to operation that block,0 means the first
+ *	block
+ * @pagenum: which mean you want to operation that page in blocknum block,
+ *	0 means the first page in one block
+ * @sectorbitmap: operation page bitmap in sector
+ * @rmbuf: read main data buf
+ * @rspare: read spare data buf
+ * Returns: 0 success;otherwise fail
+ */
+int spinand_nftl_read_page(unsigned short dienum, unsigned short blocknum,
+		unsigned short pagenum, unsigned short sectorbitmap,
+		void *rmbuf, void *rspare)
+{
+
+	int ret = 0;
+	struct aw_spinand *spinand = g_spinand;
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *chip_ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	unsigned int req_data_len = 0;
+
+	pr_debug("calling nftl read page: die num:%d blocknum:%d pagenum:%d "
+			"sectorbitmap:%d rbuf:%p rspare:%p\n",
+			dienum, blocknum, pagenum, sectorbitmap, rmbuf, rspare);
+
+	if (unlikely(OP_FLASH_MEMORY_LEGALITY_CHECK(dienum, blocknum, pagenum))
+			|| unlikely(OP_BUF_LEGALITY_CHECK(rmbuf, rspare)))
+		return -EINVAL;
+
+	if (rmbuf == NULL && rspare != NULL)
+		req_data_len = 0;
+
+	req_data_len = calc_valid_bits(sectorbitmap) << SECTOR_SHIFT;
+	NFTL_OPS_REQUEST(req, blocknum, pagenum, req_data_len, AW_NFTL_OOB_LEN,
+			rmbuf, rspare);
+
+	mutex_lock(&spinand->lock);
+
+	ret = chip_ops->read_page(chip, &req);
+	if (ret < 0)
+		pr_err("read single page failed: %d\n", ret);
+	else if (ret == ECC_LIMIT) {
+		ret = AW_NFTL_ECC_LIMIT;
+		pr_debug("ecc limit: block: %u page: %u\n", req.block,
+				req.page);
+	} else if (ret == ECC_ERR) {
+		ret = AW_NFTL_ECC_ERR;
+		pr_debug("ecc err: block: %u page: %u\n", req.block, req.page);
+	}
+
+	mutex_unlock(&spinand->lock);
+
+	pr_debug("exitng nftl read\n");
+
+	return ret;
+}
+
+/*
+ * spinand_nftl_write_page - nftl layer to write data
+ * @dienum: a chip maybe have multiple die,which mean you want to operate that
+ *	die,0 means the first die
+ * @blocknum: which mean you want to operation that block,0 means the first
+ *	block
+ * @pagenum: which mean you want to operation that page in blocknum block,
+ *	0 means the first page in one block
+ * @sectorbitmap: operation page bitmap in sector
+ * @wmbuf: write main data buf
+ * @wspare: write spare data buf
+ * Returns: 0 success;otherwise fail
+ */
+int spinand_nftl_write_page(unsigned short dienum, unsigned short blocknum,
+		unsigned short pagenum, unsigned short sectorbitmap,
+		void *wmbuf, void *wspare)
+{
+
+	int ret = 0;
+	struct aw_spinand *spinand = g_spinand;
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *chip_ops = chip->ops;
+	struct aw_spinand_chip_request req = {0};
+	unsigned int write_mdata_len = 0;
+
+	pr_debug("calling nftl write page: die num:%d blocknum:%d pagenum:%d "
+			"sectorbitmap:%d rbuf:%p wspare:%p\n",
+			dienum, blocknum, pagenum, sectorbitmap, wmbuf, wspare);
+
+	if (unlikely(OP_FLASH_MEMORY_LEGALITY_CHECK(dienum, blocknum, pagenum))
+			|| unlikely(OP_BUF_LEGALITY_CHECK(wmbuf, wspare)))
+		return -EINVAL;
+
+	if (wmbuf == NULL && wspare != NULL)
+		write_mdata_len = 0;
+
+	write_mdata_len = calc_valid_bits(sectorbitmap) << SECTOR_SHIFT;
+	NFTL_OPS_REQUEST(req, blocknum, pagenum, write_mdata_len,
+			AW_NFTL_OOB_LEN, wmbuf, wspare);
+
+	mutex_lock(&spinand->lock);
+
+	ret = chip_ops->write_page(chip, &req);
+	if (ret < 0)
+		pr_err("write single page failed: %d\n", ret);
+	else if (ret == ECC_LIMIT) {
+		ret = AW_NFTL_ECC_LIMIT;
+		pr_debug("ecc limit: block: %u page: %u\n", req.block,
+				req.page);
+	} else if (ret == ECC_ERR) {
+		ret = AW_NFTL_ECC_ERR;
+		pr_debug("ecc err: block: %u page: %u\n", req.block, req.page);
+	}
+
+	mutex_unlock(&spinand->lock);
+
+	pr_debug("exitng nftl write block@%d page@%d\n", req.block, req.page);
+
+	return ret;
+}
+
+/*
+ * spinand_nftl_erase_block - nftl layer to erase block
+ * @dienum: a chip maybe have multiple die,which mean you want to operate that
+ *	die,0 means the first die
+ * @blocknum: which mean you want to operation that block,0 means the first
+ *	block
+ * Returns: 0 success;otherwise fail
+ */
+int spinand_nftl_erase_block(unsigned short dienum, unsigned short blocknum)
+{
+
+	int ret = 0;
+	struct aw_spinand *spinand = g_spinand;
+	struct aw_spinand_chip *chip = spinand_to_chip(spinand);
+	struct aw_spinand_chip_ops *chip_ops = chip->ops;
+
+	struct aw_spinand_chip_request req = {0};
+
+	pr_debug("calling nftl write page: die num:%d blocknum:%d ", dienum,
+			blocknum);
+
+	if (unlikely(OP_FLASH_MEMORY_LEGALITY_CHECK(dienum, blocknum, 0)))
+		return -EINVAL;
+
+	NFTL_OPS_REQUEST(req, blocknum, 0, 0, 0, NULL, NULL);
+
+	mutex_lock(&spinand->lock);
+
+	ret = chip_ops->erase_block(chip, &req);
+	if (ret < 0)
+		pr_err("erase block@%d failed: %d\n", req.block, ret);
+
+	mutex_unlock(&spinand->lock);
+
+	pr_debug("exitng nftl erase block@%d\n", req.block);
+
+	return ret;
+}
+/**
+ * spinand_nftl_get_page_size - nftl layer to get page size
+ * @type: page size in sector or byte,0 in byte,1 sector
+ *
+ * Returns the size of page
+ */
+unsigned int spinand_nftl_get_page_size(int type)
+{
+	struct aw_spinand_info *info = NULL;
+
+	if (!unlikely(g_spinand) || !unlikely(g_spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	info = g_spinand->chip.info;
+
+	if (likely(type == SECTOR)) {
+		return info->page_size(&g_spinand->chip) >>
+			g_spinand->sector_shift;
+	} else if (unlikely(type == BYTE)) {
+		return info->page_size(&g_spinand->chip);
+	} else {
+		pr_err("no this type:%d page size in BYTE(0)@byte "
+				"SECTOR(1)@sector", type);
+		return -EINVAL;
+	}
+}
+EXPORT_SYMBOL(spinand_nftl_get_page_size);
+/**
+ * spinand_nftl_get_block_size - nftl layer to get block size
+ * @type: page size in sector or byte,0 in byte,1 sector,2 page
+ *
+ * Returns the size of block
+ */
+unsigned int spinand_nftl_get_block_size(int type)
+{
+	struct aw_spinand_info *info = NULL;
+
+	if (!unlikely(g_spinand) || !unlikely(g_spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	info = g_spinand->chip.info;
+
+	if (likely(type == PAGE)) {
+		return info->block_size(&g_spinand->chip) >>
+			g_spinand->page_shift;
+	} else if (unlikely(type == SECTOR)) {
+		return info->block_size(&g_spinand->chip) >>
+			g_spinand->sector_shift;
+	} else if (unlikely(type == BYTE)) {
+		return info->block_size(&g_spinand->chip);
+	} else {
+		pr_err("no this type:%d block size in BYTE(0)@byte "
+				"SECTOR(1)@sector PAGE(2)@page", type);
+		return -EINVAL;
+	}
+}
+EXPORT_SYMBOL(spinand_nftl_get_block_size);
+/**
+ * spinand_nftl_get_die_size - nftl layer to get die size
+ * @type: page size in sector or byte,0 in byte,1 sector,2 page,3 block
+ *
+ * Returns the size of die
+ */
+unsigned int spinand_nftl_get_die_size(int type)
+{
+	struct aw_spinand_info *info = NULL;
+
+	if (!unlikely(g_spinand) || !unlikely(g_spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	info = g_spinand->chip.info;
+
+	if (likely(type == BLOCK))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->block_shift >> g_spinand->die_shift;
+	else if (unlikely(type == BYTE))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->die_shift;
+	else if (unlikely(type == SECTOR))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->sector_shift >> g_spinand->die_shift;
+	else if (unlikely(type == PAGE))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->page_shift >> g_spinand->die_shift;
+	else {
+		pr_err("no this type:%d die size in BYTE(0)@byte "
+			"SECTOR(1)@sector PAGE(2)@page BLOCK(3)@block", type);
+		return -EINVAL;
+	}
+}
+EXPORT_SYMBOL(spinand_nftl_get_die_size);
+/**
+ * spinand_nftl_get_chip_size - nftl layer to get die size
+ * @type: page size in sector or byte,0 in byte,1 sector,2 page,3 block
+ *
+ * Returns the size of chip
+ */
+unsigned int spinand_nftl_get_chip_size(int type)
+{
+	struct aw_spinand_info *info = NULL;
+
+	if (!unlikely(g_spinand) || !unlikely(g_spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	info = g_spinand->chip.info;
+
+	if (likely(type == BLOCK))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->block_shift;
+	else if (unlikely(type == BYTE))
+		return info->total_size(&g_spinand->chip);
+	else if (unlikely(type == SECTOR))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->sector_shift;
+	else if (unlikely(type == PAGE))
+		return info->total_size(&g_spinand->chip) >>
+			g_spinand->page_shift;
+	else {
+		pr_err("no this type:%d die size in BYTE(0)@byte "
+			"SECTOR(1)@sector PAGE(2)@page BLOCK(3)@block", type);
+		return -EINVAL;
+	}
+}
+EXPORT_SYMBOL(spinand_nftl_get_chip_size);
+
+#define DIE_NUM_IN_OTHER_CHIP (0)
+/**
+ * spinand_nftl_get_die_num - nftl layer to get die numbers
+ *
+ * Returns the numbers of die in a chip
+ */
+unsigned int spinand_nftl_get_die_num(void)
+{
+	struct aw_spinand_info *info = NULL;
+
+	if (!unlikely(g_spinand) || !unlikely(g_spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	info = g_spinand->chip.info;
+
+	return info->die_cnt(&g_spinand->chip) + DIE_NUM_IN_OTHER_CHIP;
+}
+EXPORT_SYMBOL(spinand_nftl_get_die_num);
+
+/**
+ * spinand_nftl_get_max_erase_times - nftl layer to get chip endurace
+ *
+ * Returns the program/erase cycles
+ */
+unsigned int spinand_nftl_get_max_erase_times(void)
+{
+	struct aw_spinand_info *info = NULL;
+
+	if (!unlikely(g_spinand) || !unlikely(g_spinand->chip.info)) {
+		pr_err("nftl spinand driver not init\n");
+		return false;
+	}
+
+	info = g_spinand->chip.info;
+
+	return info->max_erase_times(&g_spinand->chip);
+}
+EXPORT_SYMBOL(spinand_nftl_get_max_erase_times);
+
+static void aw_spinand_cleanup(struct aw_spinand *spinand)
+{
+	aw_spinand_chip_exit(&spinand->chip);
+}
+
+static int aw_nftl_spinand_remove(struct spi_device *spi)
+{
+	struct aw_spinand *spinand;
+
+	spinand = spi_to_spinand(spi);
+
+	aw_spinand_cleanup(spinand);
+
+	return 0;
+}
+
+static int aw_nftl_spinand_probe(struct spi_device *spi)
+{
+	struct aw_spinand *spinand;
+	struct aw_spinand_chip *chip;
+	int ret;
+
+	if (g_spinand) {
+		pr_info("AW nftl-spinand already initialized\n");
+		return -EBUSY;
+	}
+
+	pr_info("AW SPINand NFTL Layer Version: %x.%x %x\n",
+			AW_NFTL_SPINAND_VER_MAIN, AW_NFTL_SPINAND_VER_SUB,
+			AW_NFTL_SPINAND_VER_DATE);
+
+	spinand = devm_kzalloc(&spi->dev, sizeof(*spinand), GFP_KERNEL);
+	if (!spinand)
+		return -ENOMEM;
+	chip = spinand_to_chip(spinand);
+	mutex_init(&spinand->lock);
+
+	ret = aw_spinand_chip_init(spi, chip);
+	if (ret)
+		goto err_spinand_cleanup;
+
+	spinand->sector_shift = ffs(chip->info->sector_size(chip)) - 1;
+	spinand->page_shift = ffs(chip->info->page_size(chip)) - 1;
+	spinand->block_shift = ffs(chip->info->block_size(chip)) - 1;
+	spinand->die_shift = ffs(chip->info->die_cnt(chip)) - 1;
+
+	g_spinand = spinand;
+
+	return 0;
+
+err_spinand_cleanup:
+	devm_kfree(&spi->dev, spinand);
+	return ret;
+}
+
+static const struct spi_device_id aw_spinand_ids[] = {
+	{.name = "spi-nand"},
+	{/* sentinel */},
+};
+
+static const struct of_device_id aw_spinand_of_ids[] = {
+	{.compatible = "spi-nand"},
+	{/* sentinel */},
+};
+
+static struct spi_driver aw_spinand_drv = {
+	.driver = {
+		.name = "spi-nand",
+		.of_match_table = of_match_ptr(aw_spinand_of_ids),
+	},
+	.id_table = aw_spinand_ids,
+	.probe = aw_nftl_spinand_probe,
+	.remove = aw_nftl_spinand_remove,
+};
+module_spi_driver(aw_spinand_drv);
+
+MODULE_AUTHOR("cuizhikui <cuizhikui@allwinnertech.com>");
+MODULE_DESCRIPTION(
+		"Allwinner's spinand driver base on allwinner's nftl system");
diff --git a/drivers/mtd/awnand/spinand/sunxi-spinand.h b/drivers/mtd/awnand/spinand/sunxi-spinand.h
new file mode 100644
index 000000000..62106c5fc
--- /dev/null
+++ b/drivers/mtd/awnand/spinand/sunxi-spinand.h
@@ -0,0 +1,46 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#ifndef __SUNXI_SPINAND_H
+#define __SUNXI_SPINAND_H
+
+#include <linux/mtd/aw-spinand.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mutex.h>
+
+#define SECTOR_SHIFT 9
+
+#define AW_MTD_SPINAND_VER_MAIN		0x02
+#define AW_MTD_SPINAND_VER_SUB		0x00
+#define AW_MTD_SPINAND_VER_DATE		0x20201228
+
+#define UBOOT_START_BLOCK_BIGNAND 4
+#define UBOOT_START_BLOCK_SMALLNAND 8
+#define PHY_BLKS_FOR_BOOT0	8
+#define PHY_BLKS_FOR_SECURE_STORAGE 8
+#define PSTORE_SIZE_KB	512
+#define NAND_BOOT0_BLK_START    0
+#define VALID_PAGESIZE_FOR_BOOT0 2048
+#define AW_SAMP_MODE_DL_DEFAULT 0xaaaaffff
+
+struct aw_spinand {
+	struct mutex lock;
+	struct aw_spinand_chip chip;
+	struct mtd_info mtd;
+	int sector_shift;
+	int page_shift;
+	int block_shift;
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+	struct aw_spinand_sec_sto sec_sto;
+#endif
+	unsigned int right_sample_delay;
+	unsigned int right_sample_mode;
+};
+
+extern struct aw_spinand *get_aw_spinand(void);
+
+#define spinand_to_mtd(spinand) (&spinand->mtd)
+#define spinand_to_chip(spinand) (&spinand->chip)
+#define mtd_to_spinand(mtd) container_of(mtd, struct aw_spinand, mtd)
+#define spi_to_spinand(spi) spi_get_drvdata(spi)
+
+#endif
diff --git a/drivers/mtd/chips/Kconfig b/drivers/mtd/chips/Kconfig
index a7e47e068..c3722c2da 100644
--- a/drivers/mtd/chips/Kconfig
+++ b/drivers/mtd/chips/Kconfig
@@ -3,7 +3,7 @@ menu "RAM/ROM/Flash chip drivers"
 	depends on MTD!=n
 
 config MTD_CFI
-	tristate "Detect flash chips by Common Flash Interface (CFI) probe"
+	bool "Detect flash chips by Common Flash Interface (CFI) probe"
 	select MTD_GEN_PROBE
 	select MTD_CFI_UTIL
 	help
@@ -15,7 +15,7 @@ config MTD_CFI
 	  for more information on CFI.
 
 config MTD_JEDECPROBE
-	tristate "Detect non-CFI AMD/JEDEC-compatible flash chips"
+	bool "Detect non-CFI AMD/JEDEC-compatible flash chips"
 	select MTD_GEN_PROBE
 	select MTD_CFI_UTIL
 	help
@@ -27,7 +27,7 @@ config MTD_JEDECPROBE
 	  Intel chips.
 
 config MTD_GEN_PROBE
-	tristate
+	bool
 
 config MTD_CFI_ADV_OPTIONS
 	bool "Flash chip driver advanced configuration options"
diff --git a/drivers/mtd/efi.h b/drivers/mtd/efi.h
new file mode 100644
index 000000000..cf704c1eb
--- /dev/null
+++ b/drivers/mtd/efi.h
@@ -0,0 +1,117 @@
+/*
+ * SUNXI MTD partitioning
+ *
+ * Copyright © 2016 WimHuang <huangwei@allwinnertech.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+
+#ifndef FS_PART_EFI_H_INCLUDED
+#define FS_PART_EFI_H_INCLUDED
+
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/genhd.h>
+#include <linux/kernel.h>
+#include <linux/major.h>
+#include <linux/string.h>
+#include <linux/efi.h>
+#include <linux/compiler.h>
+
+#define MSDOS_MBR_SIGNATURE 0xaa55
+#define EFI_PMBR_OSTYPE_EFI 0xEF
+#define EFI_PMBR_OSTYPE_EFI_GPT 0xEE
+
+#define GPT_MBR_PROTECTIVE 1
+#define GPT_MBR_HYBRID 2
+
+#define GPT_HEADER_SIGNATURE 0x5452415020494645ULL
+#define GPT_HEADER_REVISION_V1 0x00010000
+#define GPT_PRIMARY_PARTITION_TABLE_LBA 1
+
+#define PARTITION_SYSTEM_GUID                                                  \
+	EFI_GUID(0xC12A7328, 0xF81F, 0x11d2, 0xBA, 0x4B, 0x00, 0xA0, 0xC9,     \
+		 0x3E, 0xC9, 0x3B)
+#define LEGACY_MBR_PARTITION_GUID                                              \
+	EFI_GUID(0x024DEE41, 0x33E7, 0x11d3, 0x9D, 0x69, 0x00, 0x08, 0xC7,     \
+		 0x81, 0xF3, 0x9F)
+#define PARTITION_MSFT_RESERVED_GUID                                           \
+	EFI_GUID(0xE3C9E316, 0x0B5C, 0x4DB8, 0x81, 0x7D, 0xF9, 0x2D, 0xF0,     \
+		 0x02, 0x15, 0xAE)
+#define PARTITION_BASIC_DATA_GUID                                              \
+	EFI_GUID(0xEBD0A0A2, 0xB9E5, 0x4433, 0x87, 0xC0, 0x68, 0xB6, 0xB7,     \
+		 0x26, 0x99, 0xC7)
+#define PARTITION_LINUX_RAID_GUID                                              \
+	EFI_GUID(0xa19d880f, 0x05fc, 0x4d3b, 0xa0, 0x06, 0x74, 0x3f, 0x0f,     \
+		 0x84, 0x91, 0x1e)
+#define PARTITION_LINUX_SWAP_GUID                                              \
+	EFI_GUID(0x0657fd6d, 0xa4ab, 0x43c4, 0x84, 0xe5, 0x09, 0x33, 0xc8,     \
+		 0x4b, 0x4f, 0x4f)
+#define PARTITION_LINUX_LVM_GUID                                               \
+	EFI_GUID(0xe6d6d379, 0xf507, 0x44c2, 0xa2, 0x3c, 0x23, 0x8f, 0x2a,     \
+		 0x3d, 0xf9, 0x28)
+
+struct _gpt_header {
+	__le64 signature;
+	__le32 revision;
+	__le32 header_size;
+	__le32 header_crc32;
+	__le32 reserved1;
+	__le64 my_lba;
+	__le64 alternate_lba;
+	__le64 first_usable_lba;
+	__le64 last_usable_lba;
+	efi_guid_t disk_guid;
+	__le64 partition_entry_lba;
+	__le32 num_partition_entries;
+	__le32 sizeof_partition_entry;
+	__le32 partition_entry_array_crc32;
+
+	/* The rest of the logical block is reserved by UEFI and must be zero.
+	 * EFI standard handles this by:
+	 *
+	 * uint8_t		reserved2[ BlockSize - 92 ];
+	 */
+} __packed;
+
+struct _gpt_entry_attributes {
+	u64 required_to_function : 1;
+	u64 reserved : 47;
+	u64 type_guid_specific : 16;
+} __packed;
+
+struct _gpt_entry {
+	efi_guid_t partition_type_guid;
+	efi_guid_t unique_partition_guid;
+	__le64 starting_lba;
+	__le64 ending_lba;
+	struct _gpt_entry_attributes attributes;
+	efi_char16_t partition_name[72 / sizeof(efi_char16_t)];
+} __packed;
+
+struct _gpt_mbr_record {
+	u8 boot_indicator; /* unused by EFI, set to 0x80 for bootable */
+	u8 start_head;     /* unused by EFI, pt start in CHS */
+	u8 start_sector;   /* unused by EFI, pt start in CHS */
+	u8 start_track;
+	u8 os_type;	  /* EFI and legacy non-EFI OS types */
+	u8 end_head;	 /* unused by EFI, pt end in CHS */
+	u8 end_sector;       /* unused by EFI, pt end in CHS */
+	u8 end_track;	/* unused by EFI, pt end in CHS */
+	__le32 starting_lba; /* used by EFI - start addr of the on disk pt */
+	__le32 size_in_lba;  /* used by EFI - size of pt in LBA */
+} __packed;
+
+struct _legacy_mbr {
+	u8 boot_code[440];
+	__le32 unique_mbr_signature;
+	__le16 unknown;
+	struct _gpt_mbr_record partition_record[4];
+	__le16 signature;
+} __packed;
+
+#endif
diff --git a/drivers/mtd/mtdchar.c b/drivers/mtd/mtdchar.c
index 48832f9b2..69a072704 100644
--- a/drivers/mtd/mtdchar.c
+++ b/drivers/mtd/mtdchar.c
@@ -22,6 +22,7 @@
 #include <linux/mtd/mtd.h>
 #include <linux/mtd/partitions.h>
 #include <linux/mtd/map.h>
+#include <linux/mtd/aw-spinand.h>
 
 #include <linux/uaccess.h>
 
@@ -38,6 +39,20 @@ struct mtd_file_info {
 	enum mtd_file_modes mode;
 };
 
+struct burn_param_t {
+	void *buffer;
+	long length;
+};
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+struct secblc_op_t {
+	int item;
+	unsigned char *buf;
+	unsigned int len;
+};
+#endif
+
+
 static loff_t mtdchar_lseek(struct file *file, loff_t offset, int orig)
 {
 	struct mtd_file_info *mfi = file->private_data;
@@ -635,6 +650,14 @@ static int mtdchar_ioctl(struct file *file, u_int cmd, u_long arg)
 	void __user *argp = (void __user *)arg;
 	int ret = 0;
 	struct mtd_info_user info;
+#if IS_ENABLED(CONFIG_AW_MTD_SPINAND)
+	struct burn_param_t burn_param;
+	char *boot0_kbuf, *boot1_kbuf;
+#endif
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+	struct secblc_op_t sec_op_st;
+	char *buf_secure;
+#endif
 
 	pr_debug("MTD_ioctl\n");
 
@@ -664,6 +687,10 @@ static int mtdchar_ioctl(struct file *file, u_int cmd, u_long arg)
 	case MTDFILEMODE:
 	case BLKPG:
 	case BLKRRPART:
+	case BLKBURNBOOT0:
+	case BLKBURNBOOT1:
+	case SECBLK_READ:
+	case SECBLK_WRITE:
 		break;
 
 	/* "dangerous" commands */
@@ -681,6 +708,93 @@ static int mtdchar_ioctl(struct file *file, u_int cmd, u_long arg)
 	}
 
 	switch (cmd) {
+#if IS_ENABLED(CONFIG_AW_MTD_SPINAND)
+	case BLKBURNBOOT0:
+		if (copy_from_user(&burn_param,
+			(struct burn_param_t __user *)arg,
+			sizeof(burn_param))) {
+			pr_err("nand ioctl input arg err\n");
+			return -EINVAL;
+		}
+
+		boot0_kbuf = vmalloc(burn_param.length + 16 * 1024);
+		if (!boot0_kbuf)
+			return -ENOMEM;
+
+		if (copy_from_user(boot0_kbuf,
+			(const void *)burn_param.buffer, burn_param.length)) {
+			pr_err("nand ioctl input buffer err\n");
+			vfree(boot0_kbuf);
+			return -EINVAL;
+		}
+		ret = aw_spinand_mtd_download_boot0(mtd->priv,
+				burn_param.length, boot0_kbuf);
+		vfree(boot0_kbuf);
+		break;
+	case BLKBURNBOOT1:
+		if (copy_from_user(&burn_param,
+			(struct burn_param_t __user *)arg,
+			sizeof(burn_param))) {
+			pr_err("nand ioctl input arg err\n");
+			return -EINVAL;
+		}
+		boot1_kbuf = vmalloc(burn_param.length + 0x10000);
+		if (!boot1_kbuf)
+			return -ENOMEM;
+
+		if (copy_from_user(boot1_kbuf,
+			(const void *)burn_param.buffer, burn_param.length)) {
+			pr_err("nand ioctl input buffer err\n");
+			vfree(boot1_kbuf);
+			return -EINVAL;
+		}
+		ret = aw_spinand_mtd_download_uboot(mtd->priv,
+				burn_param.length, boot1_kbuf);
+		vfree(boot1_kbuf);
+		break;
+#endif
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+	case SECBLK_READ:
+		if (copy_from_user(&sec_op_st,
+			(struct secblc_op_t __user *)arg,
+			sizeof(sec_op_st))) {
+			pr_err("nand_ioctl input arg err\n");
+			return -EINVAL;
+		}
+		buf_secure = kmalloc(sec_op_st.len, GFP_KERNEL);
+		if (buf_secure == NULL) {
+			pr_err("buf_secure malloc fail!\n");
+			return -ENOMEM;
+		}
+		ret = aw_spinand_mtd_read_secure_storage(mtd->priv,
+				sec_op_st.item, buf_secure, sec_op_st.len);
+		if (copy_to_user(sec_op_st.buf, buf_secure, sec_op_st.len))
+			ret = -EFAULT;
+		kfree(buf_secure);
+		break;
+	case SECBLK_WRITE:
+		if (copy_from_user(&sec_op_st,
+			(struct secblc_op_t __user *)arg,
+			sizeof(sec_op_st))) {
+			pr_err("nand_ioctl input arg err\n");
+			return -EINVAL;
+		}
+		buf_secure = kmalloc(sec_op_st.len, GFP_KERNEL);
+		if (buf_secure == NULL) {
+			pr_err("buf_secure malloc fail!\n");
+			return -ENOMEM;
+		}
+		if (copy_from_user(buf_secure,
+				(const void *)sec_op_st.buf, sec_op_st.len))
+			ret = -EFAULT;
+
+		ret = aw_spinand_mtd_write_secure_storage(mtd->priv,
+				sec_op_st.item, buf_secure, sec_op_st.len);
+		if (copy_to_user(sec_op_st.buf, buf_secure, sec_op_st.len))
+			ret = -EFAULT;
+		kfree(buf_secure);
+		break;
+#endif
 	case MEMGETREGIONCOUNT:
 		if (copy_to_user(argp, &(mtd->numeraseregions), sizeof(int)))
 			return -EFAULT;
@@ -1046,6 +1160,76 @@ struct mtd_oob_buf32 {
 #define MEMWRITEOOB32		_IOWR('M', 3, struct mtd_oob_buf32)
 #define MEMREADOOB32		_IOWR('M', 4, struct mtd_oob_buf32)
 
+#if IS_ENABLED(CONFIG_AW_MTD_SPINAND)
+struct burn_param_t32 {
+	compat_caddr_t buffer;
+	compat_size_t length;
+};
+
+int mtdchar_burnboot_compat(struct file *file, unsigned int cmd,
+		struct burn_param_t32 __user *arg)
+{
+	struct burn_param_t32 burn_param32;
+	struct burn_param_t __user *burn_param;
+	int ret;
+
+	if (copy_from_user(&burn_param32, (void __user *)arg, sizeof(burn_param32)))
+		return -EFAULT;
+
+	burn_param = compat_alloc_user_space(sizeof(*burn_param));
+	if (!access_ok(VERIFY_WRITE, burn_param, sizeof(*burn_param)))
+		return -EFAULT;
+
+	if (put_user(compat_ptr(burn_param32.buffer), &burn_param->buffer) ||
+	    put_user(burn_param32.length, &burn_param->length))
+		return -EFAULT;
+
+	ret = mtdchar_ioctl(file, cmd, (unsigned long)burn_param);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+#endif
+
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+struct secblc_op_t32 {
+	u_int32_t item;
+	compat_caddr_t buf;
+	u_int32_t len;
+};
+
+int mtdchar_securestorage_compat(struct file *file, unsigned int cmd,
+		struct secblc_op_t32 __user *arg)
+{
+	struct secblc_op_t32 secblc_op32;
+	struct secblc_op_t __user *secblc_op;
+	int ret;
+
+	if (copy_from_user(&secblc_op32, (void __user *)arg, sizeof(secblc_op32)))
+		return -EFAULT;
+
+	secblc_op = compat_alloc_user_space(sizeof(*secblc_op));
+	if (!access_ok(VERIFY_WRITE, secblc_op, sizeof(*secblc_op)))
+		return -EFAULT;
+
+	if (put_user(secblc_op32.item, &secblc_op->item) ||
+	    put_user(compat_ptr(secblc_op32.buf), &secblc_op->buf) ||
+	    put_user(secblc_op32.len, &secblc_op->len))
+		return -EFAULT;
+
+	ret = mtdchar_ioctl(file, cmd, (unsigned long)secblc_op);
+	if (ret < 0)
+		return ret;
+
+	if (copy_in_user(&arg->item, &secblc_op->item, sizeof(arg->item)) ||
+	    copy_in_user(&arg->len, &secblc_op->len, sizeof(arg->len)))
+		return -EFAULT;
+
+	return 0;
+}
+#endif
+
 static long mtdchar_compat_ioctl(struct file *file, unsigned int cmd,
 	unsigned long arg)
 {
@@ -1053,6 +1237,14 @@ static long mtdchar_compat_ioctl(struct file *file, unsigned int cmd,
 	struct mtd_info *mtd = mfi->mtd;
 	void __user *argp = compat_ptr(arg);
 	int ret = 0;
+#if IS_ENABLED(CONFIG_AW_MTD_SPINAND)
+	struct burn_param_t32 burn_param32;
+	char *boot0_kbuf, *boot1_kbuf;
+#endif
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+	struct secblc_op_t32 sec_op_st32;
+	char *buf_secure;
+#endif
 
 	mutex_lock(&mtd_mutex);
 
@@ -1112,6 +1304,20 @@ static long mtdchar_compat_ioctl(struct file *file, unsigned int cmd,
 		ret = mtdchar_blkpg_ioctl(mtd, &a);
 		break;
 	}
+#if IS_ENABLED(CONFIG_AW_MTD_SPINAND)
+	case BLKBURNBOOT0:
+	case BLKBURNBOOT1: {
+		ret = mtdchar_burnboot_compat(file, cmd, compat_ptr(arg));
+		break;
+	}
+#endif
+#if IS_ENABLED(CONFIG_AW_SPINAND_SECURE_STORAGE)
+	case SECBLK_READ:
+	case SECBLK_WRITE: {
+		ret = mtdchar_securestorage_compat(file, cmd, compat_ptr(arg));
+		break;
+	}
+#endif
 
 	default:
 		ret = mtdchar_ioctl(file, cmd, (unsigned long)argp);
diff --git a/drivers/mtd/mtdcore.c b/drivers/mtd/mtdcore.c
index 036b9452b..1091fe177 100644
--- a/drivers/mtd/mtdcore.c
+++ b/drivers/mtd/mtdcore.c
@@ -616,9 +616,9 @@ int add_mtd_device(struct mtd_info *mtd)
 	 * MTD drivers should implement ->_{write,read}() or
 	 * ->_{write,read}_oob(), but not both.
 	 */
-	if (WARN_ON((mtd->_write && mtd->_write_oob) ||
+	/*if (WARN_ON((mtd->_write && mtd->_write_oob) ||
 		    (mtd->_read && mtd->_read_oob)))
-		return -EINVAL;
+		return -EINVAL;*/
 
 	if (WARN_ON((!mtd->erasesize || !mtd->_erase) &&
 		    !(mtd->flags & MTD_NO_ERASE)))
diff --git a/drivers/mtd/mtdpart.c b/drivers/mtd/mtdpart.c
index 7328c066c..9ebdd9739 100644
--- a/drivers/mtd/mtdpart.c
+++ b/drivers/mtd/mtdpart.c
@@ -425,6 +425,7 @@ static struct mtd_part *allocate_partition(struct mtd_info *parent,
 	if (parent->_put_device)
 		slave->mtd._put_device = part_put_device;
 
+	slave->mtd.priv = parent;
 	slave->mtd._erase = part_erase;
 	slave->parent = parent;
 	slave->offset = part->offset;
@@ -818,6 +819,7 @@ EXPORT_SYMBOL_GPL(deregister_mtd_parser);
 static const char * const default_mtd_part_types[] = {
 	"cmdlinepart",
 	"ofpart",
+	"sunxipart",
 	NULL
 };
 
diff --git a/drivers/mtd/nand/onenand/Makefile b/drivers/mtd/nand/onenand/Makefile
index f8b624aca..a27b635eb 100644
--- a/drivers/mtd/nand/onenand/Makefile
+++ b/drivers/mtd/nand/onenand/Makefile
@@ -9,6 +9,6 @@ obj-$(CONFIG_MTD_ONENAND)		+= onenand.o
 # Board specific.
 obj-$(CONFIG_MTD_ONENAND_GENERIC)	+= generic.o
 obj-$(CONFIG_MTD_ONENAND_OMAP2)		+= omap2.o
-obj-$(CONFIG_MTD_ONENAND_SAMSUNG)       += samsung.o
+obj-$(CONFIG_MTD_ONENAND_SAMSUNG)       += samsung_mtd.o
 
 onenand-objs = onenand_base.o onenand_bbt.o
diff --git a/drivers/mtd/nand/onenand/samsung_mtd.c b/drivers/mtd/nand/onenand/samsung_mtd.c
new file mode 100644
index 000000000..55e5536a5
--- /dev/null
+++ b/drivers/mtd/nand/onenand/samsung_mtd.c
@@ -0,0 +1,1006 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Samsung S3C64XX/S5PC1XX OneNAND driver
+ *
+ *  Copyright © 2008-2010 Samsung Electronics
+ *  Kyungmin Park <kyungmin.park@samsung.com>
+ *  Marek Szyprowski <m.szyprowski@samsung.com>
+ *
+ * Implementation:
+ *	S3C64XX: emulate the pseudo BufferRAM
+ *	S5PC110: use DMA
+ */
+
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/onenand.h>
+#include <linux/mtd/partitions.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+
+#include "samsung.h"
+
+enum soc_type {
+	TYPE_S3C6400,
+	TYPE_S3C6410,
+	TYPE_S5PC110,
+};
+
+#define ONENAND_ERASE_STATUS		0x00
+#define ONENAND_MULTI_ERASE_SET		0x01
+#define ONENAND_ERASE_START		0x03
+#define ONENAND_UNLOCK_START		0x08
+#define ONENAND_UNLOCK_END		0x09
+#define ONENAND_LOCK_START		0x0A
+#define ONENAND_LOCK_END		0x0B
+#define ONENAND_LOCK_TIGHT_START	0x0C
+#define ONENAND_LOCK_TIGHT_END		0x0D
+#define ONENAND_UNLOCK_ALL		0x0E
+#define ONENAND_OTP_ACCESS		0x12
+#define ONENAND_SPARE_ACCESS_ONLY	0x13
+#define ONENAND_MAIN_ACCESS_ONLY	0x14
+#define ONENAND_ERASE_VERIFY		0x15
+#define ONENAND_MAIN_SPARE_ACCESS	0x16
+#define ONENAND_PIPELINE_READ		0x4000
+
+#define MAP_00				(0x0)
+#define MAP_01				(0x1)
+#define MAP_10				(0x2)
+#define MAP_11				(0x3)
+
+#define S3C64XX_CMD_MAP_SHIFT		24
+
+#define S3C6400_FBA_SHIFT		10
+#define S3C6400_FPA_SHIFT		4
+#define S3C6400_FSA_SHIFT		2
+
+#define S3C6410_FBA_SHIFT		12
+#define S3C6410_FPA_SHIFT		6
+#define S3C6410_FSA_SHIFT		4
+
+/* S5PC110 specific definitions */
+#define S5PC110_DMA_SRC_ADDR		0x400
+#define S5PC110_DMA_SRC_CFG		0x404
+#define S5PC110_DMA_DST_ADDR		0x408
+#define S5PC110_DMA_DST_CFG		0x40C
+#define S5PC110_DMA_TRANS_SIZE		0x414
+#define S5PC110_DMA_TRANS_CMD		0x418
+#define S5PC110_DMA_TRANS_STATUS	0x41C
+#define S5PC110_DMA_TRANS_DIR		0x420
+#define S5PC110_INTC_DMA_CLR		0x1004
+#define S5PC110_INTC_ONENAND_CLR	0x1008
+#define S5PC110_INTC_DMA_MASK		0x1024
+#define S5PC110_INTC_ONENAND_MASK	0x1028
+#define S5PC110_INTC_DMA_PEND		0x1044
+#define S5PC110_INTC_ONENAND_PEND	0x1048
+#define S5PC110_INTC_DMA_STATUS		0x1064
+#define S5PC110_INTC_ONENAND_STATUS	0x1068
+
+#define S5PC110_INTC_DMA_TD		(1 << 24)
+#define S5PC110_INTC_DMA_TE		(1 << 16)
+
+#define S5PC110_DMA_CFG_SINGLE		(0x0 << 16)
+#define S5PC110_DMA_CFG_4BURST		(0x2 << 16)
+#define S5PC110_DMA_CFG_8BURST		(0x3 << 16)
+#define S5PC110_DMA_CFG_16BURST		(0x4 << 16)
+
+#define S5PC110_DMA_CFG_INC		(0x0 << 8)
+#define S5PC110_DMA_CFG_CNT		(0x1 << 8)
+
+#define S5PC110_DMA_CFG_8BIT		(0x0 << 0)
+#define S5PC110_DMA_CFG_16BIT		(0x1 << 0)
+#define S5PC110_DMA_CFG_32BIT		(0x2 << 0)
+
+#define S5PC110_DMA_SRC_CFG_READ	(S5PC110_DMA_CFG_16BURST | \
+					S5PC110_DMA_CFG_INC | \
+					S5PC110_DMA_CFG_16BIT)
+#define S5PC110_DMA_DST_CFG_READ	(S5PC110_DMA_CFG_16BURST | \
+					S5PC110_DMA_CFG_INC | \
+					S5PC110_DMA_CFG_32BIT)
+#define S5PC110_DMA_SRC_CFG_WRITE	(S5PC110_DMA_CFG_16BURST | \
+					S5PC110_DMA_CFG_INC | \
+					S5PC110_DMA_CFG_32BIT)
+#define S5PC110_DMA_DST_CFG_WRITE	(S5PC110_DMA_CFG_16BURST | \
+					S5PC110_DMA_CFG_INC | \
+					S5PC110_DMA_CFG_16BIT)
+
+#define S5PC110_DMA_TRANS_CMD_TDC	(0x1 << 18)
+#define S5PC110_DMA_TRANS_CMD_TEC	(0x1 << 16)
+#define S5PC110_DMA_TRANS_CMD_TR	(0x1 << 0)
+
+#define S5PC110_DMA_TRANS_STATUS_TD	(0x1 << 18)
+#define S5PC110_DMA_TRANS_STATUS_TB	(0x1 << 17)
+#define S5PC110_DMA_TRANS_STATUS_TE	(0x1 << 16)
+
+#define S5PC110_DMA_DIR_READ		0x0
+#define S5PC110_DMA_DIR_WRITE		0x1
+
+struct s3c_onenand {
+	struct mtd_info	*mtd;
+	struct platform_device	*pdev;
+	enum soc_type	type;
+	void __iomem	*base;
+	void __iomem	*ahb_addr;
+	int		bootram_command;
+	void		*page_buf;
+	void		*oob_buf;
+	unsigned int	(*mem_addr)(int fba, int fpa, int fsa);
+	unsigned int	(*cmd_map)(unsigned int type, unsigned int val);
+	void __iomem	*dma_addr;
+	unsigned long	phys_base;
+	struct completion	complete;
+};
+
+#define CMD_MAP_00(dev, addr)		(dev->cmd_map(MAP_00, ((addr) << 1)))
+#define CMD_MAP_01(dev, mem_addr)	(dev->cmd_map(MAP_01, (mem_addr)))
+#define CMD_MAP_10(dev, mem_addr)	(dev->cmd_map(MAP_10, (mem_addr)))
+#define CMD_MAP_11(dev, addr)		(dev->cmd_map(MAP_11, ((addr) << 2)))
+
+static struct s3c_onenand *onenand;
+
+static inline int s3c_read_reg(int offset)
+{
+	return readl(onenand->base + offset);
+}
+
+static inline void s3c_write_reg(int value, int offset)
+{
+	writel(value, onenand->base + offset);
+}
+
+static inline int s3c_read_cmd(unsigned int cmd)
+{
+	return readl(onenand->ahb_addr + cmd);
+}
+
+static inline void s3c_write_cmd(int value, unsigned int cmd)
+{
+	writel(value, onenand->ahb_addr + cmd);
+}
+
+#ifdef SAMSUNG_DEBUG
+static void s3c_dump_reg(void)
+{
+	int i;
+
+	for (i = 0; i < 0x400; i += 0x40) {
+		printk(KERN_INFO "0x%08X: 0x%08x 0x%08x 0x%08x 0x%08x\n",
+			(unsigned int) onenand->base + i,
+			s3c_read_reg(i), s3c_read_reg(i + 0x10),
+			s3c_read_reg(i + 0x20), s3c_read_reg(i + 0x30));
+	}
+}
+#endif
+
+static unsigned int s3c64xx_cmd_map(unsigned type, unsigned val)
+{
+	return (type << S3C64XX_CMD_MAP_SHIFT) | val;
+}
+
+static unsigned int s3c6400_mem_addr(int fba, int fpa, int fsa)
+{
+	return (fba << S3C6400_FBA_SHIFT) | (fpa << S3C6400_FPA_SHIFT) |
+		(fsa << S3C6400_FSA_SHIFT);
+}
+
+static unsigned int s3c6410_mem_addr(int fba, int fpa, int fsa)
+{
+	return (fba << S3C6410_FBA_SHIFT) | (fpa << S3C6410_FPA_SHIFT) |
+		(fsa << S3C6410_FSA_SHIFT);
+}
+
+static void s3c_onenand_reset(void)
+{
+	unsigned long timeout = 0x10000;
+	int stat;
+
+	s3c_write_reg(ONENAND_MEM_RESET_COLD, MEM_RESET_OFFSET);
+	while (1 && timeout--) {
+		stat = s3c_read_reg(INT_ERR_STAT_OFFSET);
+		if (stat & RST_CMP)
+			break;
+	}
+	stat = s3c_read_reg(INT_ERR_STAT_OFFSET);
+	s3c_write_reg(stat, INT_ERR_ACK_OFFSET);
+
+	/* Clear interrupt */
+	s3c_write_reg(0x0, INT_ERR_ACK_OFFSET);
+	/* Clear the ECC status */
+	s3c_write_reg(0x0, ECC_ERR_STAT_OFFSET);
+}
+
+static unsigned short s3c_onenand_readw(void __iomem *addr)
+{
+	struct onenand_chip *this = onenand->mtd->priv;
+	struct device *dev = &onenand->pdev->dev;
+	int reg = addr - this->base;
+	int word_addr = reg >> 1;
+	int value;
+
+	/* It's used for probing time */
+	switch (reg) {
+	case ONENAND_REG_MANUFACTURER_ID:
+		return s3c_read_reg(MANUFACT_ID_OFFSET);
+	case ONENAND_REG_DEVICE_ID:
+		return s3c_read_reg(DEVICE_ID_OFFSET);
+	case ONENAND_REG_VERSION_ID:
+		return s3c_read_reg(FLASH_VER_ID_OFFSET);
+	case ONENAND_REG_DATA_BUFFER_SIZE:
+		return s3c_read_reg(DATA_BUF_SIZE_OFFSET);
+	case ONENAND_REG_TECHNOLOGY:
+		return s3c_read_reg(TECH_OFFSET);
+	case ONENAND_REG_SYS_CFG1:
+		return s3c_read_reg(MEM_CFG_OFFSET);
+
+	/* Used at unlock all status */
+	case ONENAND_REG_CTRL_STATUS:
+		return 0;
+
+	case ONENAND_REG_WP_STATUS:
+		return ONENAND_WP_US;
+
+	default:
+		break;
+	}
+
+	/* BootRAM access control */
+	if ((unsigned int) addr < ONENAND_DATARAM && onenand->bootram_command) {
+		if (word_addr == 0)
+			return s3c_read_reg(MANUFACT_ID_OFFSET);
+		if (word_addr == 1)
+			return s3c_read_reg(DEVICE_ID_OFFSET);
+		if (word_addr == 2)
+			return s3c_read_reg(FLASH_VER_ID_OFFSET);
+	}
+
+	value = s3c_read_cmd(CMD_MAP_11(onenand, word_addr)) & 0xffff;
+	dev_info(dev, "%s: Illegal access at reg 0x%x, value 0x%x\n", __func__,
+		 word_addr, value);
+	return value;
+}
+
+static void s3c_onenand_writew(unsigned short value, void __iomem *addr)
+{
+	struct onenand_chip *this = onenand->mtd->priv;
+	struct device *dev = &onenand->pdev->dev;
+	unsigned int reg = addr - this->base;
+	unsigned int word_addr = reg >> 1;
+
+	/* It's used for probing time */
+	switch (reg) {
+	case ONENAND_REG_SYS_CFG1:
+		s3c_write_reg(value, MEM_CFG_OFFSET);
+		return;
+
+	case ONENAND_REG_START_ADDRESS1:
+	case ONENAND_REG_START_ADDRESS2:
+		return;
+
+	/* Lock/lock-tight/unlock/unlock_all */
+	case ONENAND_REG_START_BLOCK_ADDRESS:
+		return;
+
+	default:
+		break;
+	}
+
+	/* BootRAM access control */
+	if ((unsigned int)addr < ONENAND_DATARAM) {
+		if (value == ONENAND_CMD_READID) {
+			onenand->bootram_command = 1;
+			return;
+		}
+		if (value == ONENAND_CMD_RESET) {
+			s3c_write_reg(ONENAND_MEM_RESET_COLD, MEM_RESET_OFFSET);
+			onenand->bootram_command = 0;
+			return;
+		}
+	}
+
+	dev_info(dev, "%s: Illegal access at reg 0x%x, value 0x%x\n", __func__,
+		 word_addr, value);
+
+	s3c_write_cmd(value, CMD_MAP_11(onenand, word_addr));
+}
+
+static int s3c_onenand_wait(struct mtd_info *mtd, int state)
+{
+	struct device *dev = &onenand->pdev->dev;
+	unsigned int flags = INT_ACT;
+	unsigned int stat, ecc;
+	unsigned long timeout;
+
+	switch (state) {
+	case FL_READING:
+		flags |= BLK_RW_CMP | LOAD_CMP;
+		break;
+	case FL_WRITING:
+		flags |= BLK_RW_CMP | PGM_CMP;
+		break;
+	case FL_ERASING:
+		flags |= BLK_RW_CMP | ERS_CMP;
+		break;
+	case FL_LOCKING:
+		flags |= BLK_RW_CMP;
+		break;
+	default:
+		break;
+	}
+
+	/* The 20 msec is enough */
+	timeout = jiffies + msecs_to_jiffies(20);
+	while (time_before(jiffies, timeout)) {
+		stat = s3c_read_reg(INT_ERR_STAT_OFFSET);
+		if (stat & flags)
+			break;
+
+		if (state != FL_READING)
+			cond_resched();
+	}
+	/* To get correct interrupt status in timeout case */
+	stat = s3c_read_reg(INT_ERR_STAT_OFFSET);
+	s3c_write_reg(stat, INT_ERR_ACK_OFFSET);
+
+	/*
+	 * In the Spec. it checks the controller status first
+	 * However if you get the correct information in case of
+	 * power off recovery (POR) test, it should read ECC status first
+	 */
+	if (stat & LOAD_CMP) {
+		ecc = s3c_read_reg(ECC_ERR_STAT_OFFSET);
+		if (ecc & ONENAND_ECC_4BIT_UNCORRECTABLE) {
+			dev_info(dev, "%s: ECC error = 0x%04x\n", __func__,
+				 ecc);
+			mtd->ecc_stats.failed++;
+			return -EBADMSG;
+		}
+	}
+
+	if (stat & (LOCKED_BLK | ERS_FAIL | PGM_FAIL | LD_FAIL_ECC_ERR)) {
+		dev_info(dev, "%s: controller error = 0x%04x\n", __func__,
+			 stat);
+		if (stat & LOCKED_BLK)
+			dev_info(dev, "%s: it's locked error = 0x%04x\n",
+				 __func__, stat);
+
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int s3c_onenand_command(struct mtd_info *mtd, int cmd, loff_t addr,
+			       size_t len)
+{
+	struct onenand_chip *this = mtd->priv;
+	unsigned int *m, *s;
+	int fba, fpa, fsa = 0;
+	unsigned int mem_addr, cmd_map_01, cmd_map_10;
+	int i, mcount, scount;
+	int index;
+
+	fba = (int) (addr >> this->erase_shift);
+	fpa = (int) (addr >> this->page_shift);
+	fpa &= this->page_mask;
+
+	mem_addr = onenand->mem_addr(fba, fpa, fsa);
+	cmd_map_01 = CMD_MAP_01(onenand, mem_addr);
+	cmd_map_10 = CMD_MAP_10(onenand, mem_addr);
+
+	switch (cmd) {
+	case ONENAND_CMD_READ:
+	case ONENAND_CMD_READOOB:
+	case ONENAND_CMD_BUFFERRAM:
+		ONENAND_SET_NEXT_BUFFERRAM(this);
+	default:
+		break;
+	}
+
+	index = ONENAND_CURRENT_BUFFERRAM(this);
+
+	/*
+	 * Emulate Two BufferRAMs and access with 4 bytes pointer
+	 */
+	m = onenand->page_buf;
+	s = onenand->oob_buf;
+
+	if (index) {
+		m += (this->writesize >> 2);
+		s += (mtd->oobsize >> 2);
+	}
+
+	mcount = mtd->writesize >> 2;
+	scount = mtd->oobsize >> 2;
+
+	switch (cmd) {
+	case ONENAND_CMD_READ:
+		/* Main */
+		for (i = 0; i < mcount; i++)
+			*m++ = s3c_read_cmd(cmd_map_01);
+		return 0;
+
+	case ONENAND_CMD_READOOB:
+		s3c_write_reg(TSRF, TRANS_SPARE_OFFSET);
+		/* Main */
+		for (i = 0; i < mcount; i++)
+			*m++ = s3c_read_cmd(cmd_map_01);
+
+		/* Spare */
+		for (i = 0; i < scount; i++)
+			*s++ = s3c_read_cmd(cmd_map_01);
+
+		s3c_write_reg(0, TRANS_SPARE_OFFSET);
+		return 0;
+
+	case ONENAND_CMD_PROG:
+		/* Main */
+		for (i = 0; i < mcount; i++)
+			s3c_write_cmd(*m++, cmd_map_01);
+		return 0;
+
+	case ONENAND_CMD_PROGOOB:
+		s3c_write_reg(TSRF, TRANS_SPARE_OFFSET);
+
+		/* Main - dummy write */
+		for (i = 0; i < mcount; i++)
+			s3c_write_cmd(0xffffffff, cmd_map_01);
+
+		/* Spare */
+		for (i = 0; i < scount; i++)
+			s3c_write_cmd(*s++, cmd_map_01);
+
+		s3c_write_reg(0, TRANS_SPARE_OFFSET);
+		return 0;
+
+	case ONENAND_CMD_UNLOCK_ALL:
+		s3c_write_cmd(ONENAND_UNLOCK_ALL, cmd_map_10);
+		return 0;
+
+	case ONENAND_CMD_ERASE:
+		s3c_write_cmd(ONENAND_ERASE_START, cmd_map_10);
+		return 0;
+
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static unsigned char *s3c_get_bufferram(struct mtd_info *mtd, int area)
+{
+	struct onenand_chip *this = mtd->priv;
+	int index = ONENAND_CURRENT_BUFFERRAM(this);
+	unsigned char *p;
+
+	if (area == ONENAND_DATARAM) {
+		p = onenand->page_buf;
+		if (index == 1)
+			p += this->writesize;
+	} else {
+		p = onenand->oob_buf;
+		if (index == 1)
+			p += mtd->oobsize;
+	}
+
+	return p;
+}
+
+static int onenand_read_bufferram(struct mtd_info *mtd, int area,
+				  unsigned char *buffer, int offset,
+				  size_t count)
+{
+	unsigned char *p;
+
+	p = s3c_get_bufferram(mtd, area);
+	memcpy(buffer, p + offset, count);
+	return 0;
+}
+
+static int onenand_write_bufferram(struct mtd_info *mtd, int area,
+				   const unsigned char *buffer, int offset,
+				   size_t count)
+{
+	unsigned char *p;
+
+	p = s3c_get_bufferram(mtd, area);
+	memcpy(p + offset, buffer, count);
+	return 0;
+}
+
+static int (*s5pc110_dma_ops)(dma_addr_t dst, dma_addr_t src, size_t count, int direction);
+
+static int s5pc110_dma_poll(dma_addr_t dst, dma_addr_t src, size_t count, int direction)
+{
+	void __iomem *base = onenand->dma_addr;
+	int status;
+	unsigned long timeout;
+
+	writel(src, base + S5PC110_DMA_SRC_ADDR);
+	writel(dst, base + S5PC110_DMA_DST_ADDR);
+
+	if (direction == S5PC110_DMA_DIR_READ) {
+		writel(S5PC110_DMA_SRC_CFG_READ, base + S5PC110_DMA_SRC_CFG);
+		writel(S5PC110_DMA_DST_CFG_READ, base + S5PC110_DMA_DST_CFG);
+	} else {
+		writel(S5PC110_DMA_SRC_CFG_WRITE, base + S5PC110_DMA_SRC_CFG);
+		writel(S5PC110_DMA_DST_CFG_WRITE, base + S5PC110_DMA_DST_CFG);
+	}
+
+	writel(count, base + S5PC110_DMA_TRANS_SIZE);
+	writel(direction, base + S5PC110_DMA_TRANS_DIR);
+
+	writel(S5PC110_DMA_TRANS_CMD_TR, base + S5PC110_DMA_TRANS_CMD);
+
+	/*
+	 * There's no exact timeout values at Spec.
+	 * In real case it takes under 1 msec.
+	 * So 20 msecs are enough.
+	 */
+	timeout = jiffies + msecs_to_jiffies(20);
+
+	do {
+		status = readl(base + S5PC110_DMA_TRANS_STATUS);
+		if (status & S5PC110_DMA_TRANS_STATUS_TE) {
+			writel(S5PC110_DMA_TRANS_CMD_TEC,
+					base + S5PC110_DMA_TRANS_CMD);
+			return -EIO;
+		}
+	} while (!(status & S5PC110_DMA_TRANS_STATUS_TD) &&
+		time_before(jiffies, timeout));
+
+	writel(S5PC110_DMA_TRANS_CMD_TDC, base + S5PC110_DMA_TRANS_CMD);
+
+	return 0;
+}
+
+static irqreturn_t s5pc110_onenand_irq(int irq, void *data)
+{
+	void __iomem *base = onenand->dma_addr;
+	int status, cmd = 0;
+
+	status = readl(base + S5PC110_INTC_DMA_STATUS);
+
+	if (likely(status & S5PC110_INTC_DMA_TD))
+		cmd = S5PC110_DMA_TRANS_CMD_TDC;
+
+	if (unlikely(status & S5PC110_INTC_DMA_TE))
+		cmd = S5PC110_DMA_TRANS_CMD_TEC;
+
+	writel(cmd, base + S5PC110_DMA_TRANS_CMD);
+	writel(status, base + S5PC110_INTC_DMA_CLR);
+
+	if (!onenand->complete.done)
+		complete(&onenand->complete);
+
+	return IRQ_HANDLED;
+}
+
+static int s5pc110_dma_irq(dma_addr_t dst, dma_addr_t src, size_t count, int direction)
+{
+	void __iomem *base = onenand->dma_addr;
+	int status;
+
+	status = readl(base + S5PC110_INTC_DMA_MASK);
+	if (status) {
+		status &= ~(S5PC110_INTC_DMA_TD | S5PC110_INTC_DMA_TE);
+		writel(status, base + S5PC110_INTC_DMA_MASK);
+	}
+
+	writel(src, base + S5PC110_DMA_SRC_ADDR);
+	writel(dst, base + S5PC110_DMA_DST_ADDR);
+
+	if (direction == S5PC110_DMA_DIR_READ) {
+		writel(S5PC110_DMA_SRC_CFG_READ, base + S5PC110_DMA_SRC_CFG);
+		writel(S5PC110_DMA_DST_CFG_READ, base + S5PC110_DMA_DST_CFG);
+	} else {
+		writel(S5PC110_DMA_SRC_CFG_WRITE, base + S5PC110_DMA_SRC_CFG);
+		writel(S5PC110_DMA_DST_CFG_WRITE, base + S5PC110_DMA_DST_CFG);
+	}
+
+	writel(count, base + S5PC110_DMA_TRANS_SIZE);
+	writel(direction, base + S5PC110_DMA_TRANS_DIR);
+
+	writel(S5PC110_DMA_TRANS_CMD_TR, base + S5PC110_DMA_TRANS_CMD);
+
+	wait_for_completion_timeout(&onenand->complete, msecs_to_jiffies(20));
+
+	return 0;
+}
+
+static int s5pc110_read_bufferram(struct mtd_info *mtd, int area,
+		unsigned char *buffer, int offset, size_t count)
+{
+	struct onenand_chip *this = mtd->priv;
+	void __iomem *p;
+	void *buf = (void *) buffer;
+	dma_addr_t dma_src, dma_dst;
+	int err, ofs, page_dma = 0;
+	struct device *dev = &onenand->pdev->dev;
+
+	p = this->base + area;
+	if (ONENAND_CURRENT_BUFFERRAM(this)) {
+		if (area == ONENAND_DATARAM)
+			p += this->writesize;
+		else
+			p += mtd->oobsize;
+	}
+
+	if (offset & 3 || (size_t) buf & 3 ||
+		!onenand->dma_addr || count != mtd->writesize)
+		goto normal;
+
+	/* Handle vmalloc address */
+	if (buf >= high_memory) {
+		struct page *page;
+
+		if (((size_t) buf & PAGE_MASK) !=
+		    ((size_t) (buf + count - 1) & PAGE_MASK))
+			goto normal;
+		page = vmalloc_to_page(buf);
+		if (!page)
+			goto normal;
+
+		/* Page offset */
+		ofs = ((size_t) buf & ~PAGE_MASK);
+		page_dma = 1;
+
+		/* DMA routine */
+		dma_src = onenand->phys_base + (p - this->base);
+		dma_dst = dma_map_page(dev, page, ofs, count, DMA_FROM_DEVICE);
+	} else {
+		/* DMA routine */
+		dma_src = onenand->phys_base + (p - this->base);
+		dma_dst = dma_map_single(dev, buf, count, DMA_FROM_DEVICE);
+	}
+	if (dma_mapping_error(dev, dma_dst)) {
+		dev_err(dev, "Couldn't map a %d byte buffer for DMA\n", count);
+		goto normal;
+	}
+	err = s5pc110_dma_ops(dma_dst, dma_src,
+			count, S5PC110_DMA_DIR_READ);
+
+	if (page_dma)
+		dma_unmap_page(dev, dma_dst, count, DMA_FROM_DEVICE);
+	else
+		dma_unmap_single(dev, dma_dst, count, DMA_FROM_DEVICE);
+
+	if (!err)
+		return 0;
+
+normal:
+	if (count != mtd->writesize) {
+		/* Copy the bufferram to memory to prevent unaligned access */
+		memcpy(this->page_buf, p, mtd->writesize);
+		p = this->page_buf + offset;
+	}
+
+	memcpy(buffer, p, count);
+
+	return 0;
+}
+
+static int s5pc110_chip_probe(struct mtd_info *mtd)
+{
+	/* Now just return 0 */
+	return 0;
+}
+
+static int s3c_onenand_bbt_wait(struct mtd_info *mtd, int state)
+{
+	unsigned int flags = INT_ACT | LOAD_CMP;
+	unsigned int stat;
+	unsigned long timeout;
+
+	/* The 20 msec is enough */
+	timeout = jiffies + msecs_to_jiffies(20);
+	while (time_before(jiffies, timeout)) {
+		stat = s3c_read_reg(INT_ERR_STAT_OFFSET);
+		if (stat & flags)
+			break;
+	}
+	/* To get correct interrupt status in timeout case */
+	stat = s3c_read_reg(INT_ERR_STAT_OFFSET);
+	s3c_write_reg(stat, INT_ERR_ACK_OFFSET);
+
+	if (stat & LD_FAIL_ECC_ERR) {
+		s3c_onenand_reset();
+		return ONENAND_BBT_READ_ERROR;
+	}
+
+	if (stat & LOAD_CMP) {
+		int ecc = s3c_read_reg(ECC_ERR_STAT_OFFSET);
+		if (ecc & ONENAND_ECC_4BIT_UNCORRECTABLE) {
+			s3c_onenand_reset();
+			return ONENAND_BBT_READ_ERROR;
+		}
+	}
+
+	return 0;
+}
+
+static void s3c_onenand_check_lock_status(struct mtd_info *mtd)
+{
+	struct onenand_chip *this = mtd->priv;
+	struct device *dev = &onenand->pdev->dev;
+	unsigned int block, end;
+	int tmp;
+
+	end = this->chipsize >> this->erase_shift;
+
+	for (block = 0; block < end; block++) {
+		unsigned int mem_addr = onenand->mem_addr(block, 0, 0);
+		tmp = s3c_read_cmd(CMD_MAP_01(onenand, mem_addr));
+
+		if (s3c_read_reg(INT_ERR_STAT_OFFSET) & LOCKED_BLK) {
+			dev_err(dev, "block %d is write-protected!\n", block);
+			s3c_write_reg(LOCKED_BLK, INT_ERR_ACK_OFFSET);
+		}
+	}
+}
+
+static void s3c_onenand_do_lock_cmd(struct mtd_info *mtd, loff_t ofs,
+				    size_t len, int cmd)
+{
+	struct onenand_chip *this = mtd->priv;
+	int start, end, start_mem_addr, end_mem_addr;
+
+	start = ofs >> this->erase_shift;
+	start_mem_addr = onenand->mem_addr(start, 0, 0);
+	end = start + (len >> this->erase_shift) - 1;
+	end_mem_addr = onenand->mem_addr(end, 0, 0);
+
+	if (cmd == ONENAND_CMD_LOCK) {
+		s3c_write_cmd(ONENAND_LOCK_START, CMD_MAP_10(onenand,
+							     start_mem_addr));
+		s3c_write_cmd(ONENAND_LOCK_END, CMD_MAP_10(onenand,
+							   end_mem_addr));
+	} else {
+		s3c_write_cmd(ONENAND_UNLOCK_START, CMD_MAP_10(onenand,
+							       start_mem_addr));
+		s3c_write_cmd(ONENAND_UNLOCK_END, CMD_MAP_10(onenand,
+							     end_mem_addr));
+	}
+
+	this->wait(mtd, FL_LOCKING);
+}
+
+static void s3c_unlock_all(struct mtd_info *mtd)
+{
+	struct onenand_chip *this = mtd->priv;
+	loff_t ofs = 0;
+	size_t len = this->chipsize;
+
+	if (this->options & ONENAND_HAS_UNLOCK_ALL) {
+		/* Write unlock command */
+		this->command(mtd, ONENAND_CMD_UNLOCK_ALL, 0, 0);
+
+		/* No need to check return value */
+		this->wait(mtd, FL_LOCKING);
+
+		/* Workaround for all block unlock in DDP */
+		if (!ONENAND_IS_DDP(this)) {
+			s3c_onenand_check_lock_status(mtd);
+			return;
+		}
+
+		/* All blocks on another chip */
+		ofs = this->chipsize >> 1;
+		len = this->chipsize >> 1;
+	}
+
+	s3c_onenand_do_lock_cmd(mtd, ofs, len, ONENAND_CMD_UNLOCK);
+
+	s3c_onenand_check_lock_status(mtd);
+}
+
+static void s3c_onenand_setup(struct mtd_info *mtd)
+{
+	struct onenand_chip *this = mtd->priv;
+
+	onenand->mtd = mtd;
+
+	if (onenand->type == TYPE_S3C6400) {
+		onenand->mem_addr = s3c6400_mem_addr;
+		onenand->cmd_map = s3c64xx_cmd_map;
+	} else if (onenand->type == TYPE_S3C6410) {
+		onenand->mem_addr = s3c6410_mem_addr;
+		onenand->cmd_map = s3c64xx_cmd_map;
+	} else if (onenand->type == TYPE_S5PC110) {
+		/* Use generic onenand functions */
+		this->read_bufferram = s5pc110_read_bufferram;
+		this->chip_probe = s5pc110_chip_probe;
+		return;
+	} else {
+		BUG();
+	}
+
+	this->read_word = s3c_onenand_readw;
+	this->write_word = s3c_onenand_writew;
+
+	this->wait = s3c_onenand_wait;
+	this->bbt_wait = s3c_onenand_bbt_wait;
+	this->unlock_all = s3c_unlock_all;
+	this->command = s3c_onenand_command;
+
+	this->read_bufferram = onenand_read_bufferram;
+	this->write_bufferram = onenand_write_bufferram;
+}
+
+static int s3c_onenand_probe(struct platform_device *pdev)
+{
+	struct onenand_platform_data *pdata;
+	struct onenand_chip *this;
+	struct mtd_info *mtd;
+	struct resource *r;
+	int size, err;
+
+	pdata = dev_get_platdata(&pdev->dev);
+	/* No need to check pdata. the platform data is optional */
+
+	size = sizeof(struct mtd_info) + sizeof(struct onenand_chip);
+	mtd = devm_kzalloc(&pdev->dev, size, GFP_KERNEL);
+	if (!mtd)
+		return -ENOMEM;
+
+	onenand = devm_kzalloc(&pdev->dev, sizeof(struct s3c_onenand),
+			       GFP_KERNEL);
+	if (!onenand)
+		return -ENOMEM;
+
+	this = (struct onenand_chip *) &mtd[1];
+	mtd->priv = this;
+	mtd->dev.parent = &pdev->dev;
+	onenand->pdev = pdev;
+	onenand->type = platform_get_device_id(pdev)->driver_data;
+
+	s3c_onenand_setup(mtd);
+
+	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	onenand->base = devm_ioremap_resource(&pdev->dev, r);
+	if (IS_ERR(onenand->base))
+		return PTR_ERR(onenand->base);
+
+	onenand->phys_base = r->start;
+
+	/* Set onenand_chip also */
+	this->base = onenand->base;
+
+	/* Use runtime badblock check */
+	this->options |= ONENAND_SKIP_UNLOCK_CHECK;
+
+	if (onenand->type != TYPE_S5PC110) {
+		r = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+		onenand->ahb_addr = devm_ioremap_resource(&pdev->dev, r);
+		if (IS_ERR(onenand->ahb_addr))
+			return PTR_ERR(onenand->ahb_addr);
+
+		/* Allocate 4KiB BufferRAM */
+		onenand->page_buf = devm_kzalloc(&pdev->dev, SZ_4K,
+						 GFP_KERNEL);
+		if (!onenand->page_buf)
+			return -ENOMEM;
+
+		/* Allocate 128 SpareRAM */
+		onenand->oob_buf = devm_kzalloc(&pdev->dev, 128, GFP_KERNEL);
+		if (!onenand->oob_buf)
+			return -ENOMEM;
+
+		/* S3C doesn't handle subpage write */
+		mtd->subpage_sft = 0;
+		this->subpagesize = mtd->writesize;
+
+	} else { /* S5PC110 */
+		r = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+		onenand->dma_addr = devm_ioremap_resource(&pdev->dev, r);
+		if (IS_ERR(onenand->dma_addr))
+			return PTR_ERR(onenand->dma_addr);
+
+		s5pc110_dma_ops = s5pc110_dma_poll;
+		/* Interrupt support */
+		r = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+		if (r) {
+			init_completion(&onenand->complete);
+			s5pc110_dma_ops = s5pc110_dma_irq;
+			err = devm_request_irq(&pdev->dev, r->start,
+					       s5pc110_onenand_irq,
+					       IRQF_SHARED, "onenand",
+					       &onenand);
+			if (err) {
+				dev_err(&pdev->dev, "failed to get irq\n");
+				return err;
+			}
+		}
+	}
+
+	err = onenand_scan(mtd, 1);
+	if (err)
+		return err;
+
+	if (onenand->type != TYPE_S5PC110) {
+		/* S3C doesn't handle subpage write */
+		mtd->subpage_sft = 0;
+		this->subpagesize = mtd->writesize;
+	}
+
+	if (s3c_read_reg(MEM_CFG_OFFSET) & ONENAND_SYS_CFG1_SYNC_READ)
+		dev_info(&onenand->pdev->dev, "OneNAND Sync. Burst Read enabled\n");
+
+	err = mtd_device_register(mtd, pdata ? pdata->parts : NULL,
+				  pdata ? pdata->nr_parts : 0);
+	if (err) {
+		dev_err(&pdev->dev, "failed to parse partitions and register the MTD device\n");
+		onenand_release(mtd);
+		return err;
+	}
+
+	platform_set_drvdata(pdev, mtd);
+
+	return 0;
+}
+
+static int s3c_onenand_remove(struct platform_device *pdev)
+{
+	struct mtd_info *mtd = platform_get_drvdata(pdev);
+
+	onenand_release(mtd);
+
+	return 0;
+}
+
+static int s3c_pm_ops_suspend(struct device *dev)
+{
+	struct mtd_info *mtd = dev_get_drvdata(dev);
+	struct onenand_chip *this = mtd->priv;
+
+	this->wait(mtd, FL_PM_SUSPENDED);
+	return 0;
+}
+
+static  int s3c_pm_ops_resume(struct device *dev)
+{
+	struct mtd_info *mtd = dev_get_drvdata(dev);
+	struct onenand_chip *this = mtd->priv;
+
+	this->unlock_all(mtd);
+	return 0;
+}
+
+static const struct dev_pm_ops s3c_pm_ops = {
+	.suspend	= s3c_pm_ops_suspend,
+	.resume		= s3c_pm_ops_resume,
+};
+
+static const struct platform_device_id s3c_onenand_driver_ids[] = {
+	{
+		.name		= "s3c6400-onenand",
+		.driver_data	= TYPE_S3C6400,
+	}, {
+		.name		= "s3c6410-onenand",
+		.driver_data	= TYPE_S3C6410,
+	}, {
+		.name		= "s5pc110-onenand",
+		.driver_data	= TYPE_S5PC110,
+	}, { },
+};
+MODULE_DEVICE_TABLE(platform, s3c_onenand_driver_ids);
+
+static struct platform_driver s3c_onenand_driver = {
+	.driver         = {
+		.name	= "samsung-onenand",
+		.pm	= &s3c_pm_ops,
+	},
+	.id_table	= s3c_onenand_driver_ids,
+	.probe          = s3c_onenand_probe,
+	.remove         = s3c_onenand_remove,
+};
+
+module_platform_driver(s3c_onenand_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Kyungmin Park <kyungmin.park@samsung.com>");
+MODULE_DESCRIPTION("Samsung OneNAND controller support");
diff --git a/drivers/mtd/spi-nor/spi-nor.c b/drivers/mtd/spi-nor/spi-nor.c
index f417fb680..4cc502b32 100644
--- a/drivers/mtd/spi-nor/spi-nor.c
+++ b/drivers/mtd/spi-nor/spi-nor.c
@@ -1861,6 +1861,48 @@ static int spansion_read_cr_quad_enable(struct spi_nor *nor)
 	return 0;
 }
 
+static int write_sr_gd(struct spi_nor *nor, u8 val)
+{
+	if (nor->spimem) {
+		struct spi_mem_op op =
+			SPI_MEM_OP(SPI_MEM_OP_CMD(SPINOR_OP_WRCR, 1),
+				   SPI_MEM_OP_NO_ADDR,
+				   SPI_MEM_OP_NO_DUMMY,
+				   SPI_MEM_OP_DATA_OUT(1, &val, 1));
+
+		return spi_mem_exec_op(nor->spimem, &op);
+	}
+
+	return nor->write_reg(nor, SPINOR_OP_WRCR, &val, 1);
+}
+
+
+static int gd_read_cr_quad_enable(struct spi_nor *nor)
+{
+	int ret, val;
+
+	val = read_cr(nor);
+	if (val < 0)
+		return val;
+	if (val & CR_QUAD_EN_GD)
+		return 0;
+
+	write_enable(nor);
+	write_sr_gd(nor, val | CR_QUAD_EN_GD);
+
+	ret = spi_nor_wait_till_ready(nor);
+	if (ret)
+		return ret;
+
+	ret = read_cr(nor);
+	if (!(ret > 0 && (ret & CR_QUAD_EN_GD))) {
+		dev_err(nor->dev, "Gd Quad bit not set\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
 static int spi_nor_write_sr2(struct spi_nor *nor, u8 *sr2)
 {
 	if (nor->spimem) {
@@ -2283,7 +2325,7 @@ static const struct flash_info spi_nor_ids[] = {
 	{ "mx25u4035",   INFO(0xc22533, 0, 64 * 1024,   8, SECT_4K) },
 	{ "mx25u8035",   INFO(0xc22534, 0, 64 * 1024,  16, SECT_4K) },
 	{ "mx25u6435f",  INFO(0xc22537, 0, 64 * 1024, 128, SECT_4K) },
-	{ "mx25l12805d", INFO(0xc22018, 0, 64 * 1024, 256, 0) },
+	{ "mx25l12805d", INFO(0xc22018, 0, 64 * 1024, 256, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
 	{ "mx25l12855e", INFO(0xc22618, 0, 64 * 1024, 256, 0) },
 	{ "mx25u12835f", INFO(0xc22538, 0, 64 * 1024, 256,
 			 SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
@@ -2503,6 +2545,23 @@ static const struct flash_info spi_nor_ids[] = {
 	/* XMC (Wuhan Xinxin Semiconductor Manufacturing Corp.) */
 	{ "XM25QH64A", INFO(0x207017, 0, 64 * 1024, 128, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
 	{ "XM25QH128A", INFO(0x207018, 0, 64 * 1024, 256, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+
+	/* PUYA */
+	{ "p25q128", INFO(0x856018, 0, 64 * 1024, 256, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+	{ "p25q64h", INFO(0x856017, 0x0, 64 * 1024, 128, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+	{ "p25q32h", INFO(0x856016, 0x0, 64 * 1024, 64, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+
+	/*Zetta*/
+	{ "zd25q64b", INFO(0xba3217, 0, 64 * 1024, 128, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+
+	/* BOYA*/
+	{ "BY25q128",   INFO(0x684018, 0, 64 * 1024, 256, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+
+	/* XTX */
+	{ "xt25p1288",  INFO(0x0b4018, 0, 64 * 1024, 256, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+
+	/* FM */
+	{ "fm25w128",  INFO(0xa12818, 0, 64 * 1024, 256, SECT_4K | SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
 	{ },
 };
 
@@ -4397,9 +4456,15 @@ static void st_micron_set_default_init(struct spi_nor *nor)
 
 static void winbond_set_default_init(struct spi_nor *nor)
 {
+	nor->params.quad_enable = spansion_read_cr_quad_enable;
 	nor->params.set_4byte = winbond_set_4byte;
 }
 
+static void gigadevice_set_default_init(struct spi_nor *nor)
+{
+	nor->params.quad_enable = gd_read_cr_quad_enable;
+	nor->params.set_4byte = macronix_set_4byte;
+}
 /**
  * spi_nor_manufacturer_init_params() - Initialize the flash's parameters and
  * settings based on MFR register and ->default_init() hook.
@@ -4410,19 +4475,35 @@ static void spi_nor_manufacturer_init_params(struct spi_nor *nor)
 	/* Init flash parameters based on MFR */
 	switch (JEDEC_MFR(nor->info)) {
 	case SNOR_MFR_MACRONIX:
+	case SNOR_MFR_XMC:
+		//SNOR_MFR_MICRON
+		if (nor->info->id[1] >> 4 == 'b') {
+			st_micron_set_default_init(nor);
+			break;//because XMC and MICRON id[0] equal
+		}
 		macronix_set_default_init(nor);
 		break;
-
-	case SNOR_MFR_ST:
 	case SNOR_MFR_MICRON:
 		st_micron_set_default_init(nor);
 		break;
+	case SNOR_MFR_GIGADEVICE:
+	case SNOR_MFR_ADESTO:
+	case SNOR_MFR_PUYA:
+	case SNOR_MFR_ZETTA:
+	case SNOR_MFR_BOYA:
+		gigadevice_set_default_init(nor);
+		break;
 
+	case SNOR_MFR_SPANSION:
 	case SNOR_MFR_WINBOND:
+	case SNOR_MFR_XTX:
+	case SNOR_MFR_FM:
 		winbond_set_default_init(nor);
 		break;
 
 	default:
+		dev_err(nor->dev, "SF: Need set QEB func for %02x flash\n",
+				JEDEC_MFR(nor->info));
 		break;
 	}
 
diff --git a/drivers/mtd/sunxipart.c b/drivers/mtd/sunxipart.c
new file mode 100644
index 000000000..9d90c7c0e
--- /dev/null
+++ b/drivers/mtd/sunxipart.c
@@ -0,0 +1,220 @@
+/*
+ * SUNXI MTD partitioning
+ *
+ * Copyright © 2016 WimHuang <huangwei@allwinnertech.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#define pr_fmt(fmt)	"sunxipart: " fmt
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/partitions.h>
+#include "efi.h"
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW16) || IS_ENABLED(CONFIG_ARCH_SUN8IW18) || \
+		IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_UBOOT_DISP_ENABLE)
+#define MBR_OFFSET		((1024 - 16) * 1024)
+#else
+#define MBR_OFFSET		((512 - 16) * 1024)
+#endif
+#define MBR_SIZE		(16 * 1024)
+#define DL_SIZE			(16 * 1024)
+#define MBR_MAGIC		"softw411"
+#define MBR_MAX_PART_COUNT	120
+#define MBR_RESERVED		(MBR_SIZE - 32 - (MBR_MAX_PART_COUNT * sizeof(struct sunxi_partition)))
+#define NOR_BLK_SIZE		512
+
+/* partition information */
+struct sunxi_partition {
+	unsigned  int		addrhi;
+	unsigned  int		addrlo;
+	unsigned  int		lenhi;
+	unsigned  int		lenlo;
+	unsigned  char		classname[16];
+	unsigned  char		name[16];
+	unsigned  int		user_type;
+	unsigned  int		keydata;
+	unsigned  int		ro;
+	unsigned  char		reserved[68];
+} __packed;
+
+/* mbr information */
+struct sunxi_mbr {
+	unsigned  int		crc32;
+	unsigned  int		version;
+	unsigned  char		magic[8];
+	unsigned  int		copy;
+	unsigned  int		index;
+	unsigned  int		PartCount;
+	unsigned  int		stamp[1];
+	struct sunxi_partition	array[MBR_MAX_PART_COUNT];
+	unsigned  char		res[MBR_RESERVED];
+} __packed;
+
+/* save partition's name */
+static char partition_name[MBR_MAX_PART_COUNT][16];
+
+/* to get mbr_offset from cmdline */
+static int cmdline_mbr_offset = -1;
+
+static int __init get_mbr_offset_from_cmdline(char *p)
+{
+
+	pr_debug("%s(%s)\n", __func__, p);
+	if (p == NULL) {
+		cmdline_mbr_offset = -1;
+	} else {
+		cmdline_mbr_offset = memparse(p, &p);
+
+		if (cmdline_mbr_offset < 0)
+			cmdline_mbr_offset = -1;
+	}
+	return 0;
+}
+
+early_param("mbr_offset", get_mbr_offset_from_cmdline);
+
+static void sunxipart_add_part(struct mtd_partition *part, char *name,
+				uint64_t size, uint64_t offset)
+{
+	part->name = name;
+	part->size = size;
+	part->offset = offset;
+}
+
+static int sunxipart_parse(struct mtd_info *master,
+				const struct mtd_partition **pparts,
+				struct mtd_part_parser_data *data)
+{
+	int i, ret, nrparts, parts_size;
+	size_t bytes_read;
+	struct sunxi_mbr *sunxi_mbr;
+	struct mtd_partition *parts;
+	int mbr_offset;
+	sunxi_mbr = kzalloc(MBR_SIZE, GFP_KERNEL);
+	if (sunxi_mbr == NULL) {
+		pr_err("failed to alloc sunxi_mbr\n");
+		return -ENOMEM;
+	}
+
+	if (cmdline_mbr_offset > 0)
+		mbr_offset =  cmdline_mbr_offset;
+	else
+		mbr_offset = MBR_OFFSET;
+
+	ret = mtd_read(master, mbr_offset, MBR_SIZE,
+		       &bytes_read, (uint8_t *)sunxi_mbr);
+	if ((ret < 0)) {
+		pr_err("failed to read sunxi_mbr!\n");
+		kfree(sunxi_mbr);
+		return -EIO;
+	}
+
+	if (memcmp(sunxi_mbr->magic, MBR_MAGIC, strlen(MBR_MAGIC)) == 0) {
+		if ((sunxi_mbr->PartCount == 0)
+		     || (sunxi_mbr->PartCount > MBR_MAX_PART_COUNT)) {
+			pr_err("failed to parse sunxi_mbr)!\n");
+			kfree(sunxi_mbr);
+			return -EINVAL;
+		}
+
+		nrparts = sunxi_mbr->PartCount + 1;
+		parts_size = nrparts * sizeof(*parts);
+		parts = kzalloc(parts_size, GFP_KERNEL);
+		if (parts == NULL) {
+			pr_err("failed to alloc %d patitions\n", nrparts);
+			kfree(sunxi_mbr);
+			return -ENOMEM;
+		}
+
+		strncpy(partition_name[0], "uboot", 16);
+		sunxipart_add_part(&parts[0], partition_name[0],
+						mbr_offset + MBR_SIZE, 0);
+		for (i = 1; i < nrparts; i++) {
+			strncpy(partition_name[i], sunxi_mbr->array[i - 1].name,
+				16);
+
+			sunxipart_add_part(
+			    &parts[i], partition_name[i],
+			    sunxi_mbr->array[i - 1].lenlo * NOR_BLK_SIZE,
+			    sunxi_mbr->array[i - 1].addrlo * NOR_BLK_SIZE +
+				mbr_offset);
+		}
+	} else {
+		struct _gpt_header *gpt_head =
+		    (struct _gpt_header *)((char *)sunxi_mbr + 512);
+		struct _gpt_entry *entry =
+		    (struct _gpt_entry *)((char *)sunxi_mbr + 1024);
+		unsigned int j = 0;
+
+		if (gpt_head->signature != GPT_HEADER_SIGNATURE) {
+			pr_err("failed to parse sunxi_gpt!\n");
+			kfree(sunxi_mbr);
+			return -EINVAL;
+		}
+		nrparts = gpt_head->num_partition_entries + 1;
+		parts_size = nrparts * sizeof(*parts);
+		parts = kzalloc(parts_size, GFP_KERNEL);
+		if (parts == NULL) {
+			pr_err("failed to alloc %d patitions\n", nrparts);
+			kfree(sunxi_mbr);
+			return -ENOMEM;
+		}
+
+		strncpy(partition_name[0], "uboot", 16);
+		sunxipart_add_part(&parts[0], partition_name[0],
+				   mbr_offset + MBR_SIZE, 0);
+		for (i = 1; i < nrparts; i++) {
+			for (j = 0; j < 16; j++)
+				partition_name[i][j] =
+				    (char)(entry[i - 1].partition_name[j]);
+
+			pr_debug("GPT:%-12s: %-12llx  %-12llx\n",
+				 partition_name[i], entry[i - 1].starting_lba,
+				 entry[i - 1].ending_lba);
+
+			sunxipart_add_part(&parts[i], partition_name[i],
+					   (entry[i - 1].ending_lba -
+					    entry[i - 1].starting_lba + 1) *
+					       NOR_BLK_SIZE,
+					   entry[i - 1].starting_lba *
+						   NOR_BLK_SIZE +
+					       mbr_offset);
+		}
+	}
+
+	kfree(sunxi_mbr);
+	*pparts = parts;
+	return nrparts;
+}
+
+static struct mtd_part_parser sunxipart_mtd_parser = {
+	.owner = THIS_MODULE,
+	.parse_fn = sunxipart_parse,
+	.name = "sunxipart",
+};
+
+static int __init sunxipart_init(void)
+{
+	register_mtd_parser(&sunxipart_mtd_parser);
+
+	return 0;
+}
+
+static void __exit sunxipart_exit(void)
+{
+	deregister_mtd_parser(&sunxipart_mtd_parser);
+}
+
+module_init(sunxipart_init);
+module_exit(sunxipart_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("MTD partitioning for SUNXI flash memories");
diff --git a/drivers/mtd/ubi/Kconfig b/drivers/mtd/ubi/Kconfig
index 2ed77b7b3..e8f177783 100644
--- a/drivers/mtd/ubi/Kconfig
+++ b/drivers/mtd/ubi/Kconfig
@@ -30,7 +30,7 @@ config MTD_UBI_WL_THRESHOLD
 
 config MTD_UBI_BEB_LIMIT
 	int "Maximum expected bad eraseblock count per 1024 eraseblocks"
-	default 20
+	default 40
 	range 0 768
 	help
 	  This option specifies the maximum bad physical eraseblocks UBI
-- 
2.17.1

